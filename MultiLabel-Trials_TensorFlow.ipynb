{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4214, 138)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2020XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2020XPT_Grounded_ML_nona.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE4'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE4']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3371 samples\n",
      "Epoch 1/200\n",
      "3371/3371 [==============================] - 1s 160us/sample - loss: 102313444233368384.0000\n",
      "Epoch 2/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 1423272024529.9792\n",
      "Epoch 3/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 339398144173.7549\n",
      "Epoch 4/200\n",
      "3371/3371 [==============================] - 0s 64us/sample - loss: 333806299175.4898\n",
      "Epoch 5/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 331116907578.0196\n",
      "Epoch 6/200\n",
      "3371/3371 [==============================] - 0s 67us/sample - loss: 326090475965.3231\n",
      "Epoch 7/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 318081831822.0872\n",
      "Epoch 8/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 311314745269.2732\n",
      "Epoch 9/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 303576777792.7025\n",
      "Epoch 10/200\n",
      "3371/3371 [==============================] - 0s 60us/sample - loss: 296680165226.5464\n",
      "Epoch 11/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 287571947606.2700\n",
      "Epoch 12/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 276744056991.1741\n",
      "Epoch 13/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 267471186824.6194\n",
      "Epoch 14/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 257585260291.8730\n",
      "Epoch 15/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 251656422858.3850\n",
      "Epoch 16/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 236308376925.6363\n",
      "Epoch 17/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 226447538197.2637\n",
      "Epoch 18/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 219865915627.7235\n",
      "Epoch 19/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 204374249672.4865\n",
      "Epoch 20/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 194342548976.0522\n",
      "Epoch 21/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 185535776440.5387\n",
      "Epoch 22/200\n",
      "3371/3371 [==============================] - ETA: 0s - loss: 172892452437.33 - 0s 51us/sample - loss: 172773664389.5058\n",
      "Epoch 23/200\n",
      "3371/3371 [==============================] - 0s 63us/sample - loss: 161165668127.2121\n",
      "Epoch 24/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 153105626768.4414\n",
      "Epoch 25/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 143068875651.7591\n",
      "Epoch 26/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 129634951155.8493\n",
      "Epoch 27/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 120172907770.0006\n",
      "Epoch 28/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 110587349617.1534\n",
      "Epoch 29/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 99825131923.4032\n",
      "Epoch 30/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 99239255050.6319\n",
      "Epoch 31/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 84096601520.86860s - loss: 80425078387.6\n",
      "Epoch 32/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 76109724589.0715\n",
      "Epoch 33/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 66130655190.0801\n",
      "Epoch 34/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 59209829844.7131\n",
      "Epoch 35/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 56362878366.6425\n",
      "Epoch 36/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 45774795619.5598\n",
      "Epoch 37/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 40613971167.8766\n",
      "Epoch 38/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 34471346949.6956\n",
      "Epoch 39/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 30619744723.8018\n",
      "Epoch 40/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 25788836217.88670s - loss: 28164463974.40\n",
      "Epoch 41/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 25232984551.5467\n",
      "Epoch 42/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 18526791391.4209\n",
      "Epoch 43/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 15906048791.9217\n",
      "Epoch 44/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 13025083761.0774\n",
      "Epoch 45/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 11625132628.9030\n",
      "Epoch 46/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 10934129972.3239\n",
      "Epoch 47/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 8540965044.4379\n",
      "Epoch 48/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 7017176880.3750\n",
      "Epoch 49/200\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: 6330817731.1706s - loss: 6470037490.\n",
      "Epoch 50/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 5481090451.0234s - loss: 5642299742.\n",
      "Epoch 51/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 5116231609.9057\n",
      "Epoch 52/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 4116357454.3720\n",
      "Epoch 53/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 3633025031.2904\n",
      "Epoch 54/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 3596922051.9300\n",
      "Epoch 55/200\n",
      "3371/3371 [==============================] - 0s 38us/sample - loss: 3626372740.1388\n",
      "Epoch 56/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 3327277589.7953\n",
      "Epoch 57/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 3301011572.4948\n",
      "Epoch 58/200\n",
      "3371/3371 [==============================] - 0s 36us/sample - loss: 3547307522.9238\n",
      "Epoch 59/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 3220971366.2937\n",
      "Epoch 60/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 3370342627.9015\n",
      "Epoch 61/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 2791064409.4595\n",
      "Epoch 62/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2807286000.3560\n",
      "Epoch 63/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 2776225494.8395\n",
      "Epoch 64/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 3407891948.6348\n",
      "Epoch 65/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 3630951740.8294\n",
      "Epoch 66/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 2370703871.6203s - loss: 2689303579.04\n",
      "Epoch 67/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3753550495.1741\n",
      "Epoch 68/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 2511272561.7229\n",
      "Epoch 69/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 2617881828.6230\n",
      "Epoch 70/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2111202800.1282\n",
      "Epoch 71/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 4054608749.4322\n",
      "Epoch 72/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 2861074488.1020\n",
      "Epoch 73/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 2495301419.9703\n",
      "Epoch 74/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 4430852878.0492s - loss: 4264306776.\n",
      "Epoch 75/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3863999582.7754\n",
      "Epoch 76/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 6436300227.5503\n",
      "Epoch 77/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 26053892215.0768\n",
      "Epoch 78/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 1551014972008.0403\n",
      "Epoch 79/200\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: 1482583941403.7188\n",
      "Epoch 80/200\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: 2033538560102.0659\n",
      "Epoch 81/200\n",
      "3371/3371 [==============================] - 0s 56us/sample - loss: 2442011953093.0693\n",
      "Epoch 82/200\n",
      "3371/3371 [==============================] - 0s 65us/sample - loss: 1655266308013.7549\n",
      "Epoch 83/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 2836912906800.1475\n",
      "Epoch 84/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 535717893359.6725\n",
      "Epoch 85/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1642465387044.3000\n",
      "Epoch 86/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 3367302671280.7168\n",
      "Epoch 87/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 3388274606750.7188\n",
      "Epoch 88/200\n",
      "3371/3371 [==============================] - ETA: 0s - loss: 69733644692.347 - 0s 50us/sample - loss: 63418006605.0050\n",
      "Epoch 89/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 3918227075338.3281\n",
      "Epoch 90/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 6888299828.5897\n",
      "Epoch 91/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 5160286717529.3076\n",
      "Epoch 92/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 2884835229.8831\n",
      "Epoch 93/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 3042985500009.1792\n",
      "Epoch 94/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 2386099067919.9380\n",
      "Epoch 95/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 2183265740665.1272\n",
      "Epoch 96/200\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: 2801322548611.9111\n",
      "Epoch 97/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 290679703004.4592\n",
      "Epoch 98/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 3823127205223.3564\n",
      "Epoch 99/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 60832646050.1359\n",
      "Epoch 100/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 3232969922531.7114\n",
      "Epoch 101/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 3582948616553.6353\n",
      "Epoch 102/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 50226111869.2471\n",
      "Epoch 103/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 3821098471618.1074\n",
      "Epoch 104/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 10632102604.8152\n",
      "Epoch 105/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 3870518528575.3354\n",
      "Epoch 106/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 5696170346.8502\n",
      "Epoch 107/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 3200015337853.5317\n",
      "Epoch 108/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 1837046446520.9185\n",
      "Epoch 109/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1573419757850.8076\n",
      "Epoch 110/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 862070951244.3215\n",
      "Epoch 111/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 3738340265875.8589\n",
      "Epoch 112/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 21001218228.1341\n",
      "Epoch 113/200\n",
      "3371/3371 [==============================] - 0s 56us/sample - loss: 2692767233702.2363\n",
      "Epoch 114/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 2108060560570.2092\n",
      "Epoch 115/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 432924906420.4378\n",
      "Epoch 116/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 1365954541407.3069\n",
      "Epoch 117/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 2157777535802.9690\n",
      "Epoch 118/200\n",
      "3371/3371 [==============================] - 0s 62us/sample - loss: 2826342570455.4473\n",
      "Epoch 119/200\n",
      "3371/3371 [==============================] - 0s 57us/sample - loss: 20039204712.4201\n",
      "Epoch 120/200\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: 2770106701675.9893\n",
      "Epoch 121/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 2410185744940.1982\n",
      "Epoch 122/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 215932495867.7473\n",
      "Epoch 123/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3487554853354.2808\n",
      "Epoch 124/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 10663892913.62800s - loss: 16117708503.86\n",
      "Epoch 125/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 2427825170428.6587\n",
      "Epoch 126/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 56025056437.2732\n",
      "Epoch 127/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1429927805086.8706\n",
      "Epoch 128/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 2091041833342.2534\n",
      "Epoch 129/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 2443107783083.7427\n",
      "Epoch 130/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 38643918810.0291\n",
      "Epoch 131/200\n",
      "3371/3371 [==============================] - 0s 57us/sample - loss: 1705284469722.3330\n",
      "Epoch 132/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 3683409380875.2397\n",
      "Epoch 133/200\n",
      "3371/3371 [==============================] - 0s 64us/sample - loss: 79955529934.8656\n",
      "Epoch 134/200\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: 9670360327.8980\n",
      "Epoch 135/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 2388730146127.0557\n",
      "Epoch 136/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 1932204311863.9241\n",
      "Epoch 137/200\n",
      "3371/3371 [==============================] - 0s 61us/sample - loss: 801515787026.4586\n",
      "Epoch 138/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 1621799989503.5444\n",
      "Epoch 139/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 533564416514.1074\n",
      "Epoch 140/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1903094989534.8135\n",
      "Epoch 141/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 90064306302.3673\n",
      "Epoch 142/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 2419883717880.9941\n",
      "Epoch 143/200\n",
      "3371/3371 [==============================] - 0s 66us/sample - loss: 1473899913220.1199\n",
      "Epoch 144/200\n",
      "3371/3371 [==============================] - 0s 62us/sample - loss: 697794126493.6553\n",
      "Epoch 145/200\n",
      "3371/3371 [==============================] - 0s 63us/sample - loss: 938899540881.7324\n",
      "Epoch 146/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 2008489424838.1321\n",
      "Epoch 147/200\n",
      "3371/3371 [==============================] - 0s 38us/sample - loss: 50251351878.3981\n",
      "Epoch 148/200\n",
      "3371/3371 [==============================] - 0s 56us/sample - loss: 1806281629437.8428\n",
      "Epoch 149/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 2677944847429.7529\n",
      "Epoch 150/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 52559221320.1543\n",
      "Epoch 151/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 1786635880960.0476\n",
      "Epoch 152/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 108244247699.8588\n",
      "Epoch 153/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 1603181063633.0681\n",
      "Epoch 154/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1570613904344.1685\n",
      "Epoch 155/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 53425498131.7449\n",
      "Epoch 156/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 2243562758751.3452\n",
      "Epoch 157/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 6112193811.7449\n",
      "Epoch 158/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 2071529085198.8657\n",
      "Epoch 159/200\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: 1312507005601.8701\n",
      "Epoch 160/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 501156428577.1107\n",
      "Epoch 161/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1988586510504.5908\n",
      "Epoch 162/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 26751701288.7808\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1693812598378.1667\n",
      "Epoch 164/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 105191009605.9425\n",
      "Epoch 165/200\n",
      "3371/3371 [==============================] - 0s 57us/sample - loss: 1341873539605.2637\n",
      "Epoch 166/200\n",
      "3371/3371 [==============================] - 0s 65us/sample - loss: 1468912527680.5505\n",
      "Epoch 167/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 1343670523333.9805\n",
      "Epoch 168/200\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: 453852704163.5028\n",
      "Epoch 169/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 1593557756381.2185\n",
      "Epoch 170/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 27453833610.1952\n",
      "Epoch 171/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1382207098857.6731\n",
      "Epoch 172/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1367785726820.8506\n",
      "Epoch 173/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 43858166808.9089\n",
      "Epoch 174/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1027095884467.6405\n",
      "Epoch 175/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1328182081426.0742\n",
      "Epoch 176/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1778858011463.3665\n",
      "Epoch 177/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 139053411423.2311\n",
      "Epoch 178/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1145983145883.9846\n",
      "Epoch 179/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 79965709117.5509\n",
      "Epoch 180/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 929342345273.1367\n",
      "Epoch 181/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1703056502514.5583\n",
      "Epoch 182/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 4162768490.6224\n",
      "Epoch 183/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1174975018295.2690\n",
      "Epoch 184/200\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: 1228496939370.6414\n",
      "Epoch 185/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 37035868141.8499\n",
      "Epoch 186/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 1081694950474.0244\n",
      "Epoch 187/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 1825966321050.7693\n",
      "Epoch 188/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 19179849009.6589\n",
      "Epoch 189/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 1180371054902.2415\n",
      "Epoch 190/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 337835794806.2415\n",
      "Epoch 191/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1157113405156.8127\n",
      "Epoch 192/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 271954155280.1566\n",
      "Epoch 193/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1016306861666.1927\n",
      "Epoch 194/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 30718405634.8858\n",
      "Epoch 195/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 940901462949.0975\n",
      "Epoch 196/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 1352050625754.3708\n",
      "Epoch 197/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 2706195939.4459\n",
      "Epoch 198/200\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: 1302680885007.8718\n",
      "Epoch 199/200\n",
      "3371/3371 [==============================] - 0s 66us/sample - loss: 28491744960.5696\n",
      "Epoch 200/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 1013950156735.5729\n",
      "Accuracy: 0.9857651245551602\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.0071174377224199285\n",
      "[[[843   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[831   0]\n",
      "  [ 12   0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00        12\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        12\n",
      "   macro avg       0.00      0.00      0.00        12\n",
      "weighted avg       0.00      0.00      0.00        12\n",
      " samples avg       0.00      0.00      0.00        12\n",
      "\n",
      "Train on 3371 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [16456.053, 16792.053, 16932.053, 17004.053, 17384.053, 18000.053, 18516.053, 35056.055, 35172.055, 35352.055, 35652.055, 35656.055, 35688.055, 35696.055, 35708.055, 35752.055, 35776.055, 35784.055, 35800.055, 35820.055, 35840.055, 35844.055, 35868.055, 35880.055, 35888.055, 35896.055, 35908.055, 35928.055, 35936.055, 35944.055, 35952.055, 35976.055, 35992.055, 36008.055, 36024.055, 36032.055, 36036.055, 36048.055, 36056.055, 36060.055, 36072.055, 36080.055, 36088.055, 36092.055, 36096.055, 36124.055, 36128.055, 36136.055, 36144.055, 36152.055, 36160.055, 36168.055, 36172.055, 36192.055, 36200.055, 36208.055, 36216.055, 36232.055, 36236.055, 36240.055, 36244.055, 36248.055, 36264.055, 36272.055, 36276.055, 36280.055, 36284.055, 36288.055, 36292.055, 36296.055, 36304.055, 36312.055, 36316.055, 36320.055, 36332.055, 36336.055, 36344.055, 36348.055, 36352.055, 36356.055, 36360.055, 36368.055, 36372.055, 36376.055, 36380.055, 36384.055, 36388.055, 36396.055, 36400.055, 36404.055, 36408.055, 36416.055, 36420.055, 36424.055, 36432.055, 36436.055, 36440.055, 36448.055, 36456.055, 36460.055, 36464.055, 36472.055, 36476.055, 36480.055, 36488.055, 36492.055, 36496.055, 36504.055, 36508.055, 36520.055, 36536.055, 36540.055, 36544.055, 36552.055, 36556.055, 36560.055, 36564.055, 36568.055, 36576.055, 36580.055, 36592.055, 36600.055, 36604.055, 36608.055, 36612.055, 36616.055, 36624.055, 36628.055, 36632.055, 36636.055, 36640.055, 36644.055, 36648.055, 36652.055, 36656.055, 36660.055, 36664.055, 36672.055, 36676.055, 36680.055, 36684.055, 36688.055, 36692.055, 36700.055, 36704.055, 36708.055, 36712.055, 36716.055, 36720.055, 36724.055, 36736.055, 36740.055, 36744.055, 36748.055, 36752.055, 36756.055, 36760.055, 36764.055, 36768.055, 36772.055, 36776.055, 36784.055, 36788.055, 36792.055, 36800.055, 36804.055, 36808.055, 36812.055, 36816.055, 36820.055, 36824.055, 36832.055, 36836.055, 36840.055, 36844.055, 36848.055, 36856.055, 36860.055, 36864.055, 36868.055, 36872.055, 36876.055, 36880.055, 36888.055, 36896.055, 36904.055, 36912.055, 36916.055, 36920.055, 36924.055, 36928.055, 36936.055, 36940.055, 36944.055, 36948.055, 36952.055, 36960.055, 36964.055, 36968.055, 36972.055, 36980.055, 36984.055, 36988.055, 36992.055, 37000.055, 37008.055, 37016.055, 37024.055, 37028.055, 37032.055, 37040.055, 37044.055, 37048.055, 37056.055, 37064.055, 37072.055, 37076.055, 37080.055, 37088.055, 37092.055, 37096.055, 37100.055, 37104.055, 37112.055, 37120.055, 37124.055, 37132.055, 37136.055, 37140.055, 37144.055, 37152.055, 37156.055, 37160.055, 37164.055, 37168.055, 37176.055, 37180.055, 37184.055, 37192.055, 37200.055, 37216.055, 37224.055, 37228.055, 37232.055, 37236.055, 37240.055, 37248.055, 37256.055, 37264.055, 37268.055, 37272.055, 37276.055, 37280.055, 37288.055, 37296.055, 37304.055, 37312.055, 37316.055, 37320.055, 37324.055, 37328.055, 37332.055, 37336.055, 37340.055, 37344.055, 37352.055, 37356.055, 37360.055, 37364.055, 37368.055, 37376.055, 37380.055, 37384.055, 37400.055, 37408.055, 37420.055, 37424.055, 37432.055, 37440.055, 37448.055, 37456.055, 37460.055, 37464.055, 37468.055, 37472.055, 37476.055, 37484.055, 37488.055, 37496.055, 37500.055, 37512.055, 37516.055, 37520.055, 37528.055, 37532.055, 37536.055, 37540.055, 37544.055, 37552.055, 37560.055, 37568.055, 37572.055, 37576.055, 37580.055, 37584.055, 37588.055, 37596.055, 37600.055, 37608.055, 37616.055, 37620.055, 37624.055, 37628.055, 37636.055, 37644.055, 37648.055, 37652.055, 37664.055, 37672.055, 37676.055, 37680.055, 37684.055, 37688.055, 37692.055, 37696.055, 37700.055, 37704.055, 37708.055, 37712.055, 37716.055, 37736.055, 37740.055, 37744.055, 37748.055, 37752.055, 37756.055, 37760.055, 37772.055, 37776.055, 37780.055, 37784.055, 37788.055, 37796.055, 37808.055, 37812.055, 37824.055, 37832.055, 37840.055, 37844.055, 37848.055, 37852.055, 37856.055, 37868.055, 37872.055, 37880.055, 37888.055, 37892.055, 37896.055, 37904.055, 37908.055, 37912.055, 37916.055, 37920.055, 37924.055, 37928.055, 37936.055, 37940.055, 37944.055, 37952.055, 37960.055, 37976.055, 37988.055, 37992.055, 38000.055, 38004.055, 38020.055, 38024.055, 38032.055, 38036.055, 38052.055, 38060.055, 38064.055, 38072.055, 38080.055, 38084.055, 38088.055, 38096.055, 38100.055, 38104.055, 38116.055, 38120.055, 38128.055, 38136.055, 38140.055, 38148.055, 38156.055, 38164.055, 38168.055, 38172.055, 38176.055, 38184.055, 38200.055, 38212.055, 38216.055, 38220.055, 38228.055, 38232.055, 38248.055, 38256.055, 38264.055, 38272.055, 38284.055, 38288.055, 38296.055, 38312.055, 38324.055, 38328.055, 38332.055, 38336.055, 38352.055, 38360.055, 38368.055, 38424.055, 38436.055, 38448.055, 38456.055, 38480.055, 38496.055, 38512.055, 38536.055, 38552.055, 38560.055, 38568.055, 38624.055, 38656.055, 38760.055, 38824.055, 38900.055, 39000.055, 39056.055] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 1s 203us/sample - loss: 338651464839929.8125\n",
      "Epoch 2/200\n",
      "3371/3371 [==============================] - 0s 60us/sample - loss: 11339174759.3569\n",
      "Epoch 3/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 4927240921.0418\n",
      "Epoch 4/200\n",
      "3371/3371 [==============================] - 0s 77us/sample - loss: 3761975735.0199\n",
      "Epoch 5/200\n",
      "3371/3371 [==============================] - 0s 56us/sample - loss: 2932804468.7226\n",
      "Epoch 6/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2180953814.0042\n",
      "Epoch 7/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1752472250.8170\n",
      "Epoch 8/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 1196544461.8024\n",
      "Epoch 9/200\n",
      "3371/3371 [==============================] - 0s 63us/sample - loss: 872329390.3815\n",
      "Epoch 10/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 386675822.5524\n",
      "Epoch 11/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 218680150.2937\n",
      "Epoch 12/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 129860661.8546\n",
      "Epoch 13/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 70509983.4957\n",
      "Epoch 14/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 36500348.2581\n",
      "Epoch 15/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 13446958.0306\n",
      "Epoch 16/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 10604551.2601\n",
      "Epoch 17/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 4810175.2101\n",
      "Epoch 18/200\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: 2778422.7922\n",
      "Epoch 19/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3922174.0007\n",
      "Epoch 20/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 2898742.9014\n",
      "Epoch 21/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 5098958.6851\n",
      "Epoch 22/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3685039.1624\n",
      "Epoch 23/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 5813077.9749\n",
      "Epoch 24/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 4072417.02040s - loss: 4270924.092\n",
      "Epoch 25/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 4871722589430.5615\n",
      "Epoch 26/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 355517420831.8956\n",
      "Epoch 27/200\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: 3936657619124.1343\n",
      "Epoch 28/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 126334715984.0047\n",
      "Epoch 29/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3387509814412.3408\n",
      "Epoch 30/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 3106653828776.1733\n",
      "Epoch 31/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 1470043047539.8875\n",
      "Epoch 32/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1238655631544.8425\n",
      "Epoch 33/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 2586721325563.5195\n",
      "Epoch 34/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3583497368975.4541\n",
      "Epoch 35/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 2873646750042.5986 - loss: 13709970016.\n",
      "Epoch 36/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 538359485346.7434\n",
      "Epoch 37/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 3089239197557.4062\n",
      "Epoch 38/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 3400646977986.7905\n",
      "Epoch 39/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 346161628995.3035\n",
      "Epoch 40/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 3075260248839.7842\n",
      "Epoch 41/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 2075147160314.4563\n",
      "Epoch 42/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 590008757777.0110\n",
      "Epoch 43/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 3280132390888.4585\n",
      "Epoch 44/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 3471381852869.6006\n",
      "Epoch 45/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 517317499353.8261\n",
      "Epoch 46/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 3827982690929.0586\n",
      "Epoch 47/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 459407355942.5309\n",
      "Epoch 48/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2580975328911.5303\n",
      "Epoch 49/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 934075068275.1659\n",
      "Epoch 50/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 3310362000129.1152\n",
      "Epoch 51/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 94524742935.2382\n",
      "Epoch 52/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 2538842885467.5103\n",
      "Epoch 53/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 2219588680892.6396\n",
      "Epoch 54/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 204032042807.5325\n",
      "Epoch 55/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 4405845245593.5547\n",
      "Epoch 56/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1415474748.7962\n",
      "Epoch 57/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 5014319934955.8467\n",
      "Epoch 58/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 58257783545.7538\n",
      "Epoch 59/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1060615914.6793\n",
      "Epoch 60/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 3672551763176.6860\n",
      "Epoch 61/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 8169149929.3693\n",
      "Epoch 62/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 1735389967593.9009\n",
      "Epoch 63/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 3946303959935.4492\n",
      "Epoch 64/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 12464357098.2806\n",
      "Epoch 65/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2621207282488.7285\n",
      "Epoch 66/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 41948141219.8825\n",
      "Epoch 67/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2212986756999.7080\n",
      "Epoch 68/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1699880586212.6609\n",
      "Epoch 69/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1985730830333.7979\n",
      "Epoch 70/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 51975023132.6301\n",
      "Epoch 71/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 3242734116558.8281\n",
      "Epoch 72/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 5554427192.4912\n",
      "Epoch 73/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 2715986382445.9639\n",
      "Epoch 74/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 1955118516209.2910\n",
      "Epoch 75/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 612068389216.9019\n",
      "Epoch 76/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 2476369758849.8677\n",
      "Epoch 77/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 46346627632.9066\n",
      "Epoch 78/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 1061776161389.8119\n",
      "Epoch 79/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 2422094392575.0889\n",
      "Epoch 80/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 2002560345068.7109\n",
      "Epoch 81/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 450375780159.1836\n",
      "Epoch 82/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 2224073364136.8379\n",
      "Epoch 83/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 45053756691.99170s - loss: 63817548565.1351\n",
      "Epoch 84/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 2432319162548.7417\n",
      "Epoch 85/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 3634972846.7422\n",
      "Epoch 86/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 2179003413229.6221\n",
      "Epoch 87/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1499896077707.4294\n",
      "Epoch 88/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 449691884811.9229\n",
      "Epoch 89/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 3395816922237.8354\n",
      "Epoch 90/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 28354450215.6417\n",
      "Epoch 91/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 4164005411.9964\n",
      "Epoch 92/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 1944296861514.2334\n",
      "Epoch 93/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1546728786695.9551\n",
      "Epoch 94/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 134840970075.50990s - loss: 173684296671.80\n",
      "Epoch 95/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 1115738232875.4387\n",
      "Epoch 96/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 1359807760636.1270\n",
      "Epoch 97/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 854838277373.5699\n",
      "Epoch 98/200\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: 1688562074352.7356\n",
      "Epoch 99/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 1191797452782.3813\n",
      "Epoch 100/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 317155562130.0742\n",
      "Epoch 101/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 1341285217894.5215\n",
      "Epoch 102/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1900653988315.1682\n",
      "Epoch 103/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 53059005009.9413\n",
      "Epoch 104/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1781309810064.2896\n",
      "Epoch 105/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 17935935053.2590\n",
      "Epoch 106/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1490689338165.3110\n",
      "Epoch 107/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 60390950961.9958\n",
      "Epoch 108/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 2094616203299.8328\n",
      "Epoch 109/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 321664245.8048\n",
      "Epoch 110/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1774874639589.4961\n",
      "Epoch 111/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 5759082164.5992\n",
      "Epoch 112/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1647779937102.0684\n",
      "Epoch 113/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1141465722635.3154\n",
      "Epoch 114/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 610818198014.6322\n",
      "Epoch 115/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 109066251878.5110\n",
      "Epoch 116/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1788421509915.0874\n",
      "Epoch 117/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 100943284.6965\n",
      "Epoch 118/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 2191624957593.6731\n",
      "Epoch 119/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 599892280.5102\n",
      "Epoch 120/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 1506156289180.3311\n",
      "Epoch 121/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 24387504699.3405\n",
      "Epoch 122/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1909730300490.9543\n",
      "Epoch 123/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 13979277064.9077\n",
      "Epoch 124/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 1327875301158.3127\n",
      "Epoch 125/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 235889336249.3598\n",
      "Epoch 126/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 829146562002.1311\n",
      "Epoch 127/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 947230147114.9071\n",
      "Epoch 128/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 3426806611.4601\n",
      "Epoch 129/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1030729713968.6407\n",
      "Epoch 130/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1046282700464.4130\n",
      "Epoch 131/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 36804458838.5737\n",
      "Epoch 132/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 717217423961.7490\n",
      "Epoch 133/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 945516534856.9042\n",
      "Epoch 134/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 983618250125.1379\n",
      "Epoch 135/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 13693753066.5085\n",
      "Epoch 136/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1087893926325.5010\n",
      "Epoch 137/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 899593495926.2794\n",
      "Epoch 138/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 55952436569.9152\n",
      "Epoch 139/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 890995729322.1193\n",
      "Epoch 140/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 646108996722.7482s - loss: 304257787204.\n",
      "Epoch 141/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 750618485296.0333\n",
      "Epoch 142/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 65730709489.5521\n",
      "Epoch 143/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1588295802248.97070s - loss: 4436152578275.55\n",
      "Epoch 144/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 111522973.2430\n",
      "Epoch 145/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 976547433033.6779\n",
      "Epoch 146/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 25488042670.9700\n",
      "Epoch 147/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 741172871263.1931\n",
      "Epoch 148/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 991966119435.4767\n",
      "Epoch 149/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 25445915180.9576\n",
      "Epoch 150/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 800679897188.5470\n",
      "Epoch 151/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 37208116127.8576\n",
      "Epoch 152/200\n",
      "3371/3371 [==============================] - ETA: 0s - loss: 974474917799.746 - 0s 45us/sample - loss: 731152008696.2349\n",
      "Epoch 153/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 732493983603.0330\n",
      "Epoch 154/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 124179877387.3343\n",
      "Epoch 155/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 766548715630.2296\n",
      "Epoch 156/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 888660148777.5402\n",
      "Epoch 157/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 94897252176.6075\n",
      "Epoch 158/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 854364103019.5717\n",
      "Epoch 159/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 102625794746.1905\n",
      "Epoch 160/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 697194072476.5922\n",
      "Epoch 161/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 46758675134.9178\n",
      "Epoch 162/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 586215548439.9218\n",
      "Epoch 163/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 795877763164.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 13088982492.4972\n",
      "Epoch 165/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 503548634369.2531\n",
      "Epoch 166/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 557317874353.3622\n",
      "Epoch 167/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 632177089790.0825\n",
      "Epoch 168/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 217703692457.1653\n",
      "Epoch 169/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 736728176988.3832\n",
      "Epoch 170/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 84276467411.8588\n",
      "Epoch 171/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 702754157255.3853\n",
      "Epoch 172/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 26160429882.4373\n",
      "Epoch 173/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 548249983016.0973\n",
      "Epoch 174/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 443727736913.1818\n",
      "Epoch 175/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 65465845962.0860\n",
      "Epoch 176/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 725596046402.0292\n",
      "Epoch 177/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 105997172220.3073\n",
      "Epoch 178/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 694592679710.6616\n",
      "Epoch 179/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 22570556355.9869\n",
      "Epoch 180/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 618354956002.9143\n",
      "Epoch 181/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 478563761602.1548\n",
      "Epoch 182/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 34836464327.8790\n",
      "Epoch 183/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 537250629556.2100\n",
      "Epoch 184/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 460556096121.7348\n",
      "Epoch 185/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 450894145281.9745\n",
      "Epoch 186/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 83039881875.4791\n",
      "Epoch 187/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 424306483551.7152\n",
      "Epoch 188/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 335181787686.3127\n",
      "Epoch 189/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1212151358203.5193\n",
      "Epoch 190/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 22232704275.5930\n",
      "Epoch 191/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 152962907.2257\n",
      "Epoch 192/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 326639639909.9282\n",
      "Epoch 193/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 78679166997.1118\n",
      "Epoch 194/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 417948046760.1922\n",
      "Epoch 195/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 284558286883.1896\n",
      "Epoch 196/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 808914076015.4399\n",
      "Epoch 197/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 5166936109.5888\n",
      "Epoch 198/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 337945051293.0573\n",
      "Epoch 199/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 94900336883.4696\n",
      "Epoch 200/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 574485911173.7716\n",
      "Accuracy: 0.9822064056939501\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.008896797153024912\n",
      "[[[843   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[828   0]\n",
      "  [ 15   0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00        15\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        15\n",
      "   macro avg       0.00      0.00      0.00        15\n",
      "weighted avg       0.00      0.00      0.00        15\n",
      " samples avg       0.00      0.00      0.00        15\n",
      "\n",
      "Train on 3371 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [100537.99, 102183.99, 104903.99, 105231.99, 118109.99, 119173.99, 119775.99, 122307.99, 122465.99, 122595.99, 122597.99, 122657.99, 122663.99, 122673.99, 122689.99, 122693.99, 122717.99, 122755.99, 122775.99, 122787.99, 122799.99, 122833.99, 122835.99, 122845.99, 122849.99, 122861.99, 122885.99, 122899.99, 122901.99, 122931.99, 122945.99, 122947.99, 122983.99, 123011.99, 123021.99, 123043.99, 123069.99, 123071.99, 123077.99, 123095.99, 123097.99, 123101.99, 123111.99, 123123.99, 123137.99, 123171.99, 123191.99, 123219.99, 123317.99, 123331.99, 123335.99, 123367.99, 123373.99, 123385.99, 123387.99, 123401.99, 123415.99, 123439.99, 123441.99, 123447.99, 123449.99, 123453.99, 123457.99, 123463.99, 123465.99, 123473.99, 123475.99, 123477.99, 123509.99, 123521.99, 123559.99, 123581.99, 123603.99, 123623.99, 123627.99, 124399.99, 124403.99, 124419.99, 124445.99, 124475.99, 124479.99, 124481.99, 124489.99, 124507.99, 124579.99, 124613.99, 124651.99, 124665.99, 124673.99, 124677.99, 124705.99, 124771.99, 124799.99, 124831.99, 124887.99, 124961.99, 124963.99, 124979.99, 125003.99, 125049.99, 125057.99, 125109.99, 125123.99, 125145.99, 125149.99, 125165.99, 125185.99, 125195.99, 125221.99, 125225.99, 125231.99, 125237.99, 125261.99, 125265.99, 125285.99, 125335.99, 125773.99, 126065.99, 126085.99, 126095.99, 126161.99, 126169.99, 126183.99, 126233.99, 126317.99, 126345.99, 126349.99, 126359.99, 126373.99, 126393.99, 126419.99, 126433.99, 126439.99, 126467.99, 126491.99, 126495.99, 126497.99, 126501.99, 126515.99, 126523.99, 126531.99, 126543.99, 126549.99, 126563.99, 126571.99, 126577.99, 126583.99, 126597.99, 126599.99, 126603.99, 126613.99, 126633.99, 126639.99, 126643.99, 126651.99, 126653.99, 126655.99, 126665.99, 126679.99, 126681.99, 126687.99, 126705.99, 126707.99, 126725.99, 126729.99, 126745.99, 126759.99, 126761.99, 126777.99, 126787.99, 126805.99, 126815.99, 126833.99, 126843.99, 126845.99, 126865.99, 126873.99, 126913.99, 126933.99, 126939.99, 126943.99, 126947.99, 126965.99, 126969.99, 127609.99, 127673.99, 127681.99, 127739.99, 127751.99, 127757.99, 127759.99, 127801.99, 127813.99, 127821.99, 127827.99, 127883.99, 127935.99, 127957.99, 127963.99, 127965.99, 127975.99, 128005.99, 128013.99, 128019.99, 128045.99, 128071.99, 128073.99, 128107.99, 128111.99, 128129.99, 128133.99, 128149.99, 128153.99, 128155.99, 128163.99, 128165.99, 128169.99, 128175.99, 128183.99, 128189.99, 128193.99, 128207.99, 128209.99, 128233.99, 128245.99, 128265.99, 128293.99, 128327.99, 128367.99, 128381.99, 128399.99, 128423.99, 128425.99, 128427.99, 128437.99, 128441.99, 128509.99, 128511.99, 128541.99, 128607.99, 128613.99, 128633.99, 128655.99, 128691.99, 128885.99, 128915.99, 129245.99, 129307.99, 129343.99, 129445.99, 129471.99, 129473.99, 129487.99, 129511.99, 129557.99, 129579.99, 129591.99, 129593.99, 129609.99, 129665.99, 129693.99, 129703.99, 129707.99, 129779.99, 129793.99, 129803.99, 129837.99, 129847.99, 129869.99, 129889.99, 129895.99, 129957.99, 129977.99, 130001.99, 130003.99, 130005.99, 130031.99, 130053.99, 130085.99, 130089.99, 130105.99, 130111.99, 130125.99, 130143.99, 130163.99, 130175.99, 130177.99, 130181.99, 130193.99, 130195.99, 130199.99, 130243.99, 130271.99, 130275.99, 130281.99, 130285.99, 130313.99, 130347.99, 130395.99, 131039.99, 131051.99, 131147.98, 131181.98, 131187.98, 131203.98, 131257.98, 131265.98, 131277.98, 131297.98, 131311.98, 131321.98, 131343.98, 131345.98, 131347.98, 131355.98, 131385.98, 131405.98, 131409.98, 131423.98, 131431.98, 131445.98, 131447.98, 131451.98, 131455.98, 131471.98, 131479.98, 131489.98, 131517.98, 131521.98, 131527.98, 131539.98, 131543.98, 131551.98, 131567.98, 131569.98, 131585.98, 131603.98, 131611.98, 131621.98, 131637.98, 131643.98, 131645.98, 131647.98, 131657.98, 131669.98, 131679.98, 131681.98, 131683.98, 131723.98, 131725.98, 131755.98, 131781.98, 131785.98, 131801.98, 131815.98, 131843.98, 131845.98, 131859.98, 131873.98, 131899.98, 131903.98, 131913.98, 131921.98, 131947.98, 131967.98, 132041.98, 132051.98, 132057.98, 132077.98, 132097.98, 132115.98, 132661.98, 132719.98, 132975.98, 132979.98, 132989.98, 133003.98, 133037.98, 133051.98, 133077.98, 133089.98, 133105.98, 133109.98, 133143.98, 133145.98, 133163.98, 133177.98, 133183.98, 133191.98, 133237.98, 133241.98, 133245.98, 133269.98, 133271.98, 133273.98, 133323.98, 133333.98, 133363.98, 133367.98, 133391.98, 133395.98, 133413.98, 133431.98, 133461.98, 133475.98, 133487.98, 133489.98, 133501.98, 133511.98, 133531.98, 133533.98, 133539.98, 133543.98, 133551.98, 133555.98, 133559.98, 133573.98, 133575.98, 133579.98, 133583.98, 133585.98, 133591.98, 133593.98, 133595.98, 133599.98, 133601.98, 133611.98, 133625.98, 133631.98, 133637.98, 133665.98, 133667.98, 133679.98, 133681.98, 133683.98, 133685.98, 133699.98, 133711.98, 133713.98, 133719.98, 133721.98, 133733.98, 133757.98, 133907.98, 134331.98, 134343.98, 134433.98, 134477.98, 134495.98, 134531.98, 134555.98, 134569.98, 134587.98, 134603.98, 134663.98, 134755.98, 134763.98, 134843.98, 134875.98, 134881.98, 134887.98, 134889.98, 134911.98, 134917.98, 134929.98, 134943.98, 134947.98, 134959.98, 134973.98, 134977.98, 134983.98, 134993.98, 134995.98, 134997.98, 134999.98, 135009.98, 135013.98, 135025.98, 135029.98, 135037.98, 135039.98, 135041.98, 135043.98, 135051.98, 135055.98, 135057.98, 135059.98, 135063.98, 135091.98, 135103.98, 135105.98, 135107.98, 135113.98, 135115.98, 135121.98, 135127.98, 135131.98, 135135.98, 135137.98, 135143.98, 135145.98, 135147.98, 135157.98, 135159.98, 135163.98, 135177.98, 135181.98, 135197.98, 135201.98, 135211.98, 135213.98, 135221.98, 135223.98, 135229.98, 135231.98, 135237.98, 135239.98, 135241.98, 135243.98, 135251.98, 135259.98, 135283.98, 135307.98, 135327.98, 135355.98, 135379.98, 135405.98, 135407.98, 135427.98, 135433.98, 135445.98, 135467.98, 135477.98, 135485.98, 135491.98, 136153.98, 136227.98, 136299.98, 136309.98, 136313.98, 136367.98, 136469.98, 136483.98, 136605.98, 136623.98, 136641.98, 136685.98, 136689.98, 136697.98, 136699.98, 136701.98, 136707.98, 136717.98, 136729.98, 136763.98, 136777.98, 136779.98, 136781.98, 136793.98, 136795.98, 136803.98, 136813.98, 136841.98, 136843.98, 136849.98, 136859.98, 136863.98, 136865.98, 136873.98, 136879.98, 136891.98, 136929.98, 136949.98, 136951.98, 137021.98, 137025.98, 137035.98, 137045.98, 137057.98, 137063.98, 137067.98, 137097.98, 137099.98, 137103.98, 137105.98, 137107.98, 137141.98, 137143.98, 137145.98, 137175.98, 137181.98, 137187.98, 137191.98, 137215.98, 137225.98, 138141.98, 138227.98, 138239.98, 138271.98, 138283.98, 138291.98, 138355.98, 138369.98, 138371.98, 138391.98, 138431.98, 138433.98, 138435.98, 138453.98, 138455.98, 138459.98, 138465.98, 138467.98, 138483.98, 138489.98, 138491.98, 138523.98, 138579.98, 138585.98, 138591.98, 138623.98, 138629.98, 138647.98, 138649.98, 138651.98, 138653.98, 138661.98, 138679.98, 138687.98, 138703.98, 138721.98, 138735.98, 138747.98, 138759.98, 138769.98, 138773.98, 138779.98, 138783.98, 138787.98, 138799.98, 138801.98, 138805.98, 138809.98, 138813.98, 138815.98, 138855.98, 138871.98, 139059.98, 139097.98, 139149.98, 139163.98, 139185.98, 139901.98, 139985.98, 139987.98, 140079.98, 140111.98, 140115.98, 140121.98, 140133.98, 140135.98, 140141.98, 140151.98, 140153.98, 140173.98, 140175.98, 140177.98, 140183.98, 140237.98, 140251.98, 140261.98, 140275.98, 140279.98, 140283.98, 140303.98, 140307.98, 140321.98, 140323.98, 140329.98, 140357.98, 140383.98, 140393.98, 140399.98, 140403.98, 140405.98, 140417.98, 140421.98, 140427.98, 140435.98, 140443.98, 140445.98, 140457.98, 140469.98, 140479.98, 140493.98, 140503.98, 140509.98, 140515.98, 140521.98, 140525.98, 140527.98, 140529.98, 140533.98, 140537.98, 140547.98, 140571.98, 140593.98, 140595.98, 140603.98, 140627.98, 140629.98, 140673.98, 140689.98, 140701.98, 140713.98, 140745.98, 140757.98, 140765.98, 140793.98, 140795.98, 140799.98, 140807.98, 140851.98, 140867.98, 140873.98, 141079.98, 141099.98, 141109.98, 141243.98, 141271.98, 141285.98, 141309.98, 141361.98, 141387.98, 141399.98, 141405.98, 141409.98, 141429.98, 141437.98, 141439.98, 141451.98, 141483.98, 141517.98, 141793.98, 141821.98, 142009.98, 142017.98, 142041.98, 142043.98, 142065.98, 142069.98, 142075.98, 142085.98, 142105.98, 142161.98, 142165.98, 142169.98, 142177.98, 142191.98, 142213.98, 142217.98, 142221.98, 142229.98, 142245.98, 142255.98, 142259.98, 142265.98, 142277.98, 142295.98, 142345.98, 142395.98, 142407.98, 142447.98] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 1s 210us/sample - loss: 81645171945530752.0000\n",
      "Epoch 2/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 1195041069371.3105\n",
      "Epoch 3/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 409721852439.8457\n",
      "Epoch 4/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 403467396965.0786\n",
      "Epoch 5/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 395673940559.7390\n",
      "Epoch 6/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 386265837221.7051\n",
      "Epoch 7/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 378856745655.6274\n",
      "Epoch 8/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 367161558406.3411\n",
      "Epoch 9/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 360775960758.8680\n",
      "Epoch 10/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 344591398374.3317\n",
      "Epoch 11/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 337071727255.4281\n",
      "Epoch 12/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 320007299390.3483\n",
      "Epoch 13/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 306984410875.9751\n",
      "Epoch 14/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 296567982262.8680\n",
      "Epoch 15/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 277058638457.0513\n",
      "Epoch 16/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 265219013090.9902\n",
      "Epoch 17/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 250778799774.4147\n",
      "Epoch 18/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 234810771483.3391\n",
      "Epoch 19/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 223063875868.3263\n",
      "Epoch 20/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 207218186640.3655\n",
      "Epoch 21/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 194426384494.5713\n",
      "Epoch 22/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 179883291264.3417\n",
      "Epoch 23/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 166405296129.2151\n",
      "Epoch 24/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 148443019669.2258\n",
      "Epoch 25/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 137809594242.2403\n",
      "Epoch 26/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 126330288754.3684\n",
      "Epoch 27/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 113479241495.9217\n",
      "Epoch 28/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 101172644559.9288\n",
      "Epoch 29/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 90445883487.3830\n",
      "Epoch 30/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 82949990184.3251\n",
      "Epoch 31/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 71275089345.2720\n",
      "Epoch 32/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 61602683065.2981\n",
      "Epoch 33/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 53933475150.4479\n",
      "Epoch 34/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 45652864511.2406\n",
      "Epoch 35/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 39658001408.3038\n",
      "Epoch 36/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 32836560457.9674\n",
      "Epoch 37/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 27754299080.9422\n",
      "Epoch 38/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 23720707081.4168\n",
      "Epoch 39/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 18724350543.7389\n",
      "Epoch 40/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 14989751070.1489\n",
      "Epoch 41/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 12135531748.4331\n",
      "Epoch 42/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 9761955525.6007\n",
      "Epoch 43/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 7520489800.9801\n",
      "Epoch 44/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 5979559638.6876\n",
      "Epoch 45/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 4497021353.9199\n",
      "Epoch 46/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 3287883099.3581\n",
      "Epoch 47/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 2530169076.0771\n",
      "Epoch 48/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 1890060695.4660\n",
      "Epoch 49/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1286170306.5061\n",
      "Epoch 50/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1075818718.7375\n",
      "Epoch 51/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 690669114.3708\n",
      "Epoch 52/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 455631375.4542\n",
      "Epoch 53/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 328603063.9691\n",
      "Epoch 54/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 231668781.3468\n",
      "Epoch 55/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 180196361.1890\n",
      "Epoch 56/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 136466110.4954\n",
      "Epoch 57/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 137261224.8330\n",
      "Epoch 58/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 93186249.8036\n",
      "Epoch 59/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 75500106.6864\n",
      "Epoch 60/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 71182846.7541\n",
      "Epoch 61/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 57669204.5838\n",
      "Epoch 62/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 56602025.6755\n",
      "Epoch 63/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 51862473.2376\n",
      "Epoch 64/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 52717740.7570\n",
      "Epoch 65/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 53709871.1433\n",
      "Epoch 66/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 46903368.9908\n",
      "Epoch 67/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 52861018.0481\n",
      "Epoch 68/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 46861740.0344\n",
      "Epoch 69/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 57621479.3604\n",
      "Epoch 70/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 49293826.2355\n",
      "Epoch 71/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 55384112.5808\n",
      "Epoch 72/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 39469770.5417\n",
      "Epoch 73/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 49455802.3684\n",
      "Epoch 74/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 47220154.5939\n",
      "Epoch 75/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 59962265.1605\n",
      "Epoch 76/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 48763781.0279\n",
      "Epoch 77/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 43396943.4910\n",
      "Epoch 78/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 61244599.4613\n",
      "Epoch 79/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 21883812706.3827\n",
      "Epoch 80/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 1593991152241.3623\n",
      "Epoch 81/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1428003633054.1108\n",
      "Epoch 82/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 1095745245554.5962\n",
      "Epoch 83/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1875858301284.0154\n",
      "Epoch 84/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 93087298500.0629\n",
      "Epoch 85/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 2534796986511.3784\n",
      "Epoch 86/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1426735886727.5562\n",
      "Epoch 87/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 441689767752.6004\n",
      "Epoch 88/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 2121562699304.4771\n",
      "Epoch 89/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1605347015975.6794- loss: 1728762254183.350\n",
      "Epoch 90/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1566092460538.5322\n",
      "Epoch 91/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1391650715485.6362\n",
      "Epoch 92/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 418692471900.0416\n",
      "Epoch 93/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 3932132762935.6655\n",
      "Epoch 94/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 5459877147.4530\n",
      "Epoch 95/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1670895169506.2310\n",
      "Epoch 96/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 659385463141.9519\n",
      "Epoch 97/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 1350813911347.1089\n",
      "Epoch 98/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 908998613694.3103\n",
      "Epoch 99/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1734661926241.2815\n",
      "Epoch 100/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 2099989464428.0654\n",
      "Epoch 101/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 27537432320.9872\n",
      "Epoch 102/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2090500011174.0042\n",
      "Epoch 103/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1550949652983.7983\n",
      "Epoch 104/200\n",
      "3371/3371 [==============================] - ETA: 0s - loss: 36779298266.888 - 0s 44us/sample - loss: 912103854668.2456\n",
      "Epoch 105/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 1252335913922.3352\n",
      "Epoch 106/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 1781046808447.9622\n",
      "Epoch 107/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 87758662446.1157\n",
      "Epoch 108/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 2022539345428.1626\n",
      "Epoch 109/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 1024086747285.4916\n",
      "Epoch 110/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 757577912613.2115\n",
      "Epoch 111/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 2171281894768.5837\n",
      "Epoch 112/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 19362437576.0308\n",
      "Epoch 113/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1532820501272.3013\n",
      "Epoch 114/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1687110377958.0088\n",
      "Epoch 115/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 285798684972.2166\n",
      "Epoch 116/200\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: 2303129039214.4619\n",
      "Epoch 117/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 102290112067.2845\n",
      "Epoch 118/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 1074833873110.7635\n",
      "Epoch 119/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 107187980417.9745\n",
      "Epoch 120/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 2056552878816.3323\n",
      "Epoch 121/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 473581035088.2706\n",
      "Epoch 122/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 583100016514.8478\n",
      "Epoch 123/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1283183738969.3076\n",
      "Epoch 124/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 2099715434956.1316\n",
      "Epoch 125/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 14895355966.8799\n",
      "Epoch 126/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1702587766716.1174\n",
      "Epoch 127/200\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: 32000844406.9629\n",
      "Epoch 128/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1083643949847.9216\n",
      "Epoch 129/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1882251584868.6230\n",
      "Epoch 130/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 306878114001.3634\n",
      "Epoch 131/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1645006401.2910\n",
      "Epoch 132/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1467317455995.6475\n",
      "Epoch 133/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 781610871929.3361\n",
      "Epoch 134/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 499757328789.8333\n",
      "Epoch 135/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1105886112084.2195\n",
      "Epoch 136/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 11990976254.9748\n",
      "Epoch 137/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1608070051893.3682\n",
      "Epoch 138/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 4430707354.9594\n",
      "Epoch 139/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 1205078245239.6084\n",
      "Epoch 140/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 1004572678798.7708\n",
      "Epoch 141/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 42727498265.0608\n",
      "Epoch 142/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 898948329650.3115\n",
      "Epoch 143/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 643941930957.2709\n",
      "Epoch 144/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1094315475071.2786\n",
      "Epoch 145/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 6428820259.4648\n",
      "Epoch 146/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 1069330075946.2997\n",
      "Epoch 147/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 786860194948.7463\n",
      "Epoch 148/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 29731190270.3625\n",
      "Epoch 149/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 1231143796599.9502\n",
      "Epoch 150/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 353998722.0884\n",
      "Epoch 151/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1525214832403.9680\n",
      "Epoch 152/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 867278040.3204\n",
      "Epoch 153/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 638511776108.8627\n",
      "Epoch 154/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1001027817334.7731\n",
      "Epoch 155/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 20082595299.1753\n",
      "Epoch 156/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1410048045221.6765\n",
      "Epoch 157/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 5427783542.5429\n",
      "Epoch 158/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 592149282504.8853\n",
      "Epoch 159/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 169242682944.9872\n",
      "Epoch 160/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 1336349591304.7712\n",
      "Epoch 161/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 6421079502.3625\n",
      "Epoch 162/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 616499320831.2406\n",
      "Epoch 163/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 172215407371.8659\n",
      "Epoch 164/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 762040347658.3186\n",
      "Epoch 165/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 14295766530.3542\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 0s 44us/sample - loss: 798818042458.8359\n",
      "Epoch 167/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 832184404887.4957\n",
      "Epoch 168/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 84914426839.6559\n",
      "Epoch 169/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 734337853765.5627\n",
      "Epoch 170/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 83014482186.1003\n",
      "Epoch 171/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 885879449787.9371\n",
      "Epoch 172/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 32572116598.9700\n",
      "Epoch 173/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 552050611576.7025\n",
      "Epoch 174/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 55238672193.4571\n",
      "Epoch 175/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 686814592478.2510\n",
      "Epoch 176/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 640499689.0344\n",
      "Epoch 177/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 1306312113614.3435\n",
      "Epoch 178/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 694180539.9371\n",
      "Epoch 179/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 287239671.3738\n",
      "Epoch 180/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 693924501872.0427\n",
      "Epoch 181/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 10832922118.4551\n",
      "Epoch 182/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 405881827645.7217\n",
      "Epoch 183/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 15451412708.8888\n",
      "Epoch 184/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 661042341395.4032\n",
      "Epoch 185/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 2617436716.2361\n",
      "Epoch 186/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 494683535190.1940\n",
      "Epoch 187/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 286854265299.8019\n",
      "Epoch 188/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 26057357808.1471\n",
      "Epoch 189/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 602974693527.7128\n",
      "Epoch 190/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 1305278620.9718\n",
      "Epoch 191/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 400284618550.1465\n",
      "Epoch 192/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 267379293028.3951\n",
      "Epoch 193/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 61184868727.5811\n",
      "Epoch 194/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 738278952148.4130\n",
      "Epoch 195/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 1894018602.2972\n",
      "Epoch 196/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 135176903.8190\n",
      "Epoch 197/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 364688783359.3782\n",
      "Epoch 198/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 5599727584.1044\n",
      "Epoch 199/200\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: 188380992217.0418\n",
      "Epoch 200/200\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: 300610329177.6375\n",
      "Accuracy: 0.9833926453143536\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.00830367734282325\n",
      "[[[843   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[829   0]\n",
      "  [ 14   0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00        14\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        14\n",
      "   macro avg       0.00      0.00      0.00        14\n",
      "weighted avg       0.00      0.00      0.00        14\n",
      " samples avg       0.00      0.00      0.00        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-11170.542, -11294.542, -11324.542, -11340.042, -11357.042, -11357.542, -11363.542, -11370.042, -11417.042, -11429.042, -11432.042, -11465.042, -11468.542, -11469.042, -11469.542, -11472.542, -11490.042, -11495.042, -11497.542, -11505.542, -11528.542, -11529.042, -11541.542, -11544.542, -11559.042, -11561.042, -11573.542, -11578.042, -11600.042, -11604.542, -11607.042, -11609.542, -11613.542, -11616.542, -11628.042, -11633.542, -11635.042, -11641.042, -11649.042, -11654.042, -11659.542, -11673.542, -11678.042, -11683.042, -11685.042, -11692.542, -11713.042, -11715.042, -11718.542, -11719.042, -11724.042, -11734.542, -11750.042, -11781.042, -11785.042, -11796.042, -11800.042, -11800.542, -11801.542, -11809.542, -11821.542, -11822.042, -11825.042, -11835.042, -11849.042, -11867.042, -11887.042, -11929.542, -11941.042, -11949.542, -11958.542, -12011.042, -12032.542, -1318.0416, -13716.542, -1373.5416, -1391.0416, -1401.0416, -14114.542, -14151.042, -14164.542, -14174.042, -14194.042, -14232.542, -14235.042, -14253.042, -14265.042, -14265.542, -14274.542, -14290.042, -14291.542, -14306.042, -14307.042, -14307.542, -14315.042, -14337.542, -14344.542, -14345.542, -14367.042, -14372.042, -14378.542, -14396.542, -14410.042, -14415.042, -14432.042, -14440.042, -14447.542, -14452.542, -14458.042, -14464.542, -14465.042, -14478.542, -1448.5416, -14480.042, -14499.042, -14499.542, -14505.042, -1451.0416, -14516.542, -14522.542, -14523.042, -14525.542, -14538.542, -14547.542, -14555.542, -14560.042, -14565.042, -14566.542, -14568.042, -14568.542, -14574.542, -14576.542, -14578.042, -14578.542, -14579.042, -14579.542, -14582.042, -14583.542, -14588.042, -14589.042, -14603.542, -14610.042, -14614.042, -14619.042, -14621.542, -14639.042, -14640.042, -14644.542, -1465.5416, -14650.042, -14662.542, -1467.5416, -14670.042, -14674.542, -14676.042, -14676.542, -14685.542, -14688.542, -14695.042, -14715.042, -14719.542, -14722.542, -14723.542, -14746.042, -14751.042, -14754.042, -14758.042, -14762.042, -14794.042, -14810.542, -14817.042, -14822.542, -14837.542, -14842.042, -14847.542, -14851.042, -14865.042, -14868.042, -14868.542, -14876.042, -14879.042, -14887.042, -1489.0416, -14897.542, -14899.042, -14911.042, -14912.042, -14913.542, -14914.542, -14919.042, -14922.042, -14922.542, -14924.542, -14930.042, -14942.042, -14951.042, -14954.042, -14961.042, -14970.042, -14987.542, -14988.042, -14995.542, -15005.542, -15007.542, -15010.042, -15012.042, -15033.042, -15040.042, -1505.5416, -15053.042, -15056.042, -15070.042, -15119.042, -15213.042, -1534.0416, -1534.5416, -1543.0416, -1559.0416, -1566.5416, -1594.0416, -1626.0416, -1658.5416, -1669.5416, -1672.0416, -1680.5416, -1703.0416, -1708.5416, -17196.041, -1720.0416, -17223.041, -17242.041, -17269.541, -1727.0416, -17272.041, -17273.041, -17274.541, -17279.541, -17281.041, -17287.541, -17324.041, -17330.041, -17337.041, -17338.041, -17340.041, -17348.041, -17353.041, -17366.041, -17374.541, -17394.041, -17398.541, -17405.541, -17408.041, -17409.541, -17417.041, -17426.041, -17427.041, -17433.041, -17436.541, -17437.041, -17450.541, -17455.041, -17457.041, -17458.041, -17461.541, -17464.041, -17466.541, -17472.541, -17476.041, -17478.541, -17479.041, -17483.041, -17487.541, -17488.541, -17493.541, -17497.041, -17504.041, -17506.041, -17509.041, -17509.541, -17518.041, -17522.041, -17528.041, -17537.541, -17541.041, -17545.541, -17549.041, -17556.041, -17560.041, -17560.541, -17568.041, -17569.041, -17592.541, -17595.041, -17597.041, -17598.041, -17612.041, -17633.041, -17640.541, -17642.041, -17644.541, -17654.041, -17656.541, -17661.541, -17668.541, -17673.541, -17681.041, -17691.041, -17696.041, -17705.041, -17706.541, -17709.041, -17721.041, -17724.541, -17728.041, -17729.041, -17729.541, -17734.041, -17734.541, -17737.041, -17742.041, -17752.041, -17818.041, -17818.541, -17828.041, -17848.041, -1786.5416, -17869.541, -1787.0416, -17881.541, -17884.041, -17886.541, -17894.541, -17897.541, -17909.041, -17924.041, -1794.5416, -17940.541, -17942.541, -17948.041, -1799.5416, -18068.541, -18158.541, -18169.041, -1824.5416, -18266.041, -18283.041, -1850.0416, -1854.0416, -1873.0416, -1904.5416, -1928.0416, -1937.0416, -1952.5416, -1953.0416, -1959.5416, -1964.5416, -1967.5416, -2007.0416, -2013.0416, -2018.5416, -20236.541, -2027.5416, -20290.041, -20362.041, -20437.541, -20474.041, -20520.541, -20525.041, -20527.541, -20533.041, -2054.0417, -20563.541, -20568.041, -20589.041, -20599.041, -20601.541, -20617.041, -20617.541, -20619.041, -20626.041, -20632.541, -20658.041, -20684.041, -20708.541, -20718.541, -20747.041, -20755.041, -20780.041, -20780.541, -20792.541, -20797.041, -20804.541, -20805.541, -20811.041, -20843.041, -20871.041, -20881.041, -20922.541, -20937.541, -2094.5417, -20951.041, -20957.041, -2096.5417, -20961.541, -20967.041, -20976.041, -20984.541, -21017.041, -21048.041, -21062.041, -21067.041, -21071.041, -21097.041, -21102.541, -2111.5417, -21149.541, -21151.541, -21155.541, -21161.041, -21191.041, -21194.541, -21257.041, -21296.541, -21298.041, -21323.541, -21338.541, -21340.041, -21344.041, -2135.0417, -21410.041, -21451.541, -21463.041, -21480.041, -2168.5417, -2189.0417, -2196.0417, -2196.5417, -2213.0417, -2214.5417, -2247.5417, -2261.0417, -2312.0417, -2369.5417, -2377.5417, -2388.5417, -23911.541, -23923.541, -23952.041, -23991.541, -24031.041, -24036.541, -24114.041, -24124.041, -24137.541, -24139.541, -24160.541, -24167.541, -24224.541, -24227.541, -24246.041, -24263.041, -24270.041, -24297.541, -24313.541, -24328.541, -24392.541, -24448.041, -2445.0417, -24470.041, -24482.041, -24500.041, -24505.041, -24512.041, -24520.541, -24531.041, -24589.041, -24618.041, -2465.5417, -24667.541, -2518.5417, -2582.5417, -26922.041, -26927.541, -26942.541, -26949.041, -26992.041, -26995.041, -27019.541, -27020.041, -27102.041, -27112.541, -27124.541, -27145.041, -27152.041, -27152.541, -27154.041, -27210.541, -27220.041, -27286.041, -27299.041, -27353.541, -27400.041, -27406.541, -27415.541, -27421.041, -27443.041, -27452.541, -27467.041, -27467.541, -27470.041, -27488.541, -27555.541, -27576.541, -27621.541, -27688.041, -27770.541, -27806.541, -27815.041, -27886.041, -27899.041, -4560.5415, -4629.5415, -4756.0415, -4823.5415, -4828.0415, -4848.0415, -4901.0415, -4905.0415, -4921.5415, -4937.0415, -4939.5415, -4968.5415, -4972.0415, -4975.0415, -4993.5415, -4994.0415, -5001.5415, -5050.0415, -5088.0415, -5107.5415, -5132.0415, -5217.5415, -5282.0415, -5294.5415, -5295.0415, -5318.5415, -5320.0415, -5325.5415, -5327.5415, -5355.5415, -5411.0415, -5413.5415, -5418.5415, -5426.5415, -5444.0415, -5457.5415, -5481.0415, -5508.5415, -5521.0415, -5526.0415, -5632.5415, -5692.5415, -5758.0415, -8014.5415, -8041.0415, -8118.5415, -8121.0415, -8198.542, -8210.042, -8259.042, -8263.042, -8292.042, -8314.542, -8348.042, -8371.542, -8373.042, -8428.042, -8511.042, -8514.042, -8520.042, -8524.042, -8527.542, -8529.542, -8559.542, -8568.542, -8581.042, -8587.042, -8607.042, -8619.042, -8621.542, -8627.542, -8630.042, -8643.542, -8664.042, -8675.542, -8677.542, -8693.542, -8703.042, -8720.042, -8721.542, -8725.042, -8738.542, -8762.042, -8772.042, -8773.542, -8877.542, -8897.042, -8933.542, -8948.042, -9187.042, 1007.9583, 1021.9583, 1033.4584, 1035.4584, 1053.9584, 10549.458, 1058.9584, 1067.4584, 10681.958, 1079.4584, 1091.9584, 1097.9584, 1100.9584, 1106.4584, 1106.9584, 1121.4584, 1124.4584, 1124.9584, 1127.9584, 1155.9584, 1170.9584, 1171.4584, 1184.9584, 1194.4584, 1200.4584, 1205.9584, 1206.9584, 1218.9584, 1221.4584, 1222.9584, 1229.4584, 1233.4584, 1233.9584, 1234.4584, 1241.4584, 1241.9584, 1242.9584, 1244.9584, 1260.9584, 1281.9584, 1285.4584, 1291.4584, 1292.9584, 1296.4584, 1302.9584, 1304.4584, 1308.4584, 1311.9584, 1321.9584, 13276.958, 1334.4584, 13380.958, 1349.4584, 13546.958, 1366.4584, 13694.458, 13715.458, 1373.9584, 1388.9584, 1396.9584, 1397.4584, 1406.4584, 1414.9584, 1420.9584, 1428.9584, 1431.4584, 1440.9584, 1453.9584, 1458.4584, 1467.9584, 1481.4584, 1496.4584, 1517.9584, 1520.9584, 1523.9584, 1528.9584, 1541.9584, 1543.9584, 1556.9584, 1575.9584, 1577.9584, 1605.9584, 1621.4584, 1625.9584, 1633.9584, 1637.4584, 1638.9584, 16673.459, 1677.9584, 1681.9584, 1688.4584, 1721.4584, 1747.9584, 1782.9584, 1811.4584, 1838.4584, 3720.4583, 3781.9583, 3797.9583, 3852.9583, 3901.4583, 3960.9583, 3966.4583, 3980.9583, 4021.4583, 4040.9583, 4061.4583, 4076.9583, 4081.9583, 4098.9585, 4120.4585, 4123.4585, 4150.9585, 4180.9585, 4186.4585, 4194.4585, 4199.4585, 4210.9585, 4212.4585, 4218.9585, 4225.4585, 4231.9585, 4245.4585, 4248.4585, 4269.4585, 4278.4585, 4286.9585, 4293.9585, 4326.9585, 4331.4585, 4345.9585, 4363.4585, 4374.9585, 4375.9585, 4405.4585, 4439.9585, 4457.4585, 4457.9585, 4475.9585, 4479.9585, 4480.4585, 4498.4585, 4539.4585, 4542.9585, 456.95834, 4563.4585, 4574.9585, 4584.4585, 4587.4585, 4659.9585, 4686.4585, 4696.9585, 4717.4585, 7002.4585, 7082.9585, 7121.4585, 7202.4585, 7232.9585, 729.4583, 7346.9585, 7358.4585, 7395.9585, 7527.4585, 7540.9585, 7556.9585, 7573.4585, 7592.4585, 7594.9585, 7602.9585, 7610.9585, 7612.9585, 7657.9585, 7661.9585, 7710.9585, 7747.4585, 7766.4585, 7779.9585, 7780.4585, 7800.4585, 7809.9585, 7817.9585, 7838.4585, 7853.9585, 7867.9585, 7923.9585, 7940.9585, 7949.4585, 795.4583, 7971.4585, 7977.4585, 8006.4585, 802.9583, 806.9583, 8086.4585, 8090.4585, 8146.4585, 8155.9585, 8187.4585, 848.4583, 852.9583, 864.9583, 865.9583, 877.9583, 884.4583, 884.9583, 917.9583, 925.9583, 928.9583, 934.4583, 944.4583, 963.4583, 966.4583, 971.4583, 981.4583, 985.9583, 994.4583] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3371 samples\n",
      "Epoch 1/200\n",
      "3371/3371 [==============================] - 1s 170us/sample - loss: 4392230808086811.0000\n",
      "Epoch 2/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 189281660807.7081\n",
      "Epoch 3/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 137791068905.4453\n",
      "Epoch 4/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 132670236774.6734\n",
      "Epoch 5/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 127133501807.5586\n",
      "Epoch 6/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 119342888048.0902\n",
      "Epoch 7/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 113168893641.5497\n",
      "Epoch 8/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 107660882628.3857\n",
      "Epoch 9/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 98260706848.6550\n",
      "Epoch 10/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 90598479169.0822\n",
      "Epoch 11/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 86636399780.9457\n",
      "Epoch 12/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 75399153692.5541\n",
      "Epoch 13/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 69134209341.1332\n",
      "Epoch 14/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 60153485302.8870\n",
      "Epoch 15/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 54362834326.1370\n",
      "Epoch 16/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 49567531918.0872\n",
      "Epoch 17/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 41630724934.3981\n",
      "Epoch 18/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 35674022624.9398\n",
      "Epoch 19/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 30632668505.9911\n",
      "Epoch 20/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 26951303063.5040\n",
      "Epoch 21/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 23551411870.4147\n",
      "Epoch 22/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 18024183050.4040\n",
      "Epoch 23/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 15005413126.6069\n",
      "Epoch 24/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 12400387889.2863\n",
      "Epoch 25/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 10034506296.5007\n",
      "Epoch 26/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 8165936358.8632\n",
      "Epoch 27/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 7467784784.0427\n",
      "Epoch 28/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 5560738199.2002\n",
      "Epoch 29/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 3906448301.1474\n",
      "Epoch 30/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 2894381558.1276\n",
      "Epoch 31/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 2516103896.0166\n",
      "Epoch 32/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 1929761884.1175\n",
      "Epoch 33/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1545648200.8282\n",
      "Epoch 34/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1159143031.5325\n",
      "Epoch 35/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 899457056.0854\n",
      "Epoch 36/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 778980820.9220\n",
      "Epoch 37/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 683489665.4429\n",
      "Epoch 38/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 639639910.9677\n",
      "Epoch 39/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 609726234.2094\n",
      "Epoch 40/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 518106012.2219\n",
      "Epoch 41/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 497149024.1827\n",
      "Epoch 42/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 432511206.0279\n",
      "Epoch 43/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 469207069.0715\n",
      "Epoch 44/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 352553285.9282\n",
      "Epoch 45/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 426634928.2895\n",
      "Epoch 46/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 358270794.4705\n",
      "Epoch 47/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 322464584.4865\n",
      "Epoch 48/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 561796182.0991\n",
      "Epoch 49/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 409653415.2964\n",
      "Epoch 50/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 313610788.0961\n",
      "Epoch 51/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 309938998.6141\n",
      "Epoch 52/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 361104487.8600\n",
      "Epoch 53/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 362269041.8938\n",
      "Epoch 54/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 346708325.5271\n",
      "Epoch 55/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 445447637.9377\n",
      "Epoch 56/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 448592328.5007\n",
      "Epoch 57/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 340252907.4577\n",
      "Epoch 58/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 777452665.0133\n",
      "Epoch 59/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 1371604291485.6553\n",
      "Epoch 60/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 13717437672.1543\n",
      "Epoch 61/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 805303388080.7167\n",
      "Epoch 62/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 935393891599.5680\n",
      "Epoch 63/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 528859159997.0193s - loss: 15513032275.0\n",
      "Epoch 64/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 506899622334.0825\n",
      "Epoch 65/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 869800568488.2871\n",
      "Epoch 66/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 959703018183.4232\n",
      "Epoch 67/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1163291542755.0662\n",
      "Epoch 68/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 30119183940.8603\n",
      "Epoch 69/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 1147272008462.8086\n",
      "Epoch 70/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 351630484861.6648\n",
      "Epoch 71/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 724291757606.0754\n",
      "Epoch 72/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 1199579893605.2827\n",
      "Epoch 73/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 24750494526.5001\n",
      "Epoch 74/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1370862409531.8420\n",
      "Epoch 75/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 2197943954.1311\n",
      "Epoch 76/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 1004709329748.3334\n",
      "Epoch 77/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 30039993567.0223\n",
      "Epoch 78/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1108432357616.9705\n",
      "Epoch 79/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 962349991412.0629\n",
      "Epoch 80/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 160108058503.8280\n",
      "Epoch 81/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 914931183935.2667\n",
      "Epoch 82/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 26113309948.1839\n",
      "Epoch 83/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 833445744054.4883\n",
      "Epoch 84/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 823128527354.4563\n",
      "Epoch 85/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 91305906905.3960\n",
      "Epoch 86/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 903243414142.7091\n",
      "Epoch 87/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 3387742066.9950\n",
      "Epoch 88/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 894357759562.9546\n",
      "Epoch 89/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 287711946738.3400\n",
      "Epoch 90/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 745090892636.6870\n",
      "Epoch 91/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 29386799044.2717\n",
      "Epoch 92/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 585682519291.3676\n",
      "Epoch 93/200\n",
      "3371/3371 [==============================] - 0s 38us/sample - loss: 484502889322.6223\n",
      "Epoch 94/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 856010185976.5197\n",
      "Epoch 95/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 8417450833.5616\n",
      "Epoch 96/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 589014377208.9374\n",
      "Epoch 97/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 156885025766.8822\n",
      "Epoch 98/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 788475909933.0525\n",
      "Epoch 99/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 221317977685.4536\n",
      "Epoch 100/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 306483391687.5182\n",
      "Epoch 101/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 724705018898.2450\n",
      "Epoch 102/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 2643592339.8588\n",
      "Epoch 103/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 465417698552.0475\n",
      "Epoch 104/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 1108376050649.4595\n",
      "Epoch 105/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 8143230841.4577\n",
      "Epoch 106/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 295227506.1359\n",
      "Epoch 107/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 589593089238.8585\n",
      "Epoch 108/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 3165610831.7342\n",
      "Epoch 109/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 617769768332.7179\n",
      "Epoch 110/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 219711618742.5642\n",
      "Epoch 111/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 428871588083.2076\n",
      "Epoch 112/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 46526273501.2186\n",
      "Epoch 113/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 361411669486.6001\n",
      "Epoch 114/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 383051591729.2483\n",
      "Epoch 115/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 583582420455.6986\n",
      "Epoch 116/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 20541201126.5880\n",
      "Epoch 117/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 704090820867.9490\n",
      "Epoch 118/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 138055285372.3358\n",
      "Epoch 119/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 234019141.1166\n",
      "Epoch 120/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 360802658355.9632\n",
      "Epoch 121/200\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: 63229041144.4818\n",
      "Epoch 122/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 274553440988.8389\n",
      "Epoch 123/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 371487239594.6034\n",
      "Epoch 124/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 309431563332.3666\n",
      "Epoch 125/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 21860372108.8721\n",
      "Epoch 126/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 405769681476.2527\n",
      "Epoch 127/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 314749276652.6253\n",
      "Epoch 128/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 31597534980.7084\n",
      "Epoch 129/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 238912179113.2305\n",
      "Epoch 130/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 406616253267.3082\n",
      "Epoch 131/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 15416887722.4135\n",
      "Epoch 132/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 333248198298.9973\n",
      "Epoch 133/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 229989462839.8742\n",
      "Epoch 134/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 49956597270.1276\n",
      "Epoch 135/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 318758536558.1276\n",
      "Epoch 136/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 321993141638.2652\n",
      "Epoch 137/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 5597893894.4551\n",
      "Epoch 138/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 257160824460.9006\n",
      "Epoch 139/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 318943541519.6060\n",
      "Epoch 140/200\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: 43329174451.5076\n",
      "Epoch 141/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 203195697630.9843\n",
      "Epoch 142/200\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: 314574002923.8375\n",
      "Epoch 143/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 166080886009.7348\n",
      "Epoch 144/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 43934658862.6283\n",
      "Epoch 145/200\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: 312335164997.7716\n",
      "Epoch 146/200\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: 103899695366.4171\n",
      "Epoch 147/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 283532523586.0848\n",
      "Epoch 148/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 267633394655.7912\n",
      "Epoch 149/200\n",
      "3371/3371 [==============================] - 0s 37us/sample - loss: 234728426173.8499\n",
      "Epoch 150/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 247094856.5102\n",
      "Epoch 151/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 174610721107.7924\n",
      "Epoch 152/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 388986581844.7938\n",
      "Epoch 153/200\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: 17017319831.3664\n",
      "Epoch 154/200\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: 21719627540.2243\n",
      "Epoch 155/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 255789234984.3441\n",
      "Epoch 156/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 234937725067.2773\n",
      "Epoch 157/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 5558686270.3435\n",
      "Epoch 158/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 260354628213.9828\n",
      "Epoch 159/200\n",
      "3371/3371 [==============================] - 0s 36us/sample - loss: 191921209030.5713\n",
      "Epoch 160/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 63316128003.9252\n",
      "Epoch 161/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 262814748112.6574\n",
      "Epoch 162/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 1941380275.8208\n",
      "Epoch 163/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 557445638430.4526\n",
      "Epoch 164/200\n",
      "3371/3371 [==============================] - 0s 36us/sample - loss: 1672118391.0611\n",
      "Epoch 165/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 20800497.8956\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371/3371 [==============================] - 0s 35us/sample - loss: 57333142187.8048\n",
      "Epoch 167/200\n",
      "3371/3371 [==============================] - 0s 36us/sample - loss: 190168085249.9766\n",
      "Epoch 168/200\n",
      "3371/3371 [==============================] - 0s 38us/sample - loss: 209480105242.5441\n",
      "Epoch 169/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 66090076377.0134\n",
      "Epoch 170/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 10465865986.3260\n",
      "Epoch 171/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 294820379320.3963\n",
      "Epoch 172/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 46410811.3305\n",
      "Epoch 173/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 221684425190.9914\n",
      "Epoch 174/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 2405381405.6268\n",
      "Epoch 175/200\n",
      "3371/3371 [==============================] - 0s 36us/sample - loss: 177387642756.8128\n",
      "Epoch 176/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 193290881499.5811\n",
      "Epoch 177/200\n",
      "3371/3371 [==============================] - 0s 36us/sample - loss: 2066730134.1282\n",
      "Epoch 178/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 845292315715.2856\n",
      "Epoch 179/200\n",
      "3371/3371 [==============================] - 0s 38us/sample - loss: 890381212.4081\n",
      "Epoch 180/200\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: 2242012.2150\n",
      "Epoch 181/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 1528354.0280\n",
      "Epoch 182/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 2397734.0963\n",
      "Epoch 183/200\n",
      "3371/3371 [==============================] - 0s 36us/sample - loss: 8570325.6762\n",
      "Epoch 184/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 125825886868.8081\n",
      "Epoch 185/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 109359167753.7775\n",
      "Epoch 186/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 78171577329.0727\n",
      "Epoch 187/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 76697725975.1348\n",
      "Epoch 188/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 156411323067.5289\n",
      "Epoch 189/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 2076575406.3139\n",
      "Epoch 190/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 110246817607.9264\n",
      "Epoch 191/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 191553058044.8792\n",
      "Epoch 192/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 1258933720.0011\n",
      "Epoch 193/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 143529792236.6348\n",
      "Epoch 194/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 1941578677.1403\n",
      "Epoch 195/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 115926473706.5132\n",
      "Epoch 196/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 99308344810.1121\n",
      "Epoch 197/200\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: 5922825420.6965\n",
      "Epoch 198/200\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: 124525925313.9697\n",
      "Epoch 199/200\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: 118429955957.3824\n",
      "Epoch 200/200\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: 1479115486.8988\n",
      "Accuracy: 0.9810201660735468\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.009489916963226572\n",
      "[[[843   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[827   0]\n",
      "  [ 16   0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        16\n",
      "   macro avg       0.00      0.00      0.00        16\n",
      "weighted avg       0.00      0.00      0.00        16\n",
      " samples avg       0.00      0.00      0.00        16\n",
      "\n",
      "Train on 3372 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-37477.965, -37481.965, -37489.965, -37493.965, -37497.965, -37503.965, -37505.965, -37515.965, -37517.965, -37519.965, -37521.965, -37527.965, -37531.965, -37533.965, -37535.965, -37537.965, -37539.965, -37541.965, -37543.965, -37545.965, -37547.965, -37549.965, -37551.965, -37553.965, -37555.965, -37557.965, -37559.965, -37561.965, -37563.965, -37565.965, -37567.965, -37569.965, -37571.965, -37573.965, -37575.965, -37577.965, -37579.965, -37581.965, -37583.965, -37585.965, -37587.965, -37589.965, -37591.965, -37593.965, -37595.965, -37597.965, -37599.965, -37601.965, -37603.965, -37605.965, -37607.965, -37609.965, -37611.965, -37613.965, -37615.965, -37617.965, -37619.965, -37621.965, -37623.965, -37625.965, -37627.965, -37629.965, -37631.965, -37633.965, -37635.965, -37637.965, -37639.965, -37641.965, -37643.965, -37645.965, -37647.965, -37649.965, -37651.965, -37653.965, -37655.965, -37657.965, -37659.965, -37661.965, -37663.965, -37665.965, -37667.965, -37669.965, -37671.965, -37673.965, -37675.965, -37677.965, -37679.965, -37681.965, -37683.965, -37685.965, -37687.965, -37689.965, -37691.965, -37693.965, -37695.965, -37697.965, -37699.965, -37701.965, -37703.965, -37705.965, -37707.965, -37709.965, -37711.965, -37713.965, -37715.965, -37717.965, -37719.965, -37721.965, -37723.965, -37725.965, -37727.965, -37729.965, -37731.965, -37733.965, -37735.965, -37737.965, -37739.965, -37741.965, -37743.965, -37745.965, -37747.965, -37749.965, -37751.965, -37753.965, -37755.965, -37757.965, -37759.965, -37761.965, -37763.965, -37765.965, -37767.965, -37769.965, -37771.965, -37773.965, -37775.965, -37777.965, -37779.965, -37781.965, -37785.965, -37787.965, -37789.965, -37793.965, -37795.965, -37799.965, -37801.965, -37803.965, -37805.965, -37809.965, -37811.965, -37819.965, -37821.965, -37823.965, -37829.965, -37845.965, -38007.965, -38009.965, -38013.965, -38019.965, -38021.965, -38023.965, -38025.965, -38029.965, -38031.965, -38035.965, -38039.965, -38043.965, -38045.965, -38049.965, -38053.965, -38057.965, -38061.965, -38063.965, -38065.965, -38067.965, -38071.965, -38073.965, -38083.965, -38087.965, -38781.965, -38837.965, -38853.965, -38857.965, -38897.965, -38913.965, -38931.965, -38963.965, -38965.965, -39061.965, -39069.965, -39083.965] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 1s 163us/sample - loss: 120451644199803.0625\n",
      "Epoch 2/200\n",
      "3372/3372 [==============================] - 0s 40us/sample - loss: 2168577028.4603\n",
      "Epoch 3/200\n",
      "3372/3372 [==============================] - 0s 60us/sample - loss: 90328964.3369\n",
      "Epoch 4/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 109919671.6204\n",
      "Epoch 5/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 98483754.7521\n",
      "Epoch 6/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 105183524.1566\n",
      "Epoch 7/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 107115266.2017\n",
      "Epoch 8/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 90033660.9442\n",
      "Epoch 9/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 127402891.1032\n",
      "Epoch 10/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 117246107.5967\n",
      "Epoch 11/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 123688459.7200\n",
      "Epoch 12/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 121582532.3843\n",
      "Epoch 13/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 127500371.9668\n",
      "Epoch 14/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 135357678.3867\n",
      "Epoch 15/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 145852633.2242 - loss: 135909534.000\n",
      "Epoch 16/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 164180697.3333\n",
      "Epoch 17/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 233718802.3772\n",
      "Epoch 18/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 349219107.0273\n",
      "Epoch 19/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 333381380.3488\n",
      "Epoch 20/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 2168062174423.7056\n",
      "Epoch 21/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 822731533929.0724\n",
      "Epoch 22/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1171559365208.6738\n",
      "Epoch 23/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 1402575033710.2349\n",
      "Epoch 24/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 3530757521295.7725\n",
      "Epoch 25/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 576655051637.5231\n",
      "Epoch 26/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 5611853131.9193\n",
      "Epoch 27/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1312093324274.9419\n",
      "Epoch 28/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 1165502429660.7734\n",
      "Epoch 29/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1800689712776.3511\n",
      "Epoch 30/200\n",
      "3372/3372 [==============================] - 0s 42us/sample - loss: 1082581518864.0189\n",
      "Epoch 31/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 363838640981.0296\n",
      "Epoch 32/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1606453445502.0261\n",
      "Epoch 33/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 2253128603799.0034\n",
      "Epoch 34/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 12496328044.7165\n",
      "Epoch 35/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 1495306442370.2776\n",
      "Epoch 36/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 1121786795626.5149\n",
      "Epoch 37/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 2079318863722.5908\n",
      "Epoch 38/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 65829028024.4840\n",
      "Epoch 39/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 1589498375803.0652\n",
      "Epoch 40/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 19807462397.2289\n",
      "Epoch 41/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 2342694794826.6665\n",
      "Epoch 42/200\n",
      "3372/3372 [==============================] - 0s 42us/sample - loss: 756654946.1068\n",
      "Epoch 43/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 2280648137766.5669\n",
      "Epoch 44/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 3266256012.8304\n",
      "Epoch 45/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 1395227307831.2693\n",
      "Epoch 46/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 1281724250002.8281\n",
      "Epoch 47/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 1076475898584.0665\n",
      "Epoch 48/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 1102823091663.7153\n",
      "Epoch 49/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 131187231711.6584\n",
      "Epoch 50/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1823108050749.4187\n",
      "Epoch 51/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1450874840511.8862\n",
      "Epoch 52/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 476322715976.9585\n",
      "Epoch 53/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 1229934081031.5918\n",
      "Epoch 54/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 89337526876.6216\n",
      "Epoch 55/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 1436064087587.8340\n",
      "Epoch 56/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1125123468039.1743\n",
      "Epoch 57/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 877270115469.3618\n",
      "Epoch 58/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 227202804326.6429\n",
      "Epoch 59/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1044170631691.2361\n",
      "Epoch 60/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 821966730229.6749\n",
      "Epoch 61/200\n",
      "3372/3372 [==============================] - 0s 53us/sample - loss: 740702952315.5206\n",
      "Epoch 62/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1149871446165.3333\n",
      "Epoch 63/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1382763754196.4604\n",
      "Epoch 64/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 2564921005.7699\n",
      "Epoch 65/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 1789503754462.8232\n",
      "Epoch 66/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 586714531.6453\n",
      "Epoch 67/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1760911233092.7925\n",
      "Epoch 68/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1096063074.6667\n",
      "Epoch 69/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 1821985441748.8779\n",
      "Epoch 70/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 12117966077.1151\n",
      "Epoch 71/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 642667469969.7651\n",
      "Epoch 72/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1288540867321.4709\n",
      "Epoch 73/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 3297270451.2598\n",
      "Epoch 74/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1427071419922.0688\n",
      "Epoch 75/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 25339317562.7995\n",
      "Epoch 76/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 827661839620.7450\n",
      "Epoch 77/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 1059402777330.9419\n",
      "Epoch 78/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1103144111381.9028\n",
      "Epoch 79/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 148180089627.9383\n",
      "Epoch 80/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 655723225373.0012\n",
      "Epoch 81/200\n",
      "3372/3372 [==============================] - 0s 40us/sample - loss: 1153178942734.0735\n",
      "Epoch 82/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 12893116465.8410\n",
      "Epoch 83/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 806088769417.8695\n",
      "Epoch 84/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 1007227006697.8315\n",
      "Epoch 85/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 11752901844.0237\n",
      "Epoch 86/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1106430049303.3452\n",
      "Epoch 87/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 309186725850.5908\n",
      "Epoch 88/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 602168446386.6761\n",
      "Epoch 89/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 818067244468.9537\n",
      "Epoch 90/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 3718290031.1139\n",
      "Epoch 91/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 1699385172582.3394\n",
      "Epoch 92/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 500239382.7568\n",
      "Epoch 93/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 893262947059.8528\n",
      "Epoch 94/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 111024574289.2598\n",
      "Epoch 95/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 1026778901586.5527\n",
      "Epoch 96/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 22517787983.5255\n",
      "Epoch 97/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 804153156635.2550\n",
      "Epoch 98/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 3916548441.6892\n",
      "Epoch 99/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 1075337376448.0000\n",
      "Epoch 100/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 393793388.5030\n",
      "Epoch 101/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 1027305604611.8719\n",
      "Epoch 102/200\n",
      "3372/3372 [==============================] - 0s 42us/sample - loss: 677298982.9561\n",
      "Epoch 103/200\n",
      "3372/3372 [==============================] - 0s 55us/sample - loss: 704694852063.9431\n",
      "Epoch 104/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 712811294445.4662\n",
      "Epoch 105/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 40161665390.8921\n",
      "Epoch 106/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 1231031765155.9370\n",
      "Epoch 107/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 7249278957.3571\n",
      "Epoch 108/200\n",
      "3372/3372 [==============================] - 0s 52us/sample - loss: 130970930.7877\n",
      "Epoch 109/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 1016113628420.7070\n",
      "Epoch 110/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 1260063539.6916\n",
      "Epoch 111/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 618756932153.4330\n",
      "Epoch 112/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 36168609351.5730\n",
      "Epoch 113/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 822223391337.1199\n",
      "Epoch 114/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 175826395.8956\n",
      "Epoch 115/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 964873422836.9158\n",
      "Epoch 116/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 2350196829.7699\n",
      "Epoch 117/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 426564178687.6584\n",
      "Epoch 118/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 163602986129.7651\n",
      "Epoch 119/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 431771805394.5243\n",
      "Epoch 120/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 314039003786.6287\n",
      "Epoch 121/200\n",
      "3372/3372 [==============================] - 0s 52us/sample - loss: 404446248932.5018\n",
      "Epoch 122/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 612912854576.3820\n",
      "Epoch 123/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 2379827819.3120\n",
      "Epoch 124/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 419928467944.3986\n",
      "Epoch 125/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 485110483421.6465\n",
      "Epoch 126/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 7764415871.2883\n",
      "Epoch 127/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 476694835394.9229\n",
      "Epoch 128/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 485968221955.4164\n",
      "Epoch 129/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 5684300127.4270\n",
      "Epoch 130/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 512875666031.1459\n",
      "Epoch 131/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 262089774231.3452\n",
      "Epoch 132/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 199175816937.2242\n",
      "Epoch 133/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 1350501484918.6619\n",
      "Epoch 134/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 74669549954.8612\n",
      "Epoch 135/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 33270120.1803\n",
      "Epoch 136/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 36507721.6613\n",
      "Epoch 137/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 65323878832.9870\n",
      "Epoch 138/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 191850312555.2550\n",
      "Epoch 139/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 374760356385.6512\n",
      "Epoch 140/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 502280735969.6559\n",
      "Epoch 141/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 9119584518.2823\n",
      "Epoch 142/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 297354717039.4116\n",
      "Epoch 143/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 270488095381.7604\n",
      "Epoch 144/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 215125444115.4117\n",
      "Epoch 145/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 28307411992.1020\n",
      "Epoch 146/200\n",
      "3372/3372 [==============================] - 0s 56us/sample - loss: 302667857118.6548\n",
      "Epoch 147/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 721920654055.1074\n",
      "Epoch 148/200\n",
      "3372/3372 [==============================] - 0s 52us/sample - loss: 17291146875.6157\n",
      "Epoch 149/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 92140029.0012\n",
      "Epoch 150/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 324297426473.2859\n",
      "Epoch 151/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 7711818346.0593\n",
      "Epoch 152/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 258218364889.6987\n",
      "Epoch 153/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 187571397854.0261\n",
      "Epoch 154/200\n",
      "3372/3372 [==============================] - 0s 53us/sample - loss: 174565639327.7343\n",
      "Epoch 155/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 226430974875.3310\n",
      "Epoch 156/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 300014087856.2847\n",
      "Epoch 157/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 3084937681.1957\n",
      "Epoch 158/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 286006425223.7248\n",
      "Epoch 159/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 366842290119.2787\n",
      "Epoch 160/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 13270649613.8446\n",
      "Epoch 161/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 329409896480.7642\n",
      "Epoch 162/200\n",
      "3372/3372 [==============================] - 0s 52us/sample - loss: 21749612733.9069\n",
      "Epoch 163/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 297484604670.2473\n",
      "Epoch 164/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 17534403992.5273\n",
      "Epoch 165/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 300003448833.2811\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 0s 45us/sample - loss: 4074951336.7883\n",
      "Epoch 167/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 302418964057.1720\n",
      "Epoch 168/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 872573048.6868\n",
      "Epoch 169/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 335763160644.8934\n",
      "Epoch 170/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 283684127.4262\n",
      "Epoch 171/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 272191368353.7035\n",
      "Epoch 172/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 2868170148.5931\n",
      "Epoch 173/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 150588952246.0789\n",
      "Epoch 174/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 452399899264.3037\n",
      "Epoch 175/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 2056198870.6856\n",
      "Epoch 176/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 91507168.2064\n",
      "Epoch 177/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 168962540253.0581\n",
      "Epoch 178/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 355677914537.6180\n",
      "Epoch 179/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 78639769442.6382\n",
      "Epoch 180/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 292102985.3144\n",
      "Epoch 181/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 174879393400.3322\n",
      "Epoch 182/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 19852152719.7722\n",
      "Epoch 183/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 279281495157.4662\n",
      "Epoch 184/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 682104105.5652\n",
      "Epoch 185/200\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: 224748243816.9063\n",
      "Epoch 186/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 7486475491.2456\n",
      "Epoch 187/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 281362578460.1160\n",
      "Epoch 188/200\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: 11859954041.5854\n",
      "Epoch 189/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 140032162.9488\n",
      "Epoch 190/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 200319688716.2621\n",
      "Epoch 191/200\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: 109615153440.0569\n",
      "Epoch 192/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 31883984924.4223\n",
      "Epoch 193/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 152072347970.0878\n",
      "Epoch 194/200\n",
      "3372/3372 [==============================] - 0s 43us/sample - loss: 217287928914.8944\n",
      "Epoch 195/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 306435426429.2100\n",
      "Epoch 196/200\n",
      "3372/3372 [==============================] - 0s 45us/sample - loss: 31121813.6536\n",
      "Epoch 197/200\n",
      "3372/3372 [==============================] - 0s 51us/sample - loss: 5739043.0946\n",
      "Epoch 198/200\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: 10224989.5768\n",
      "Epoch 199/200\n",
      "3372/3372 [==============================] - 0s 48us/sample - loss: 225311504021.9226\n",
      "Epoch 200/200\n",
      "3372/3372 [==============================] - 0s 50us/sample - loss: 602746821.1483\n",
      "Accuracy: 0.9809976247030879\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.009501187648456057\n",
      "[[[842   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[826   0]\n",
      "  [ 16   0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        16\n",
      "   macro avg       0.00      0.00      0.00        16\n",
      "weighted avg       0.00      0.00      0.00        16\n",
      " samples avg       0.00      0.00      0.00        16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-195.99792, -357.99792, -4017.998, -4095.998, -4133.998, -4137.998, -4143.998, -4153.998, -4163.998, -4165.998, -4171.998, -4183.998, -4187.998, -4189.998, -4191.998, -4195.998, -4199.998, -4203.998, -4207.998, -4213.998, -4225.998, -4235.998, -4251.998, -4257.998, -4261.998, -4263.998, -4271.998, -4279.998, -4283.998, -4287.998, -4293.998, -4295.998, -4299.998, -4305.998, -4311.998, -4313.998, -4315.998, -4323.998, -4327.998, -4333.998, -4337.998, -4339.998, -4343.998, -4347.998, -4349.998, -4351.998, -4353.998, -4357.998, -4361.998, -4363.998, -4365.998, -4367.998, -4371.998, -4373.998, -4375.998, -4379.998, -4381.998, -4383.998, -4385.998, -4387.998, -4391.998, -4395.998, -4403.998, -4407.998, -4411.998, -4413.998, -4415.998, -4419.998, -4431.998, -4433.998, -4447.998, -4449.998, -4451.998, -4453.998, -4455.998, -4463.998, -4465.998, -4467.998, -4469.998, -4473.998, -4477.998, -4479.998, -4485.998, -4491.998, -4495.998, -4499.998, -4503.998, -4507.998, -4509.998, -4513.998, -4521.998, -4523.998, -4525.998, -4527.998, -4531.998, -4537.998, -4539.998, -4543.998, -4547.998, -4553.998, -4555.998, -4567.998, -4571.998, -4573.998, -4575.998, -4577.998, -4579.998, -4581.998, -4583.998, -4585.998, -4587.998, -4589.998, -4595.998, -4599.998, -4603.998, -4605.998, -4611.998, -4615.998, -4619.998, -4621.998, -4625.998, -4627.998, -4629.998, -4631.998, -4633.998, -4635.998, -4639.998, -4641.998, -4643.998, -4653.998, -4665.998, -4667.998, -4669.998, -4671.998, -4675.998, -4679.998, -4687.998, -4691.998, -4695.998, -4699.998, -4707.998, -4709.998, -4711.998, -4717.998, -4719.998, -4721.998, -4723.998, -4727.998, -4729.998, -4731.998, -4733.998, -4735.998, -4739.998, -4743.998, -4753.998, -4755.998, -4759.998, -4761.998, -4763.998, -4767.998, -4773.998, -4775.998, -4779.998, -4787.998, -4789.998, -4791.998, -4795.998, -4799.998, -4801.998, -4803.998, -4807.998, -4809.998, -4811.998, -4815.998, -4817.998, -4819.998, -4821.998, -4831.998, -4833.998, -4837.998, -4841.998, -4843.998, -4849.998, -4851.998, -4853.998, -4855.998, -4857.998, -4859.998, -4861.998, -4863.998, -4867.998, -4869.998, -4871.998, -4875.998, -4879.998, -4883.998, -4887.998, -4889.998, -4891.998, -4893.998, -4895.998, -4897.998, -4899.998, -4903.998, -4907.998, -4909.998, -4915.998, -4917.998, -4927.998, -4929.998, -4933.998, -4937.998, -4939.998, -4941.998, -4943.998, -4945.998, -4947.998, -4951.998, -4953.998, -4955.998, -4957.998, -4961.998, -4963.998, -4965.998, -4969.998, -4971.998, -4975.998, -4977.998, -4979.998, -4981.998, -4983.998, -4985.998, -4987.998, -4989.998, -4991.998, -4993.998, -4995.998, -4999.998, -5001.998, -5003.998, -5007.998, -5009.998, -5011.998, -5013.998, -5015.998, -5017.998, -5019.998, -5021.998, -5023.998, -5025.998, -5027.998, -5029.998, -5033.998, -5035.998, -5039.998, -5041.998, -5043.998, -5045.998, -5047.998, -5049.998, -5051.998, -5057.998, -5059.998, -5061.998, -5063.998, -5065.998, -5067.998, -5069.998, -5075.998, -5079.998, -5083.998, -5085.998, -5087.998, -5091.998, -5093.998, -5099.998, -5101.998, -5103.998, -5107.998, -5109.998, -5115.998, -5117.998, -5119.998, -5121.998, -5123.998, -5125.998, -5129.998, -5131.998, -5135.998, -5137.998, -5139.998, -5143.998, -5145.998, -5147.998, -5149.998, -5153.998, -5167.998, -5169.998, -5171.998, -5173.998, -5175.998, -5177.998, -5179.998, -5181.998, -5183.998, -5187.998, -5189.998, -5191.998, -5195.998, -5199.998, -5203.998, -5205.998, -5209.998, -5211.998, -5213.998, -5215.998, -5217.998, -5219.998, -5227.998, -5229.998, -5231.998, -5235.998, -5243.998, -5247.998, -5249.998, -5251.998, -5253.998, -5255.998, -5257.998, -5259.998, -5263.998, -5265.998, -5267.998, -5271.998, -5273.998, -5279.998, -5281.998, -5283.998, -5285.998, -5291.998, -5293.998, -5295.998, -5297.998, -5299.998, -5305.998, -5307.998, -5311.998, -5319.998, -5321.998, -5323.998, -5325.998, -5327.998, -5329.998, -5331.998, -5337.998, -5341.998, -5349.998, -5351.998, -5353.998, -5355.998, -5359.998, -5361.998, -5363.998, -5365.998, -5367.998, -5371.998, -5373.998, -5377.998, -5379.998, -5381.998, -5383.998, -5385.998, -5387.998, -5389.998, -5391.998, -5393.998, -5395.998, -5401.998, -5403.998, -5405.998, -5407.998, -5409.998, -5415.998, -5417.998, -5419.998, -5421.998, -5423.998, -5427.998, -5429.998, -5431.998, -5435.998, -5437.998, -5439.998, -5441.998, -5443.998, -5447.998, -5451.998, -5453.998, -5455.998, -5457.998, -5463.998, -5465.998, -5467.998, -5469.998, -5471.998, -5473.998, -5475.998, -5479.998, -5481.998, -5483.998, -5485.998, -5487.998, -5489.998, -5495.998, -5501.998, -5503.998, -5507.998, -5509.998, -5511.998, -5513.998, -5515.998, -5519.998, -5521.998, -5523.998, -5525.998, -5527.998, -5529.998, -5531.998, -5533.998, -5535.998, -5537.998, -5541.998, -5543.998, -5545.998, -5547.998, -5549.998, -5551.998, -5553.998, -5555.998, -5557.998, -5559.998, -5563.998, -5565.998, -5567.998, -5573.998, -5575.998, -5577.998, -5579.998, -5581.998, -5583.998, -5585.998, -5587.998, -5597.998, -5599.998, -5601.998, -5603.998, -5609.998, -5611.998, -5615.998, -5623.998, -5627.998, -5629.998, -5633.998, -5635.998, -5639.998, -5643.998, -5647.998, -5649.998, -5651.998, -5653.998, -5665.998, -5673.998, -5675.998, -5677.998, -5679.998, -5681.998, -5685.998, -5687.998, -5691.998, -5705.998, -5709.998, -5711.998, -5715.998, -5717.998, -5719.998, -5727.998, -5733.998, -5737.998, -5747.998, -5755.998, -5759.998, -5779.998, -5781.998, -5783.998, -5809.998, -5811.998, -5815.998, -5819.998, -5843.998, -5845.998, -5875.998, -5899.998, -5903.998, -6343.998, -6395.998, -6411.998, -6415.998, -6435.998, -6469.998, -6605.998, -749.9979, 48.002068, 96.00207] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(1)\n",
    "  ])    \n",
    "\n",
    "  #clf = mlr.fit(X_train, mlb.transform(y_train))\n",
    "\n",
    "  model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.0025))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=200)\n",
    "  \n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "  #print(y_test)\n",
    "  #print(mlb.fit(y_pred))\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  print(\"Recall: \" + str(rec))\n",
    "\n",
    "  pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  print(\"Precision: \" + str(pre))\n",
    "\n",
    "  f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  print(multilabel_confusion_matrix(y_test, y_pred2))\n",
    "  cm = multilabel_confusion_matrix(y_test, y_pred2)\n",
    "  #print(cm)\n",
    "  print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224650, 138)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2020XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2020XPT_Grounded_MissingFix.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE4'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE4']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [10] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: 195123494173224.7812\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: 59517013185681.5469\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: 30237566751263.7539\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 9s 53us/sample - loss: 18569651398459.5234\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 10097254347920.9648\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 9s 47us/sample - loss: 4899524090.3728\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 11s 60us/sample - loss: 792144841.3144\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: 74595939.9420\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: 312633.6580\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: 1147.5282\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: 4.2806\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 18213320261983.2148\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 108903874480.0857\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 1124101402.0390\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: 5623070.1037\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 7s 38us/sample - loss: 20857.4000\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 79.4649\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: 4685892930777.6406\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 46759564606.0626\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: 2237351988.7340\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: 13390552.3016\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 88434.4968\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 228.5759\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: 0.8679\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: 19664542584958.0898\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: 789407266.9088\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: 269168547.9010\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: 1192188.2269\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 3049.3800\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 8s 47us/sample - loss: 17.7484\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 8s 47us/sample - loss: 7856527467203.6631\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: 82236897.0555\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: 2190492405.5838\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 10s 53us/sample - loss: 20474125.0221\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 85803.9459\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: 353.5315\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 9s 52us/sample - loss: 1.77560s - loss: 1.787\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: 16520467031745.1875\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: 88188464763.1434\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: 3712133462.4968\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 10s 53us/sample - loss: 13627261.8411\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: 61801.5490\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 9s 47us/sample - loss: 232.5724\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: 0.9442\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 2996452283211.8530\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 804821974.80100s - los\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 25779580.2425\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 77326.0282\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: 444.0264\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 2.2609\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: 6433633864127.4756\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 7152159027.5072\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: 705492611.6854\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: 3951960.2153\n",
      "Epoch 55/100\n",
      "179720/179720 [==============================] - 9s 47us/sample - loss: 14620.8769\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: 54.6732\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 0.4639\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 8659557460260.9199\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 21401031501.7310\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 186488086.05500s - lo\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 973387.0209\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 4208.5118\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 13.2394\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: 0.2882\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: 5389692344688.0947\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: 222338498.9821\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 39049409.3614\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 169625.7623\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: 801.5925\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 3.3558\n",
      "Epoch 71/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 0.2669\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 2543019761334.8018\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 2341402749.5654\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: 12368558.1349\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: 43266.6316\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: 262.2831\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 12s 64us/sample - loss: 1.1690\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: 2873779088991.3086\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: 1624700945.9555\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: 1084032612.6251\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 9682101.3451\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: 18495.7738\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: 111.0402\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 0.56600s - loss\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 1957077186160.8721\n",
      "Epoch 86/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: 158623998.3478\n",
      "Epoch 87/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: 66412059.9694\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 7s 42us/sample - loss: 312970.4675\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 2027.5853\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: 3.2450\n",
      "Epoch 91/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: 0.2770\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: 1380419488967.4407\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 10718222.3498\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: 5289569.2856\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: 20407.5660\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: 102.9619\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: 0.6675\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: 4210405786592.0269\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: 54512260.9999\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: 319315958.6476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-0.6035199, -1.6035199, -10.603519, -100.60352, -101.60352, -102.60352, -103.60352, -104.60352, -105.60352, -106.60352, -107.60352, -108.60352, -109.60352, -11.603519, -110.60352, -111.60352, -112.60352, -113.60352, -114.60352, -115.60352, -116.60352, -117.60352, -118.60352, -119.60352, -12.603519, -120.60352, -121.60352, -122.60352, -123.60352, -124.60352, -125.60352, -126.60352, -127.60352, -128.60352, -129.60352, -13.603519, -130.60352, -131.60352, -132.60352, -133.60352, -134.60352, -135.60352, -136.60352, -137.60352, -138.60352, -139.60352, -14.603519, -140.60352, -141.60352, -142.60352, -143.60352, -144.60352, -145.60352, -146.60352, -147.60352, -148.60352, -149.60352, -15.603519, -150.60352, -151.60352, -152.60352, -153.60352, -154.60352, -155.60352, -156.60352, -157.60352, -158.60352, -159.60352, -16.60352, -160.60352, -161.60352, -162.60352, -163.60352, -164.60352, -165.60352, -166.60352, -167.60352, -168.60352, -169.60352, -17.60352, -170.60352, -171.60352, -172.60352, -173.60352, -174.60352, -175.60352, -176.60352, -177.60352, -178.60352, -179.60352, -18.60352, -180.60352, -181.60352, -182.60352, -183.60352, -184.60352, -185.60352, -186.60352, -187.60352, -188.60352, -189.60352, -19.60352, -190.60352, -191.60352, -192.60352, -193.60352, -194.60352, -195.60352, -196.60352, -197.60352, -198.60352, -199.60352, -20.60352, -200.60352, -201.60352, -202.60352, -203.60352, -204.60352, -205.60352, -206.60352, -207.60352, -208.60352, -209.60352, -21.60352, -210.60352, -211.60352, -212.60352, -213.60352, -214.60352, -215.60352, -216.60352, -217.60352, -218.60352, -219.60352, -22.60352, -220.60352, -221.60352, -222.60352, -223.60352, -224.60352, -225.60352, -226.60352, -227.60352, -228.60352, -229.60352, -23.60352, -230.60352, -231.60352, -232.60352, -233.60352, -234.60352, -235.60352, -236.60352, -237.60352, -238.60352, -239.60352, -24.60352, -240.60352, -241.60352, -242.60352, -243.60352, -244.60352, -245.60352, -246.60352, -247.60352, -248.60352, -249.60352, -25.60352, -250.60352, -251.60352, -252.60352, -253.60352, -254.60352, -255.60352, -256.60352, -257.60352, -258.60352, -259.60352, -26.60352, -260.60352, -261.60352, -262.60352, -263.60352, -264.60352, -265.60352, -266.60352, -267.60352, -268.60352, -269.60352, -27.60352, -270.60352, -271.60352, -272.60352, -273.60352, -274.60352, -275.60352, -276.60352, -277.60352, -278.60352, -279.60352, -28.60352, -280.60352, -281.60352, -282.60352, -283.60352, -284.60352, -285.60352, -286.60352, -287.60352, -288.60352, -289.60352, -29.60352, -290.60352, -291.60352, -292.60352, -293.60352, -294.60352, -295.60352, -296.60352, -297.60352, -298.60352, -299.60352, -3.60352, -30.60352, -300.60352, -301.60352, -302.60352, -303.60352, -304.60352, -305.60352, -306.60352, -307.60352, -308.60352, -309.60352, -31.60352, -310.60352, -311.60352, -312.60352, -313.60352, -314.60352, -315.60352, -316.60352, -317.60352, -318.60352, -319.60352, -32.60352, -320.60352, -321.60352, -322.60352, -33.60352, -34.60352, -35.60352, -36.60352, -37.60352, -38.60352, -39.60352, -40.60352, -41.60352, -42.60352, -43.60352, -44.60352, -45.60352, -46.60352, -47.60352, -48.60352, -49.60352, -5.60352, -50.60352, -51.60352, -52.60352, -53.60352, -54.60352, -55.60352, -56.60352, -57.60352, -58.60352, -59.60352, -6.60352, -60.60352, -61.60352, -62.60352, -63.60352, -64.60352, -65.60352, -66.60352, -67.60352, -68.60352, -69.60352, -7.60352, -70.60352, -71.60352, -72.60352, -73.60352, -75.60352, -76.60352, -78.60352, -79.60352, -8.603519, -80.60352, -81.60352, -82.60352, -83.60352, -84.60352, -85.60352, -86.60352, -87.60352, -88.60352, -89.60352, -9.603519, -90.60352, -91.60352, -92.60352, -93.60352, -94.60352, -95.60352, -96.60352, -97.60352, -98.60352, -99.60352, 1.3964801, 2.39648, 24.39648, 3.39648, 8.396481] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7625417315824616\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.12941241931894057\n",
      "[[[39827     0]\n",
      "  [ 5103     0]]\n",
      "\n",
      " [[38404     0]\n",
      "  [ 6526     0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      5103\n",
      "           1       0.00      0.00      0.00      6526\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     11629\n",
      "   macro avg       0.00      0.00      0.00     11629\n",
      "weighted avg       0.00      0.00      0.00     11629\n",
      " samples avg       0.00      0.00      0.00     11629\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 7s 42us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 7s 38us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 7s 42us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nans - los\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 10s 53us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 10s 53us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: nans -\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 9s 47us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nans - l\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 8s 47us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - ETA: 0s - loss: n - 8s 45us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nans - los - E\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 71/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 11s 63us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 11s 61us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7899176496772757\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.1137992432673047\n",
      "[[[40819     0]\n",
      "  [ 4111     0]]\n",
      "\n",
      " [[38815     0]\n",
      "  [ 6115     0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4111\n",
      "           1       0.00      0.00      0.00      6115\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     10226\n",
      "   macro avg       0.00      0.00      0.00     10226\n",
      "weighted avg       0.00      0.00      0.00     10226\n",
      " samples avg       0.00      0.00      0.00     10226\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 7s 42us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 7s 42us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 7s 42us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nans - loss: n\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 14s 78us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 11s 60us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 12s 66us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 11s 64us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 14s 79us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 11s 60us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 11s 62us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - ETA: 0s - loss: n - 12s 68us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 11s 63us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 8s 47us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 71/100\n",
      "179720/179720 [==============================] - 7s 38us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 9s 47us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 11s 60us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 11s 60us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: nan\n",
      "Accuracy: 0.7829957711996439\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.11831738259514801\n",
      "[[[40603     0]\n",
      "  [ 4327     0]]\n",
      "\n",
      " [[38625     0]\n",
      "  [ 6305     0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4327\n",
      "           1       0.00      0.00      0.00      6305\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     10632\n",
      "   macro avg       0.00      0.00      0.00     10632\n",
      "weighted avg       0.00      0.00      0.00     10632\n",
      " samples avg       0.00      0.00      0.00     10632\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan - loss:\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 10s 53us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 11s 63us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 14s 77us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 8s 43us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 8s 46us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 8s 47us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 9s 52us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 8s 47us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 9s 49us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 9s 52us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - 8s 45us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 9s 47us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 9s 52us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 9s 48us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 7s 41us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: nan\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179720/179720 [==============================] - 9s 53us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 8s 47us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 7s 40us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 8s 42us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 8s 44us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 7s 39us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 9s 53us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 86/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 9s 53us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 9s 51us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 10s 53us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 14s 78us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 9s 53us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - ETA: 0s - loss: n - 10s 56us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 11s 63us/sample - loss: nan\n",
      "Accuracy: 0.7957489427999109\n",
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "F1 Score: 0.0\n",
      "Hamming Loss: 0.1086356554640552\n",
      "[[[41721     0]\n",
      "  [ 3209     0]]\n",
      "\n",
      " [[38377     0]\n",
      "  [ 6553     0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3209\n",
      "           1       0.00      0.00      0.00      6553\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      9762\n",
      "   macro avg       0.00      0.00      0.00      9762\n",
      "weighted avg       0.00      0.00      0.00      9762\n",
      " samples avg       0.00      0.00      0.00      9762\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 11s 62us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 10s 57us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 11s 60us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 9s 52us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 15s 82us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 11s 60us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 10s 56us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 10s 55us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 10s 58us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 10s 54us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 9s 50us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 11s 59us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 11s 61us/sample - loss: nan\n",
      "Epoch 20/100\n",
      " 29056/179720 [===>..........................] - ETA: 9s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8ae2263f35c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[1;31m#model.fit(X_train, labels, epochs=20)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(1)\n",
    "  ])    \n",
    "\n",
    "  #clf = mlr.fit(X_train, mlb.transform(y_train))\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.01))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  #model.fit(X_train, mlb.transform(y_train), epochs=20)\n",
    "\n",
    "  model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.0025))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "  #print(y_test)\n",
    "  #print(mlb.fit(y_pred))\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  print(\"Recall: \" + str(rec))\n",
    "\n",
    "  pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  print(\"Precision: \" + str(pre))\n",
    "\n",
    "  f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  print(multilabel_confusion_matrix(y_test, y_pred2))\n",
    "  cm = multilabel_confusion_matrix(y_test, y_pred2)\n",
    "  #print(cm)\n",
    "  print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4214, 138)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2020XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2020XPT_Grounded_ML_Nona_KG.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE4'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE4']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3371 samples\n",
      "Epoch 1/20\n",
      "3371/3371 [==============================] - 1s 169us/sample - loss: nan               \n",
      "Epoch 2/20\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "3371/3371 [==============================] - 0s 65us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.0\n",
      "Hamming Loss: 0.5\n",
      "Train on 3371 samples\n",
      "Epoch 1/20\n",
      "3371/3371 [==============================] - 1s 176us/sample - loss: nan               \n",
      "Epoch 2/20\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: nans - loss:\n",
      "Epoch 11/20\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "3371/3371 [==============================] - 0s 40us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.004744958481613286\n",
      "Hamming Loss: 0.49762752075919336\n",
      "Train on 3371 samples\n",
      "Epoch 1/20\n",
      "3371/3371 [==============================] - 1s 160us/sample - loss: nan              \n",
      "Epoch 2/20\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "3371/3371 [==============================] - 0s 37us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "3371/3371 [==============================] - 0s 41us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "3371/3371 [==============================] - 0s 51us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "3371/3371 [==============================] - 0s 48us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "3371/3371 [==============================] - 0s 43us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "3371/3371 [==============================] - 0s 53us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "3371/3371 [==============================] - 0s 49us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.002372479240806643\n",
      "Hamming Loss: 0.4988137603795967\n",
      "Train on 3371 samples\n",
      "Epoch 1/20\n",
      "3371/3371 [==============================] - 1s 172us/sample - loss: nan          \n",
      "Epoch 2/20\n",
      "3371/3371 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "3371/3371 [==============================] - 0s 54us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "3371/3371 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "3371/3371 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "3371/3371 [==============================] - 0s 46us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "3371/3371 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "3371/3371 [==============================] - 0s 45us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "3371/3371 [==============================] - 0s 39us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "3371/3371 [==============================] - 0s 44us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "3371/3371 [==============================] - 0s 42us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "3371/3371 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "3371/3371 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "3371/3371 [==============================] - 0s 58us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "3371/3371 [==============================] - 0s 56us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "3371/3371 [==============================] - 0s 50us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.0035587188612099642\n",
      "Hamming Loss: 0.498220640569395\n",
      "Train on 3372 samples\n",
      "Epoch 1/20\n",
      "3372/3372 [==============================] - 1s 255us/sample - loss: nan               \n",
      "Epoch 2/20\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "3372/3372 [==============================] - 0s 55us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "3372/3372 [==============================] - 0s 60us/sample - loss: nans - loss:\n",
      "Epoch 5/20\n",
      "3372/3372 [==============================] - 0s 49us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "3372/3372 [==============================] - 0s 52us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "3372/3372 [==============================] - 0s 46us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "3372/3372 [==============================] - ETA: 0s - loss: n - 0s 44us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "3372/3372 [==============================] - 0s 47us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "3372/3372 [==============================] - 0s 36us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "3372/3372 [==============================] - 0s 40us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "3372/3372 [==============================] - 0s 38us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "3372/3372 [==============================] - 0s 42us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "3372/3372 [==============================] - 0s 39us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "3372/3372 [==============================] - 0s 38us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "3372/3372 [==============================] - 0s 59us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "3372/3372 [==============================] - 0s 44us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.0035629453681710215\n",
      "Hamming Loss: 0.4982185273159145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(1)\n",
    "  ])    \n",
    "\n",
    "  #clf = mlr.fit(X_train, mlb.transform(y_train))\n",
    "\n",
    "  model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.01))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=20)\n",
    "  \n",
    "  #y_pred = model.predict(X_test)\n",
    "  y_pred = model.predict(X_test)\n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  \n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "  print(y_pred2)\n",
    "  #print(mlb.fit(y_pred))\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  y_test2 = mlb.transform(y_test)\n",
    "  print(y_test2)\n",
    "  #print(y_pred)    \n",
    "  #print(y_pred2)\n",
    "    \n",
    "  #y_test2 = np.argmax(y_test, axis=1)\n",
    "  #print(y_test2)\n",
    "  #y_pred3 = np.argmax(y_pred2, axis=1)\n",
    "\n",
    "\n",
    "  #print(y_test2)\n",
    "  #print(y_pred3)\n",
    "    \n",
    "  #df = pd.DataFrame(y_test)\n",
    "  #df.to_csv('y_test.csv') \n",
    "\n",
    "  #df = pd.DataFrame(y_pred2)\n",
    "  #df.to_csv('y_pred2.csv') \n",
    "\n",
    "  acc=accuracy_score(y_test2,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test2,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  #print(multilabel_confusion_matrix(y_test, y_pred2))\n",
    "  #cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "  #print(cm)\n",
    "  #print(classification_report(y_test,y_pred2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224650, 278)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2020XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2020XPT_Grounded_MissingFix_KG.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE4'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE4']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 179720 samples\n",
      "Epoch 1/20\n",
      "179720/179720 [==============================] - 14s 77us/sample - loss: nan\n",
      "Epoch 2/20\n",
      "179720/179720 [==============================] - 13s 73us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "179720/179720 [==============================] - 16s 91us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "179720/179720 [==============================] - 14s 78us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "179720/179720 [==============================] - 15s 83us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "179720/179720 [==============================] - 15s 84us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "179720/179720 [==============================] - 15s 82us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "179720/179720 [==============================] - 14s 77us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "179720/179720 [==============================] - 13s 72us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "179720/179720 [==============================] - 15s 81us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "179720/179720 [==============================] - 16s 87us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "179720/179720 [==============================] - 14s 77us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.021366570220342756\n",
      "Hamming Loss: 0.48931671488982864\n",
      "Train on 179720 samples\n",
      "Epoch 1/20\n",
      "179720/179720 [==============================] - 16s 90us/sample - loss: nan\n",
      "Epoch 2/20\n",
      "179720/179720 [==============================] - 15s 84us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.017516136211885153\n",
      "Hamming Loss: 0.4912419318940574\n",
      "Train on 179720 samples\n",
      "Epoch 1/20\n",
      "179720/179720 [==============================] - 13s 73us/sample - loss: nan\n",
      "Epoch 2/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.019630536389939907\n",
      "Hamming Loss: 0.49018473180503\n",
      "Train on 179720 samples\n",
      "Epoch 1/20\n",
      "179720/179720 [==============================] - 13s 72us/sample - loss: nan\n",
      "Epoch 2/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 11/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.013020253728021366\n",
      "Hamming Loss: 0.4934898731359893\n",
      "Train on 179720 samples\n",
      "Epoch 1/20\n",
      "179720/179720 [==============================] - 13s 73us/sample - loss: nan\n",
      "Epoch 2/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 3/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 4/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 5/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 6/20\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 7/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 8/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 9/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 10/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 12/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 13/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 14/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 15/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 16/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 17/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 18/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 19/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 20/20\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.02474961050523036\n",
      "Hamming Loss: 0.4876251947473848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(1)\n",
    "  ])    \n",
    "\n",
    "  #clf = mlr.fit(X_train, mlb.transform(y_train))\n",
    "\n",
    "  model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.01))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=20)\n",
    "  \n",
    "  #y_pred = model.predict(X_test)\n",
    "  y_pred = model.predict(X_test)\n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  \n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "  print(y_pred2)\n",
    "  #print(mlb.fit(y_pred))\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  y_test2 = mlb.transform(y_test)\n",
    "  print(y_test2)\n",
    "  #print(y_pred)    \n",
    "  #print(y_pred2)\n",
    "    \n",
    "  #y_test2 = np.argmax(y_test, axis=1)\n",
    "  #print(y_test2)\n",
    "  #y_pred3 = np.argmax(y_pred2, axis=1)\n",
    "\n",
    "\n",
    "  #print(y_test2)\n",
    "  #print(y_pred3)\n",
    "    \n",
    "  #df = pd.DataFrame(y_test)\n",
    "  #df.to_csv('y_test.csv') \n",
    "\n",
    "  #df = pd.DataFrame(y_pred2)\n",
    "  #df.to_csv('y_pred2.csv') \n",
    "\n",
    "  acc=accuracy_score(y_test2,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test2,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  #print(multilabel_confusion_matrix(y_test, y_pred2))\n",
    "  #cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "  #print(cm)\n",
    "  #print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2020XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2020XPT_Grounded_nona_KGFS20.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE4'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE4']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 11s 64us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 13s 74us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 13s 74us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 13s 74us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 14s 76us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 13s 72us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - ETA: 0s - loss: n - 12s 69us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 13s 73us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 13s 73us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 71/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 86/100\n",
      "179720/179720 [==============================] - 13s 72us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "179720/179720 [==============================] - 13s 73us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.021366570220342756\n",
      "Hamming Loss: 0.48931671488982864\n",
      "[[[39827     0]\n",
      "  [ 5103     0]]\n",
      "\n",
      " [[38404     0]\n",
      "  [ 6526     0]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      5103\n",
      "           1       0.00      0.00      0.00      6526\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     11629\n",
      "   macro avg       0.00      0.00      0.00     11629\n",
      "weighted avg       0.00      0.00      0.00     11629\n",
      " samples avg       0.00      0.00      0.00     11629\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 13s 74us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 71/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 86/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.017516136211885153\n",
      "Hamming Loss: 0.4912419318940574\n",
      "[[[40819     0]\n",
      "  [ 4111     0]]\n",
      "\n",
      " [[38815     0]\n",
      "  [ 6115     0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4111\n",
      "           1       0.00      0.00      0.00      6115\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     10226\n",
      "   macro avg       0.00      0.00      0.00     10226\n",
      "weighted avg       0.00      0.00      0.00     10226\n",
      " samples avg       0.00      0.00      0.00     10226\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 55/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 86/100\n",
      "179720/179720 [==============================] - ETA: 0s - loss: n - 12s 69us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.019630536389939907\n",
      "Hamming Loss: 0.49018473180503\n",
      "[[[40603     0]\n",
      "  [ 4327     0]]\n",
      "\n",
      " [[38625     0]\n",
      "  [ 6305     0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4327\n",
      "           1       0.00      0.00      0.00      6305\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     10632\n",
      "   macro avg       0.00      0.00      0.00     10632\n",
      "weighted avg       0.00      0.00      0.00     10632\n",
      " samples avg       0.00      0.00      0.00     10632\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 13s 75us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 25/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 26/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 27/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 28/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 29/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 30/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 31/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 32/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 33/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 34/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 35/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 36/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 37/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 38/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 39/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 40/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 41/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 42/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 43/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 44/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 45/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 46/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 47/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 48/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 49/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 50/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 51/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 52/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 53/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 54/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 56/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 57/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 58/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 59/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 60/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 67/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 68/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 69/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 70/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 71/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 72/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 73/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 74/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 75/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 76/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 84/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 85/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 86/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 87/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 88/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 89/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 90/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 91/100\n",
      "179720/179720 [==============================] - 12s 70us/sample - loss: nan\n",
      "Epoch 92/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 93/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 94/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 95/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 96/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 97/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 98/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 99/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 100/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "Accuracy: 0.013020253728021366\n",
      "Hamming Loss: 0.4934898731359893\n",
      "[[[41721     0]\n",
      "  [ 3209     0]]\n",
      "\n",
      " [[38377     0]\n",
      "  [ 6553     0]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3209\n",
      "           1       0.00      0.00      0.00      6553\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      9762\n",
      "   macro avg       0.00      0.00      0.00      9762\n",
      "weighted avg       0.00      0.00      0.00      9762\n",
      " samples avg       0.00      0.00      0.00      9762\n",
      "\n",
      "Train on 179720 samples\n",
      "Epoch 1/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 2/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 3/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 4/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 5/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 6/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 7/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 8/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 9/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 10/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 11/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 12/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 13/100\n",
      "179720/179720 [==============================] - 13s 70us/sample - loss: nan\n",
      "Epoch 14/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 15/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 16/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 17/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 18/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 19/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 20/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 21/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 22/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 23/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 24/100\n",
      "179720/179720 [==============================] - 13s 74us/sample - loss: nan\n",
      "Epoch 61/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 62/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 63/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 64/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 65/100\n",
      "179720/179720 [==============================] - 12s 69us/sample - loss: nan\n",
      "Epoch 66/100\n",
      "179720/179720 [==============================] - 13s 75us/sample - loss: nan\n",
      "Epoch 77/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 78/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 79/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 80/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 81/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 82/100\n",
      "179720/179720 [==============================] - 13s 71us/sample - loss: nan\n",
      "Epoch 83/100\n",
      "108192/179720 [=================>............] - ETA: 5s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(31),\n",
    "  tf.keras.layers.Dense(31),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ])    \n",
    "\n",
    "  #clf = mlr.fit(X_train, mlb.transform(y_train))\n",
    "\n",
    "  model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.01))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "  \n",
    "  #y_pred = model.predict(X_test)\n",
    "  y_pred = model.predict(X_test)\n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  \n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "  print(y_pred2)\n",
    "  #print(mlb.fit(y_pred))\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  y_test2 = mlb.transform(y_test)\n",
    "  print(y_test2)\n",
    "  #print(y_pred)    \n",
    "  #print(y_pred2)\n",
    "    \n",
    "  #y_test2 = np.argmax(y_test, axis=1)\n",
    "  #print(y_test2)\n",
    "  #y_pred3 = np.argmax(y_pred2, axis=1)\n",
    "\n",
    "\n",
    "  #print(y_test2)\n",
    "  #print(y_pred3)\n",
    "    \n",
    "  #df = pd.DataFrame(y_test)\n",
    "  #df.to_csv('y_test.csv') \n",
    "\n",
    "  #df = pd.DataFrame(y_pred2)\n",
    "  #df.to_csv('y_pred2.csv') \n",
    "\n",
    "  acc=accuracy_score(y_test2,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test2,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  print(multilabel_confusion_matrix(y_test, y_pred2))\n",
    "  cm = multilabel_confusion_matrix(y_test, y_pred2)\n",
    "  #print(cm)\n",
    "  print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
