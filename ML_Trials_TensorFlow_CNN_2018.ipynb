{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3520, 129)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_ML_nona.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 241us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 86us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 84us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 82us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 82us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 86us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 82us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 84us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 13.5790 - binary_accuracy: 0.1174s - loss: 13.5501 - binary_accuracy: 0.\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 87us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 106us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 81us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 100us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 83us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 100us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 101us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 84us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 68/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816/2816 [==============================] - 0s 127us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 126us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 131us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 123us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 106us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 109us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 111us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 113us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 106us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 115us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 115us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 108us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 127us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 128us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 124us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 118us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 116us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 116us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 13.5790 - binary_accuracy: 0.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-78255570.0, -78256190.0, -78256210.0, -78258850.0, -78260136.0, -78260860.0, -78260900.0, -78260920.0, -78263520.0, -78264130.0, -78264856.0, -78266100.0, -78266160.0, -78266190.0, -78266760.0, -78266850.0, -78266880.0, -78266910.0, -78267390.0, -78267490.0, -78268140.0, -78268744.0, -78269416.0, -78270100.0, -78271340.0, -78272050.0, -78272080.0, -78272710.0, -78272740.0, -78272750.0, -78272850.0, -78272860.0, -78273390.0, -78273520.0, -78274050.0, -78274190.0, -78274710.0, -78274760.0, -78274824.0, -78275440.0, -78323220.0, -78325040.0, -78325120.0, -78325144.0, -78325660.0, -78325780.0, -78326370.0, -78326400.0, -78326450.0, -78327120.0, -78327680.0, -78328450.0, -78328460.0, -78329656.0, -78329730.0, -78329750.0, -78331000.0, -78331016.0, -78331020.0, -78331140.0, -78331170.0, -78331790.0, -78332420.0, -78332910.0, -78333730.0, -78333760.0, -78334330.0, -78334960.0, -78334980.0, -78335070.0, -78335144.0, -78337580.0, -78337600.0, -78339730.0, -78339740.0, -78340240.0, -78340450.0, -78340470.0, -78341010.0, -78341016.0, -78341040.0, -78341060.0, -78341080.0, -78341100.0, -78341660.0, -78341680.0, -78341690.0, -78342240.0, -78342870.0, -78389336.0, -78390020.0, -78390050.0, -78390060.0, -78390070.0, -78390110.0, -78390640.0, -78390700.0, -78390750.0, -78391384.0, -78391900.0, -78392620.0, -78392790.0, -78393240.0, -78393280.0, -78393300.0, -78393360.0, -78393950.0, -78394020.0, -78394480.0, -78394540.0, -78394550.0, -78394610.0, -78395220.0, -78395230.0, -78395260.0, -78396690.0, -78397230.0, -78397840.0, -78397920.0, -78398050.0, -78398720.0, -78399220.0, -78399310.0, -78399384.0, -78399890.0, -78400050.0, -78402660.0, -78402710.0, -78403330.0, -78403820.0, -78404590.0, -78405140.0, -78405250.0, -78405310.0, -78405840.0, -78406510.0, -78407180.0, -78407220.0, -78407890.0, -78407920.0, -78408450.0, -78409140.0, -78455680.0, -78456216.0, -78456830.0, -78457460.0, -78457530.0, -78457580.0, -78457630.0, -78458150.0, -78458180.0, -78458200.0, -78458210.0, -78458230.0, -78458250.0, -78458260.0, -78458270.0, -78458320.0, -78458820.0, -78458850.0, -78458860.0, -78458940.0, -78459050.0, -78459500.0, -78460190.0, -78460220.0, -78460930.0, -78461000.0, -78461650.0, -78462240.0, -78462250.0, -78462320.0, -78462890.0, -78463520.0, -78463560.0, -78463620.0, -78463630.0, -78464780.0, -78464820.0, -78466940.0, -78467570.0, -78468100.0, -78468184.0, -78468216.0, -78468910.0, -78469560.0, -78470240.0, -78470260.0, -78470850.0, -78470880.0, -78470900.0, -78471520.0, -78471576.0, -78472856.0, -78474110.0, -78474216.0, -78520660.0, -78520720.0, -78521350.0, -78521410.0, -78521650.0, -78521656.0, -78521920.0, -78522560.0, -78523200.0, -78523470.0, -78523784.0, -78523840.0, -78524760.0, -78525000.0, -78525040.0, -78525340.0, -78525980.0, -78526050.0, -78526240.0, -78526340.0, -78526510.0, -78526970.0, -78527510.0, -78527810.0, -78528110.0, -78528140.0, -78528450.0, -78528770.0, -78529700.0, -78530024.0, -78530590.0, -78533070.0, -78533710.0, -78533720.0, -78533736.0, -78533740.0, -78534340.0, -78534370.0, -78534590.0, -78534640.0, -78535020.0, -78535500.0, -78537500.0, -78537980.0, -78538060.0, -78538120.0, -78538630.0, -78582590.0, -78583260.0, -78584240.0, -78584290.0, -78584320.0, -78584460.0, -78584740.0, -78584840.0, -78585430.0, -78585440.0, -78585470.0, -78585820.0, -78586290.0, -78586760.0, -78587550.0, -78587580.0, -78587600.0, -78587900.0, -78587970.0, -78588010.0, -78588030.0, -78588180.0, -78588510.0, -78588540.0, -78588560.0, -78588580.0, -78588590.0, -78588900.0, -78589070.0, -78589870.0, -78590050.0, -78590060.0, -78591710.0, -78592280.0, -78592540.0, -78592950.0, -78593790.0, -78594160.0, -78594400.0, -78595090.0, -78596050.0, -78596290.0, -78596670.0, -78596690.0, -78596730.0, -78596910.0, -78598080.0, -78598770.0, -78600020.0, -78600030.0, -78644590.0, -78644620.0, -78645470.0, -78646100.0, -78647730.0, -78647944.0, -78647950.0, -78648320.0, -78648580.0, -78648620.0, -78649170.0, -78649310.0, -78649860.0, -78650370.0, -78650536.0, -78650540.0, -78651010.0, -78651110.0, -78651700.0, -78651780.0, -78652020.0, -78652340.0, -78652960.0, -78653200.0, -78654760.0, -78654824.0, -78654830.0, -78654910.0, -78655070.0, -78655440.0, -78655460.0, -78655976.0, -78656030.0, -78656104.0, -78656296.0, -78656344.0, -78656420.0, -78656540.0, -78656640.0, -78656700.0, -78656740.0, -78656980.0, -78657340.0, -78657350.0, -78657384.0, -78657550.0, -78657610.0, -78659140.0, -78659680.0, -78659740.0, -78659790.0, -78660270.0, -78660290.0, -78660350.0, -78660460.0, -78660490.0, -78661016.0, -78661070.0, -78661620.0, -78661670.0, -78661680.0, -78662220.0, -78662340.0, -78663500.0, -78663520.0, -78706584.0, -78707470.0, -78708984.0, -78709010.0, -78710610.0, -78710630.0, -78710830.0, -78710880.0, -78710930.0, -78711480.0, -78711810.0, -78712080.0, -78712376.0, -78712456.0, -78712460.0, -78713120.0, -78713690.0, -78713700.0, -78713730.0, -78713740.0, -78713900.0, -78714650.0, -78714850.0, -78714860.0, -78714984.0, -78715790.0, -78715840.0, -78716100.0, -78716136.0, -78716430.0, -78716460.0, -78716480.0, -78716810.0, -78716820.0, -78716856.0, -78717070.0, -78717416.0, -78717580.0, -78717670.0, -78717704.0, -78718920.0, -78719150.0, -78719510.0, -78719770.0, -78719864.0, -78720450.0, -78720500.0, -78720540.0, -78721064.0, -78721090.0, -78721384.0, -78721680.0, -78721730.0, -78722310.0, -78722380.0, -78722390.0, -78722420.0, -78722720.0, -78722930.0, -78722990.0, -78723470.0, -78723540.0, -78723544.0, -78723560.0, -78724040.0, -78724890.0, -78725460.0, -78726100.0, -78769680.0, -78769730.0, -78769976.0, -78770050.0, -78770056.0, -78771250.0, -78771300.0, -78771740.0, -78772460.0, -78772540.0, -78773064.0, -78773090.0, -78773110.0, -78773720.0, -78773760.0, -78774090.0, -78774220.0, -78774270.0, -78774330.0, -78774936.0, -78775560.0, -78775920.0, -78776190.0, -78776220.0, -78776540.0, -78776820.0, -78777830.0, -78778340.0, -78778620.0, -78778670.0, -78778740.0, -78778744.0, -78779030.0, -78779064.0, -78779550.0, -78779630.0, -78779650.0, -78779990.0, -78780220.0, -78780450.0, -78780540.0, -78780856.0, -78780880.0, -78781150.0, -78781430.0, -78781736.0, -78781830.0, -78781864.0, -78782056.0, -78782060.0, -78782110.0, -78782370.0, -78782500.0, -78783110.0, -78784260.0, -78785550.0, -78785570.0, -78785780.0, -78785840.0, -78786140.0, -78786400.0, -78830350.0, -78831010.0, -78831060.0, -78831390.0, -78831420.0, -78831920.0, -78832320.0, -78832800.0, -78832856.0, -78833150.0, -78834104.0, -78834130.0, -78834660.0, -78834780.0, -78835340.0, -78835360.0, -78835730.0, -78835900.0, -78835980.0, -78836340.0, -78836500.0, -78836530.0, -78836590.0, -78836880.0, -78837170.0, -78837176.0, -78837260.0, -78837600.0, -78837790.0, -78837820.0, -78838104.0, -78838136.0, -78838200.0, -78838750.0, -78838830.0, -78840000.0, -78840610.0, -78841310.0, -78841820.0, -78841870.0, -78841890.0, -78843170.0, -78844260.0, -78844430.0, -78845220.0, -78845900.0, -78846080.0, -78846184.0, -78846190.0, -78846456.0, -78846500.0, -78846530.0, -78846850.0, -78847090.0, -78847110.0, -78847730.0, -78848340.0, -78848720.0, -78848920.0, -78893540.0, -78894220.0, -78894470.0, -78894840.0, -78895140.0, -78895430.0, -78895440.0, -78895700.0, -78896344.0, -78897200.0, -78897544.0, -78897690.0, -78897890.0, -78898480.0, -78898840.0, -78898860.0, -78899120.0, -78899160.0, -78899330.0, -78901620.0, -78901650.0, -78901920.0, -78902296.0, -78902500.0, -78902850.0, -78902904.0, -78903176.0, -78903440.0, -78903480.0, -78903740.0, -78903760.0, -78903770.0, -78904104.0, -78904110.0, -78904130.0, -78904136.0, -78904360.0, -78904740.0, -78905010.0, -78905020.0, -78905060.0, -78905070.0, -78905310.0, -78905390.0, -78905576.0, -78906020.0, -78906184.0, -78906240.0, -78906260.0, -78906300.0, -78906520.0, -78906580.0, -78906620.0, -78907200.0, -78907210.0, -78907500.0, -78908030.0, -78908130.0, -78908184.0, -78908970.0, -78909000.0, -78909010.0, -78909040.0, -78909060.0, -78909350.0, -78909680.0, -78909920.0, -78910240.0, -78910290.0, -78910600.0, -78910690.0, -78911860.0, -78954210.0, -78954260.0, -78954300.0, -78954510.0, -78954820.0, -78954860.0, -78954910.0, -78955040.0, -78955150.0, -78955180.0, -78955200.0, -78955210.0, -78955220.0, -78955230.0, -78955420.0, -78955464.0, -78955760.0, -78955980.0, -78956160.0, -78956170.0, -78956180.0, -78956296.0, -78956450.0, -78957400.0, -78957920.0, -78957940.0, -78959570.0, -78960190.0, -78960216.0, -78960240.0, -78962020.0, -78962110.0, -78963860.0, -78964240.0, -78965170.0, -78965176.0, -78965180.0, -78966370.0, -78966400.0, -78966410.0, -78966670.0, -78966690.0, -78967260.0, -78967900.0, -78967960.0, -78968220.0, -78968230.0, -78968240.0, -78968270.0, -78968296.0, -78968620.0, -78968830.0, -78968900.0, -78969770.0, -78971040.0, 283988900.0, 283989250.0, 283989540.0, 283989700.0, 283991500.0, 283992500.0, 283993380.0, 283993500.0, 283993920.0, 283994140.0, 283994530.0, 283996130.0, 283996200.0, 283996220.0, 283996930.0, 283996960.0, 283997060.0, 283997400.0, 283997950.0, 283998200.0, 283998750.0, 283998820.0, 283998880.0, 283998940.0, 283999100.0, 283999230.0, 283999420.0, 283999780.0, 283999800.0, 283999940.0, 283999970.0, 284000400.0, 284000670.0, 284001500.0, 284001570.0, 284003740.0, 284003870.0, 284004580.0, 284005000.0, 284005500.0, 284006270.0, 284044640.0, 284045570.0, 284045860.0, 284045950.0, 284047000.0, 284047330.0, 284048580.0, 284049120.0, 284049180.0, 284049440.0, 284049660.0, 284049820.0, 284049900.0, 284050080.0, 284050560.0, 284050700.0, 284051520.0, 284051700.0, 284052670.0, 284053020.0, 284053250.0, 284053540.0, 284054050.0, 284054100.0, 284054460.0, 284054800.0, 284055680.0, 284055840.0, 284056320.0, 284056400.0, 284057100.0, 284057150.0, 284057570.0, 284057660.0, 284059300.0, 284060000.0, 284061150.0, 284061380.0, 284062050.0, 284062080.0, 284062500.0, 284062530.0, 284062700.0, 284062720.0, 284062750.0, 284063040.0, 284063230.0, 284101150.0, 284102180.0, 284103200.0, 284103420.0, 284103520.0, 284103740.0, 284104400.0, 284104420.0, 284104450.0, 284104500.0, 284104670.0, 284104700.0, 284104740.0, 284104770.0, 284105020.0, 284105100.0, 284105120.0, 284105180.0, 284105280.0, 284105400.0, 284106500.0, 284106700.0, 284107000.0, 284107170.0, 284107620.0, 284107680.0, 284107780.0, 284108100.0, 284108400.0, 284108900.0, 284109470.0, 284110340.0, 284111420.0, 284112200.0, 284112350.0, 284112640.0, 284112770.0, 284113200.0, 284113860.0, 284114500.0, 284115420.0, 284115460.0, 284115520.0, 284115900.0, 284116100.0, 284116220.0, 284116740.0, 284117060.0, 284117250.0, 284117400.0, 284117540.0, 284119840.0, 284157600.0, 284157820.0, 284158300.0, 284158600.0, 284158980.0, 284159070.0, 284159170.0, 284159550.0, 284159700.0, 284159740.0, 284160030.0, 284160600.0, 284161180.0, 284162660.0, 284162700.0, 284162940.0, 284162980.0, 284163070.0, 284163140.0, 284163330.0, 284163620.0, 284164100.0, 284164700.0, 284164830.0, 284165470.0, 284165630.0, 284166200.0, 284166560.0, 284166600.0, 284166880.0, 284167230.0, 284167550.0, 284167680.0, 284168060.0, 284168200.0, 284170880.0, 284171040.0, 284171600.0, 284171620.0, 284173060.0, 284173150.0, 284173280.0, 284173630.0, 284173730.0, 284174340.0, 284174430.0, 284174560.0, 284175140.0, 284175360.0, 284176600.0, 284177020.0, 284215300.0, 284216900.0, 284218000.0, 284218080.0, 284218240.0, 284218340.0, 284218530.0, 284218980.0, 284219040.0, 284219460.0, 284219650.0, 284219780.0, 284220350.0, 284220860.0, 284221060.0, 284221570.0, 284222370.0, 284222500.0, 284222660.0, 284223460.0, 284223550.0, 284223600.0, 284224200.0, 284224600.0, 284225060.0, 284225100.0, 284226400.0, 284227360.0, 284227650.0, 284228400.0, 284228600.0, 284230700.0, 284230850.0, 284230940.0, 284231040.0, 284231360.0, 284231400.0, 284231550.0, 284232320.0, 284232930.0, 284233400.0, 284234980.0, 284235360.0, 284235460.0, 284235800.0, 284275940.0, 284276400.0, 284277700.0, 284278050.0, 284278560.0, 284278620.0, 284279140.0, 284279360.0, 284279460.0, 284279600.0, 284279740.0, 284279800.0, 284280160.0, 284280320.0, 284280450.0, 284280830.0, 284281700.0, 284281820.0, 284282180.0, 284282340.0, 284282430.0, 284282600.0, 284282700.0, 284283140.0, 284283600.0, 284283620.0, 284284400.0, 284284450.0, 284284830.0, 284284900.0, 284285020.0, 284285200.0, 284285630.0, 284285820.0, 284286340.0, 284286500.0, 284287970.0, 284288000.0, 284288420.0, 284288480.0, 284288600.0, 284288770.0, 284289900.0, 284290200.0, 284291460.0, 284292200.0, 284292580.0, 284293860.0, 284294460.0, 284296200.0, 284296220.0, 284297800.0, 284298000.0, 284340260.0, 284340350.0, 284341020.0, 284341060.0, 284341470.0, 284342460.0, 284342560.0, 284342720.0, 284342800.0, 284342820.0, 284343170.0, 284343500.0, 284344060.0, 284344200.0, 284344450.0, 284345380.0, 284345470.0, 284345500.0, 284345950.0, 284346100.0, 284346400.0, 284346880.0, 284347040.0, 284347230.0, 284348450.0, 284348960.0, 284349000.0, 284349150.0, 284349220.0, 284349300.0, 284349730.0, 284349800.0, 284349820.0, 284349950.0, 284350100.0, 284350500.0, 284350620.0, 284351170.0, 284351200.0, 284351400.0, 284352700.0, 284352900.0, 284353120.0, 284353150.0, 284353440.0, 284353570.0, 284353600.0, 284353820.0, 284354000.0, 284354020.0, 284354340.0, 284354400.0, 284354500.0, 284354600.0, 284354750.0, 284354780.0, 284354800.0, 284354880.0, 284354980.0, 284355360.0, 284355520.0, 284355700.0, 284355800.0, 284356220.0, 284400320.0, 284403200.0, 284403260.0, 284404640.0, 284404930.0, 284405060.0, 284405100.0, 284405820.0, 284406180.0, 284406270.0, 284406980.0, 284407550.0, 284407840.0, 284409340.0, 284409470.0, 284409500.0, 284409570.0, 284409730.0, 284409800.0, 284409860.0, 284410080.0, 284410660.0, 284410940.0, 284410980.0, 284412740.0, 284413020.0, 284413100.0, 284413980.0, 284414370.0, 284414530.0, 284414600.0, 284414660.0, 284414750.0, 284414900.0, 284414980.0, 284415140.0, 284415200.0, 284415260.0, 284415420.0, 284415580.0, 284415780.0, 284415800.0, 284416100.0, 284416130.0, 284416260.0, 284416300.0, 284416540.0, 284417120.0, 284417250.0, 284417470.0, 284417820.0, 284418270.0, 284418340.0, 284418370.0, 284418430.0, 284418500.0, 284418700.0, 284418800.0, 284420540.0, 284421150.0, 284463040.0, 284463170.0, 284464300.0, 284464350.0, 284465250.0, 284465500.0, 284466700.0, 284466800.0, 284467520.0, 284468000.0, 284468320.0, 284468670.0, 284468740.0, 284468800.0, 284468860.0, 284469540.0, 284469760.0, 284469950.0, 284470050.0, 284470820.0, 284471170.0, 284471400.0, 284471740.0, 284471800.0, 284471970.0, 284472100.0, 284472930.0, 284473200.0, 284474050.0, 284474530.0, 284474700.0, 284474750.0, 284475140.0, 284475360.0, 284475460.0, 284475700.0, 284475940.0, 284476000.0, 284476800.0, 284476860.0, 284476900.0, 284477220.0, 284477570.0, 284477950.0, 284478000.0, 284478600.0, 284478700.0, 284478800.0, 284479040.0, 284479200.0, 284479650.0, 284480220.0, 284480320.0, 284480900.0, 284481100.0, 284481380.0, 284484930.0, 284485280.0, 284485380.0, 284524600.0, 284524640.0, 284527500.0, 284528960.0, 284529060.0, 284529820.0, 284530460.0, 284530750.0, 284530980.0, 284531520.0, 284531550.0, 284531650.0, 284532000.0, 284532030.0, 284532200.0, 284532350.0, 284533200.0, 284533220.0, 284533470.0, 284533630.0, 284533730.0, 284534100.0, 284534270.0, 284534300.0, 284534560.0, 284534750.0, 284534850.0, 284535230.0, 284535330.0, 284535600.0, 284535780.0, 284536100.0, 284536200.0, 284536300.0, 284536400.0, 284536500.0, 284536540.0, 284536670.0, 284536830.0, 284536960.0, 284537300.0, 284537380.0, 284537600.0, 284537800.0, 284540000.0, 284540500.0, 284541700.0, 284541800.0, 284542140.0, 284542240.0, 284544800.0, 284544830.0, 284545540.0, 284545630.0, 284545700.0, 284546430.0, 284546880.0, 284547680.0, 284589250.0, 284590370.0, 284591230.0, 284591500.0, 284592320.0, 284592600.0, 284592700.0, 284593380.0, 284593630.0, 284594020.0, 284594720.0, 284594780.0, 284594800.0, 284594850.0, 284597300.0, 284597380.0, 284597900.0, 284597950.0, 284598050.0, 284598530.0, 284598560.0, 284598700.0, 284598800.0, 284598880.0, 284598980.0, 284599740.0, 284600000.0, 284600100.0, 284600500.0, 284600740.0, 284600770.0, 284600960.0, 284601300.0, 284601380.0, 284601660.0, 284601820.0, 284601920.0, 284601950.0, 284602020.0, 284602560.0, 284602620.0, 284602880.0, 284603260.0, 284603400.0, 284603520.0, 284603550.0, 284603700.0, 284603800.0, 284603870.0, 284604030.0, 284604220.0, 284604500.0, 284605020.0, 284605280.0, 284605300.0, 284605920.0, 284606000.0, 284606050.0, 284606080.0, 284606180.0, 284606200.0, 284606530.0, 284606880.0, 284606940.0, 284608450.0, 284608500.0, 284608770.0, 284608900.0, 284609100.0, 284609400.0, 284610000.0, 284610300.0, 284648540.0, 284650270.0, 284650530.0, 284651140.0, 284651200.0, 284651330.0, 284651360.0, 284651420.0, 284651840.0, 284652000.0, 284653250.0, 284653820.0, 284654050.0, 284654200.0, 284654300.0, 284654340.0, 284654430.0, 284654460.0, 284654700.0, 284654720.0, 284655360.0, 284655580.0, 284655840.0, 284655870.0, 284656000.0, 284656030.0, 284656300.0, 284656400.0, 284657570.0, 284657630.0, 284658050.0, 284658370.0, 284658400.0, 284658430.0, 284659300.0, 284659420.0, 284660580.0, 284660600.0, 284661000.0, 284662430.0, 284662530.0, 284662560.0, 284662600.0, 284662800.0, 284663140.0, 284663360.0, 284664400.0, 284667000.0, 284667360.0, 284667970.0, 284668000.0, 284668640.0, 284668800.0, 284670340.0, 284671700.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8210227272727273\n",
      "Hamming Loss: 0.08948863636363637\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       578\n",
      "           1       0.18      1.00      0.30       126\n",
      "\n",
      "    accuracy                           0.18       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.18      0.05       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 337us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 125us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 135us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 136us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 119us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 114us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 120us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 158us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 150us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 152us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 148us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 153us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 165us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 110us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 107us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 105us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 104us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 102us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 96us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 13.7153 - binary_accuracy: 0.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-100876584.0, -100878330.0, -100879410.0, -100883300.0, -100883304.0, -100884140.0, -100884190.0, -100884380.0, -100884460.0, -100884600.0, -100884620.0, -100884830.0, -100885064.0, -100885080.0, -100885200.0, -100885384.0, -100885420.0, -100885550.0, -100886380.0, -100886550.0, -100886880.0, -100887190.0, -100887790.0, -100887870.0, -100888220.0, -100889110.0, -100889380.0, -100889390.0, -100889416.0, -100889840.0, -100889890.0, -100890056.0, -100890250.0, -100891064.0, -100891190.0, -100892130.0, -100892504.0, -100892550.0, -100892650.0, -100892936.0, -100897060.0, -100923670.0, -100930350.0, -100930400.0, -100930410.0, -100930620.0, -100930856.0, -100930940.0, -100931330.0, -100931420.0, -100931500.0, -100931600.0, -100931830.0, -100931944.0, -100932130.0, -100932200.0, -100932290.0, -100932340.0, -100932344.0, -100932660.0, -100932750.0, -100932780.0, -100932900.0, -100932920.0, -100933060.0, -100933070.0, -100933160.0, -100933250.0, -100933256.0, -100933320.0, -100933490.0, -100933630.0, -100933730.0, -100934104.0, -100934650.0, -100934750.0, -100934810.0, -100935000.0, -100935624.0, -100935650.0, -100935690.0, -100935870.0, -100935880.0, -100936060.0, -100936370.0, -100936420.0, -100936440.0, -100936590.0, -100936936.0, -100937300.0, -100937490.0, -100937550.0, -100937860.0, -100937864.0, -100937970.0, -100937980.0, -100938344.0, -100938430.0, -100938460.0, -100938670.0, -100938824.0, -100939560.0, -100939860.0, -100939940.0, -100940030.0, -100940990.0, -100941430.0, -100941704.0, -100942100.0, -100942400.0, -100942940.0, -100978970.0, -100979144.0, -100979150.0, -100979450.0, -100979540.0, -100979570.0, -100979976.0, -100980170.0, -100980184.0, -100980344.0, -100980360.0, -100980580.0, -100980600.0, -100980680.0, -100980970.0, -100981340.0, -100981750.0, -100981970.0, -100981980.0, -100982370.0, -100982460.0, -100982600.0, -100982696.0, -100982824.0, -100983016.0, -100983970.0, -100984250.0, -100984570.0, -100985144.0, -100985190.0, -100985580.0, -100985600.0, -100985820.0, -100985910.0, -100986070.0, -100986160.0, -100986400.0, -100987384.0, -100988510.0, -100988900.0, -100989820.0, -100990560.0, -100990830.0, -100992040.0, -101019176.0, -101025680.0, -101025704.0, -101025864.0, -101025870.0, -101026136.0, -101026190.0, -101026390.0, -101026560.0, -101026590.0, -101026630.0, -101026780.0, -101026860.0, -101027060.0, -101027170.0, -101027260.0, -101027576.0, -101027730.0, -101027770.0, -101027850.0, -101028180.0, -101028360.0, -101028400.0, -101028456.0, -101028504.0, -101028520.0, -101028616.0, -101028640.0, -101028664.0, -101028780.0, -101028810.0, -101029010.0, -101029030.0, -101029180.0, -101029224.0, -101029256.0, -101029270.0, -101029460.0, -101029500.0, -101029580.0, -101029840.0, -101029880.0, -101030344.0, -101030520.0, -101030860.0, -101030970.0, -101030980.0, -101031210.0, -101031336.0, -101031410.0, -101031610.0, -101031880.0, -101032024.0, -101032300.0, -101032500.0, -101032670.0, -101032790.0, -101033230.0, -101033830.0, -101033840.0, -101034720.0, -101034750.0, -101034990.0, -101035656.0, -101036264.0, -101071980.0, -101072300.0, -101073450.0, -101073740.0, -101073830.0, -101074184.0, -101074440.0, -101074530.0, -101074550.0, -101074560.0, -101075240.0, -101075510.0, -101075816.0, -101075840.0, -101075864.0, -101076104.0, -101076260.0, -101076440.0, -101076770.0, -101077100.0, -101077240.0, -101077460.0, -101077544.0, -101077550.0, -101077630.0, -101077690.0, -101077710.0, -101077720.0, -101077830.0, -101078296.0, -101078540.0, -101078584.0, -101078650.0, -101078980.0, -101079240.0, -101079420.0, -101079750.0, -101079784.0, -101080584.0, -101080690.0, -101080850.0, -101080860.0, -101081230.0, -101081640.0, -101082260.0, -101082380.0, -101082400.0, -101085450.0, -101085624.0, -101086060.0, -101118340.0, -101120690.0, -101121030.0, -101121210.0, -101121240.0, -101121400.0, -101121544.0, -101121740.0, -101122030.0, -101122050.0, -101122140.0, -101122160.0, -101122270.0, -101122296.0, -101123000.0, -101123070.0, -101123400.0, -101123630.0, -101123670.0, -101123784.0, -101124030.0, -101124310.0, -101124320.0, -101124370.0, -101124376.0, -101124390.0, -101124536.0, -101124584.0, -101124670.0, -101124740.0, -101124904.0, -101125064.0, -101125330.0, -101125370.0, -101125450.0, -101125500.0, -101126190.0, -101126270.0, -101126370.0, -101126584.0, -101127240.0, -101127350.0, -101127860.0, -101128370.0, -101128560.0, -101128790.0, -101129544.0, -101130170.0, -101130650.0, -101132216.0, -101132370.0, -101167336.0, -101167670.0, -101167710.0, -101169090.0, -101169896.0, -101170030.0, -101170056.0, -101170360.0, -101172504.0, -101172664.0, -101173060.0, -101173530.0, -101173544.0, -101173590.0, -101173704.0, -101173830.0, -101174104.0, -101174110.0, -101174130.0, -101174296.0, -101174450.0, -101174504.0, -101174590.0, -101174984.0, -101175000.0, -101175610.0, -101175740.0, -101175896.0, -101176120.0, -101176616.0, -101176790.0, -101176856.0, -101176940.0, -101177300.0, -101177560.0, -101177576.0, -101177736.0, -101178390.0, -101178770.0, -101179050.0, -101179130.0, -101179180.0, -101179260.0, -101179850.0, -101180110.0, -101180200.0, -101180700.0, -101213020.0, -101213530.0, -101214520.0, -101214820.0, -101214890.0, -101215200.0, -101215210.0, -101215220.0, -101215330.0, -101215370.0, -101215440.0, -101215530.0, -101215740.0, -101215760.0, -101215780.0, -101215784.0, -101216020.0, -101216080.0, -101216090.0, -101216264.0, -101216296.0, -101216376.0, -101216450.0, -101216540.0, -101216730.0, -101216744.0, -101216880.0, -101217030.0, -101217200.0, -101217380.0, -101217416.0, -101217670.0, -101218024.0, -101218080.0, -101218376.0, -101218500.0, -101218700.0, -101218990.0, -101219064.0, -101219080.0, -101219140.0, -101219224.0, -101219240.0, -101219620.0, -101219970.0, -101220180.0, -101220290.0, -101220424.0, -101220504.0, -101220510.0, -101220530.0, -101220590.0, -101220670.0, -101220690.0, -101220790.0, -101221064.0, -101221520.0, -101221540.0, -101221600.0, -101221670.0, -101222790.0, -101222820.0, -101222830.0, -101223130.0, -101223210.0, -101223320.0, -101223390.0, -101223410.0, -101223560.0, -101223910.0, -101225210.0, -101225720.0, -101226104.0, -101226140.0, -101226310.0, -101226540.0, -101226590.0, -101226670.0, -101227150.0, -101227464.0, -101257310.0, -101260504.0, -101260620.0, -101260670.0, -101261580.0, -101261970.0, -101262150.0, -101262210.0, -101262250.0, -101262376.0, -101262830.0, -101263020.0, -101263040.0, -101263096.0, -101263140.0, -101263450.0, -101263610.0, -101263784.0, -101263970.0, -101264430.0, -101264540.0, -101264840.0, -101264950.0, -101265250.0, -101265256.0, -101265340.0, -101265360.0, -101265420.0, -101265790.0, -101266010.0, -101266250.0, -101266560.0, -101267050.0, -101267080.0, -101267100.0, -101267144.0, -101267190.0, -101267370.0, -101267390.0, -101268160.0, -101268260.0, -101268520.0, -101268530.0, -101269144.0, -101269570.0, -101269600.0, -101270504.0, -101270580.0, -101270860.0, -101270936.0, -101271190.0, -101271210.0, -101271490.0, -101271550.0, -101271560.0, -101271940.0, -101272024.0, -101272320.0, -101272440.0, -101272860.0, -101272870.0, -101273090.0, -101273830.0, -101273900.0, -101274540.0, -101302696.0, -101306056.0, -101308210.0, -101308320.0, -101309870.0, -101310000.0, -101310030.0, -101310290.0, -101311100.0, -101311180.0, -101311210.0, -101311420.0, -101311540.0, -101311650.0, -101311864.0, -101312140.0, -101312380.0, -101312560.0, -101313020.0, -101313460.0, -101313570.0, -101313750.0, -101313830.0, -101313900.0, -101313940.0, -101314030.0, -101314060.0, -101314240.0, -101314460.0, -101315064.0, -101315960.0, -101316210.0, -101316550.0, -101316590.0, -101316660.0, -101316700.0, -101316710.0, -101316740.0, -101316800.0, -101317250.0, -101317300.0, -101317310.0, -101317340.0, -101317560.0, -101317780.0, -101317840.0, -101317860.0, -101318070.0, -101318170.0, -101318200.0, -101318290.0, -101318480.0, -101318540.0, -101318710.0, -101318744.0, -101318776.0, -101319030.0, -101319060.0, -101319420.0, -101319544.0, -101319790.0, -101319900.0, -101320136.0, -101320370.0, -101320410.0, -101320580.0, -101320840.0, -101321096.0, -101354790.0, -101355100.0, -101355840.0, -101355970.0, -101356024.0, -101356490.0, -101356670.0, -101356750.0, -101356760.0, -101356840.0, -101356940.0, -101357260.0, -101357816.0, -101357890.0, -101357960.0, -101358530.0, -101358720.0, -101359360.0, -101359500.0, -101359540.0, -101359650.0, -101359690.0, -101359710.0, -101359720.0, -101359790.0, -101359810.0, -101360060.0, -101360264.0, -101360380.0, -101360710.0, -101361304.0, -101361470.0, -101362340.0, -101362910.0, -101362980.0, -101363060.0, -101363090.0, -101363220.0, -101363310.0, -101363760.0, -101363960.0, -101364160.0, -101364390.0, -101364410.0, -101364470.0, -101364600.0, -101365220.0, -101366540.0, -101366810.0, -101367200.0, -101367310.0, -101367910.0, -101367950.0, -101368130.0, -101368230.0, -101368240.0, -101368390.0, -101398400.0, -101401020.0, -101402264.0, -101402810.0, -101403624.0, -101403656.0, -101403700.0, -101403770.0, -101404030.0, -101404190.0, -101404300.0, -101404360.0, -101404590.0, -101404710.0, -101404740.0, -101404800.0, -101404810.0, -101404830.0, -101405096.0, -101405120.0, -101405550.0, -101405896.0, -101406370.0, -101406510.0, -101406536.0, -101406560.0, -101406650.0, -101406664.0, -101407120.0, -101407230.0, -101407380.0, -101407460.0, -101407470.0, -101407910.0, -101408010.0, -101408110.0, -101408330.0, -101408460.0, -101408696.0, -101409440.0, -101409610.0, -101409930.0, -101410536.0, -101410790.0, -101411370.0, -101411420.0, -101412400.0, -101412424.0, -101412480.0, -101412660.0, -101414160.0, -101414560.0, -101415290.0, -101415470.0, -101416424.0, 11636136.0, 11636258.0, 11636922.0, 11638364.0, 11638546.0, 11639632.0, 11639776.0, 11639936.0, 11640204.0, 11640220.0, 11640688.0, 11640882.0, 11641130.0, 11641506.0, 11642136.0, 11642206.0, 11642324.0, 11642330.0, 11642364.0, 11642922.0, 11642986.0, 11643074.0, 11643108.0, 11643166.0, 11643554.0, 11643586.0, 11643616.0, 11645186.0, 11645210.0, 11645228.0, 11645260.0, 11645886.0, 11646226.0, 11646554.0, 11646802.0, 11647676.0, 11647842.0, 11648214.0, 11648676.0, 11648802.0, 11648904.0, 11649392.0, 11649620.0, 11649656.0, 11649678.0, 11649816.0, 11650380.0, 11651448.0, 11651456.0, 11651638.0, 11651746.0, 11651768.0, 11652070.0, 11652152.0, 11652834.0, 11653232.0, 11676152.0, 11676792.0, 11677816.0, 11678818.0, 11679408.0, 11679664.0, 11679932.0, 11680124.0, 11680962.0, 11681022.0, 11681084.0, 11681120.0, 11681140.0, 11681154.0, 11681290.0, 11681418.0, 11681502.0, 11681508.0, 11681592.0, 11681800.0, 11681804.0, 11681842.0, 11682236.0, 11682366.0, 11682708.0, 11682778.0, 11683244.0, 11683272.0, 11683768.0, 11684174.0, 11684272.0, 11684592.0, 11684624.0, 11684658.0, 11685064.0, 11685266.0, 11685350.0, 11685362.0, 11685380.0, 11685404.0, 11685574.0, 11687752.0, 11687758.0, 11687954.0, 11688004.0, 11688038.0, 11688062.0, 11688100.0, 11688256.0, 11688500.0, 11688556.0, 11689362.0, 11689366.0, 11689376.0, 11689540.0, 11689724.0, 11689832.0, 11690398.0, 11691100.0, 11691168.0, 11692012.0, 11711614.0, 11711744.0, 11711916.0, 11712354.0, 11712774.0, 11713172.0, 11713502.0, 11713552.0, 11713590.0, 11713654.0, 11713980.0, 11714024.0, 11714368.0, 11714436.0, 11714454.0, 11714478.0, 11714648.0, 11714652.0, 11714738.0, 11714760.0, 11715080.0, 11717034.0, 11717114.0, 11717128.0, 11717182.0, 11717346.0, 11717980.0, 11718312.0, 11718788.0, 11719112.0, 11719124.0, 11719418.0, 11719492.0, 11719970.0, 11720068.0, 11720168.0, 11720534.0, 11720658.0, 11720710.0, 11720734.0, 11721488.0, 11721654.0, 11721756.0, 11721800.0, 11721848.0, 11721954.0, 11722260.0, 11722350.0, 11722660.0, 11723230.0, 11723260.0, 11724144.0, 11724906.0, 11725284.0, 11725570.0, 11725758.0, 11725762.0, 11725972.0, 11726048.0, 11726246.0, 11726444.0, 11726466.0, 11727294.0, 11727584.0, 11727742.0, 11727990.0, 11728746.0, 11730232.0, 11730310.0, 11752056.0, 11752380.0, 11752720.0, 11752806.0, 11754146.0, 11754642.0, 11755312.0, 11756942.0, 11758032.0, 11758040.0, 11758104.0, 11758272.0, 11758562.0, 11759076.0, 11759124.0, 11759246.0, 11759328.0, 11759418.0, 11759598.0, 11759650.0, 11759670.0, 11759680.0, 11759814.0, 11760054.0, 11760450.0, 11760726.0, 11760954.0, 11760982.0, 11761756.0, 11762006.0, 11762016.0, 11763056.0, 11763150.0, 11763238.0, 11763258.0, 11763828.0, 11763880.0, 11763888.0, 11763938.0, 11763964.0, 11764080.0, 11764162.0, 11764532.0, 11765234.0, 11765422.0, 11765492.0, 11765496.0, 11765606.0, 11765902.0, 11765992.0, 11766346.0, 11766350.0, 11767472.0, 11767486.0, 11767546.0, 11767602.0, 11767694.0, 11768074.0, 11768326.0, 11768416.0, 11769368.0, 11769376.0, 11769502.0, 11785546.0, 11785844.0, 11787916.0, 11788156.0, 11789886.0, 11790250.0, 11790558.0, 11790646.0, 11790810.0, 11791260.0, 11791632.0, 11791734.0, 11791898.0, 11791906.0, 11792480.0, 11792920.0, 11792966.0, 11793226.0, 11793602.0, 11794956.0, 11795442.0, 11795534.0, 11795654.0, 11796134.0, 11796452.0, 11796788.0, 11796938.0, 11796940.0, 11797196.0, 11797240.0, 11797306.0, 11797314.0, 11797642.0, 11797844.0, 11797856.0, 11797920.0, 11797936.0, 11797952.0, 11798004.0, 11798010.0, 11798114.0, 11798220.0, 11798550.0, 11799926.0, 11799956.0, 11800024.0, 11800038.0, 11800380.0, 11800606.0, 11800914.0, 11801004.0, 11801610.0, 11802062.0, 11802338.0, 11802370.0, 11802500.0, 11802504.0, 11802950.0, 11803142.0, 11803534.0, 11803560.0, 11803594.0, 11803720.0, 11804236.0, 11804302.0, 11804596.0, 11804770.0, 11805136.0, 11805188.0, 11805206.0, 11805448.0, 11805618.0, 11805868.0, 11805898.0, 11805954.0, 11806184.0, 11806212.0, 11806242.0, 11806256.0, 11807102.0, 11824566.0, 11827666.0, 11827952.0, 11828032.0, 11828380.0, 11828530.0, 11829002.0, 11829230.0, 11829298.0, 11829350.0, 11829404.0, 11829856.0, 11830058.0, 11830090.0, 11830912.0, 11831644.0, 11831796.0, 11832010.0, 11832114.0, 11832276.0, 11832278.0, 11833380.0, 11833798.0, 11834612.0, 11834804.0, 11834812.0, 11834864.0, 11834892.0, 11835786.0, 11835824.0, 11836538.0, 11837628.0, 11838118.0, 11838350.0, 11839144.0, 11839648.0, 11839814.0, 11840056.0, 11840454.0, 11840458.0, 11841246.0, 11843756.0, 11843758.0, 11844084.0, 11844120.0, 11844576.0, 11844578.0, 11866028.0, 11867024.0, 11867078.0, 11867106.0, 11867168.0, 11867260.0, 11867716.0, 11867834.0, 11867846.0, 11867902.0, 11867908.0, 11868322.0, 11868834.0, 11868870.0, 11869046.0, 11869138.0, 11869390.0, 11869512.0, 11869858.0, 11870748.0, 11871414.0, 11871670.0, 11872278.0, 11872280.0, 11872326.0, 11872334.0, 11872690.0, 11873116.0, 11873158.0, 11873622.0, 11874042.0, 11874238.0, 11874310.0, 11874618.0, 11874708.0, 11874792.0, 11874858.0, 11874864.0, 11875354.0, 11875646.0, 11876274.0, 11877194.0, 11879146.0, 11879478.0, 11879730.0, 11879938.0, 11880234.0, 11880430.0, 11881006.0, 11881764.0, 11881776.0, 11882758.0, 11903736.0, 11903962.0, 11904102.0, 11904190.0, 11905550.0, 11906008.0, 11907062.0, 11907660.0, 11907992.0, 11908222.0, 11908250.0, 11908382.0, 11908506.0, 11909268.0, 11909486.0, 11909530.0, 11909766.0, 11909792.0, 11910018.0, 11910138.0, 11910328.0, 11910362.0, 11910436.0, 11910498.0, 11910992.0, 11911082.0, 11911096.0, 11911274.0, 11911428.0, 11911482.0, 11911816.0, 11912066.0, 11912386.0, 11912618.0, 11913326.0, 11914224.0, 11914250.0, 11915354.0, 11916714.0, 11917368.0, 11918064.0, 11918076.0, 11918082.0, 11918126.0, 11918152.0, 11918446.0, 11919250.0, 11919312.0, 11920920.0, 11921360.0, 11921522.0, 11944552.0, 11944576.0, 11944892.0, 11944972.0, 11946002.0, 11946378.0, 11946466.0, 11946530.0, 11946670.0, 11946812.0, 11946824.0, 11946848.0, 11947008.0, 11947204.0, 11947322.0, 11947734.0, 11947796.0, 11948006.0, 11948086.0, 11948120.0, 11948426.0, 11948732.0, 11948774.0, 11948866.0, 11948942.0, 11948980.0, 11949332.0, 11950032.0, 11950472.0, 11950596.0, 11950730.0, 11950870.0, 11950880.0, 11950960.0, 11951614.0, 11951770.0, 11951940.0, 11952048.0, 11952314.0, 11952402.0, 11952610.0, 11952756.0, 11952820.0, 11952822.0, 11952850.0, 11952984.0, 11953092.0, 11953194.0, 11953366.0, 11953388.0, 11954508.0, 11955384.0, 11955538.0, 11956226.0, 11956534.0, 11957126.0, 11957642.0, 11958276.0, 11958304.0, 11958388.0, 11958392.0, 11958422.0, 11959908.0, 11960664.0, 11960676.0, 11982538.0, 11982598.0, 11982886.0, 11982916.0, 11983930.0, 11984848.0, 11984890.0, 11985008.0, 11985352.0, 11985536.0, 11985566.0, 11985666.0, 11986328.0, 11986698.0, 11988098.0, 11988354.0, 11988756.0, 11988930.0, 11989110.0, 11989226.0, 11989366.0, 11989450.0, 11989866.0, 11989908.0, 11990090.0, 11990140.0, 11990398.0, 11990446.0, 11990464.0, 11990850.0, 11990984.0, 11990988.0, 11991142.0, 11991144.0, 11991350.0, 11991418.0, 11991562.0, 11991598.0, 11992014.0, 11992052.0, 11992780.0, 11993322.0, 11993598.0, 11998278.0, 12018444.0, 12018744.0, 12019014.0, 12020088.0, 12020102.0, 12020580.0, 12020858.0, 12021604.0, 12021624.0, 12021640.0, 12022524.0, 12022694.0, 12023158.0, 12024482.0, 12024684.0, 12024730.0, 12025140.0, 12025548.0, 12025672.0, 12025880.0, 12026240.0, 12026260.0, 12026324.0, 12026620.0, 12026698.0, 12027038.0, 12027056.0, 12027102.0, 12027120.0, 12027182.0, 12027202.0, 12027822.0, 12027954.0, 12028770.0, 12028856.0, 12028894.0, 12028938.0, 12029032.0, 12029060.0, 12029232.0, 12029356.0, 12029454.0, 12029722.0, 12029904.0, 12029982.0, 12030498.0, 12030586.0, 12031224.0, 12031252.0, 12031304.0, 12031798.0, 12031898.0, 12032318.0, 12033456.0, 12033858.0, 12034104.0, 12034194.0, 12034710.0, 12035186.0, 12035520.0, 12035548.0, 12035848.0, 12036208.0, 12036576.0, 12036670.0, 12036712.0, 12037074.0, 12037098.0, 12037374.0, 12037442.0, 12038536.0, 12038616.0, 12058300.0, 12058728.0, 12058762.0, 12058776.0, 12059136.0, 12061094.0, 12061206.0, 12061334.0, 12062018.0, 12062040.0, 12062640.0, 12062864.0, 12063342.0, 12063378.0, 12063844.0, 12064700.0, 12065026.0, 12065082.0, 12065098.0, 12065894.0, 12066060.0, 12066062.0, 12066784.0, 12067010.0, 12067328.0, 12067804.0, 12068064.0, 12068310.0, 12068386.0, 12068658.0, 12068710.0, 12068754.0, 12068820.0, 12069218.0, 12069262.0, 12069314.0, 12069466.0, 12075392.0, 12076198.0, 12078468.0, 12078486.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7684659090909091\n",
      "Hamming Loss: 0.125\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       584\n",
      "           1       0.17      1.00      0.29       120\n",
      "\n",
      "    accuracy                           0.17       704\n",
      "   macro avg       0.09      0.50      0.15       704\n",
      "weighted avg       0.03      0.17      0.05       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 266us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 1.6950 - binary_accuracy: 0.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-102726990.0, -102727460.0, -102727490.0, -102727500.0, -102727520.0, -102727540.0, -102727570.0, -102727576.0, -102727580.0, -102727600.0, -102727610.0, -102727620.0, -102727624.0, -102727630.0, -102727650.0, -102727660.0, -102727680.0, -102727700.0, -102727720.0, -102727750.0, -102727760.0, -102727780.0, -102727800.0, -102727810.0, -102727820.0, -102727840.0, -102727860.0, -102727864.0, -102727870.0, -102727890.0, -102727900.0, -102727910.0, -102727920.0, -102727930.0, -102727940.0, -102727944.0, -102727950.0, -102727960.0, -102727970.0, -102727980.0, -102728000.0, -102728010.0, -102728020.0, -102728030.0, -102728040.0, -102728060.0, -102728100.0, -102728104.0, -102728110.0, -102728140.0, -102728150.0, -102728160.0, -102728180.0, -102728184.0, -102728190.0, -102728210.0, -102728220.0, -102728230.0, -102728240.0, -102728250.0, -102728264.0, -102728270.0, -102728280.0, -102728290.0, -102728296.0, -102728300.0, -102728320.0, -102728330.0, -102728340.0, -102728344.0, -102728350.0, -102728370.0, -102728376.0, -102728380.0, -102728390.0, -102728400.0, -102728420.0, -102728430.0, -102728440.0, -102728450.0, -102728480.0, -102728500.0, -102728510.0, -102728520.0, -102728530.0, -102728540.0, -102728560.0, -102728580.0, -102728590.0, -102728600.0, -102728610.0, -102728616.0, -102728620.0, -102728630.0, -102728640.0, -102728650.0, -102728660.0, -102728664.0, -102728670.0, -102728680.0, -102728690.0, -102728696.0, -102728700.0, -102728710.0, -102728720.0, -102728740.0, -102728750.0, -102728776.0, -102728780.0, -102728790.0, -102728820.0, -102728830.0, -102728840.0, -102728860.0, -102728870.0, -102728880.0, -102728900.0, -102728910.0, -102728930.0, -102728936.0, -102728940.0, -102728950.0, -102728960.0, -102728970.0, -102728980.0, -102728984.0, -102728990.0, -102729000.0, -102729010.0, -102729016.0, -102729020.0, -102729040.0, -102729060.0, -102729064.0, -102729090.0, -102729096.0, -102729100.0, -102729110.0, -102729120.0, -102729150.0, -102729160.0, -102729170.0, -102729176.0, -102729190.0, -102729200.0, -102729210.0, -102729220.0, -102729224.0, -102729230.0, -102729240.0, -102729250.0, -102729256.0, -102729260.0, -102729270.0, -102729280.0, -102729300.0, -102729304.0, -102729310.0, -102729330.0, -102729336.0, -102729340.0, -102729360.0, -102729390.0, -102729410.0, -102729416.0, -102729420.0, -102729460.0, -102729470.0, -102729480.0, -102729490.0, -102729500.0, -102729520.0, -102729540.0, -102729544.0, -102729550.0, -102729570.0, -102729576.0, -102729580.0, -102729590.0, -102729600.0, -102729610.0, -102729620.0, -102729624.0, -102729630.0, -102729650.0, -102729656.0, -102729660.0, -102729680.0, -102729730.0, -102729760.0, -102729770.0, -102729780.0, -102729790.0, -102729810.0, -102729820.0, -102729880.0, -102729890.0, -102729896.0, -102729900.0, -102729930.0, -102729940.0, -102729944.0, -102729950.0, -102729976.0, -102729980.0, -102729990.0, -102730000.0, -102730020.0, -102730060.0, -102730080.0, -102730100.0, -102730110.0, -102730120.0, -102730200.0, -102730350.0, -102730440.0, -102730470.0, -102730500.0, -102730530.0, -102730540.0, -102730580.0, -102730616.0, -102730640.0, -102730740.0, -102730750.0, -102730780.0, -102730960.0, -102730970.0, -102730980.0, -102731010.0, -102731020.0, -102731170.0, -102731210.0, -102731490.0, -102731540.0, -102731640.0, -102731700.0, -102731710.0, -102731830.0, -102731860.0, -102732184.0, -102732190.0, -102732210.0, -102732320.0, -102732370.0, -102732380.0, -102732400.0, -102732500.0, -102732590.0, -102732790.0, -102732820.0, -102732950.0, -102733016.0, -102733040.0, -102733070.0, -102733440.0, -102741220.0, -102741400.0, -102741540.0, -102741570.0, -102741730.0, -102742056.0, -102742100.0, -102742220.0, -102742264.0, -102742330.0, -102742380.0, -102742450.0, -102742490.0, -102742500.0, -102742520.0, -102742540.0, -102742820.0, -102742830.0, -102742900.0, -102742920.0, -102743090.0, -102743100.0, -102743330.0, -102743360.0, -102743400.0, -102743410.0, -102743450.0, -102743540.0, -102743630.0, -102743670.0, -102743750.0, -102743770.0, -102743864.0, -102743944.0, -102743980.0, -102744020.0, -102744130.0, -102744136.0, -102744420.0, -102751510.0, -102752800.0, -102753040.0, -102753090.0, -102753100.0, -102753200.0, -102753360.0, -102753390.0, -102753420.0, -102753540.0, -102753544.0, -102753600.0, -102753630.0, -102753710.0, -102753760.0, -102753780.0, -102753860.0, -102753870.0, -102753920.0, -102753940.0, -102753970.0, -102754030.0, -102754104.0, -102754220.0, -102754240.0, -102754430.0, -102754450.0, -102754470.0, -102754580.0, -102754600.0, -102754660.0, -102754830.0, -102754856.0, -102754880.0, -102754960.0, -102755070.0, -102755520.0, -102755560.0, -102755610.0, -102755620.0, -102755710.0, -102755850.0, -102755860.0, -102756030.0, -102764360.0, -102764590.0, -102764980.0, -102765220.0, -102765310.0, -102765420.0, -102765470.0, -102765520.0, -102765540.0, -102765630.0, -102765700.0, -102765810.0, -102765890.0, -102765920.0, -102765944.0, -102765980.0, -102766010.0, -102766020.0, -102766030.0, -102766110.0, -102766200.0, -102766216.0, -102766300.0, -102766350.0, -102766370.0, -102766430.0, -102766456.0, -102766460.0, -102766480.0, -102766500.0, -102766510.0, -102766560.0, -102766580.0, -102766584.0, -102766690.0, -102766750.0, -102767170.0, -102767190.0, -102767260.0, -102767380.0, -102767384.0, -102767390.0, -102767460.0, -102767490.0, -102767500.0, -102767540.0, -102767570.0, -102767680.0, -102767720.0, -102767760.0, -102767790.0, -102774560.0, -102775950.0, -102776030.0, -102776180.0, -102776190.0, -102776216.0, -102776260.0, -102776270.0, -102776380.0, -102776400.0, -102776430.0, -102776616.0, -102776640.0, -102776660.0, -102776710.0, -102776720.0, -102776870.0, -102776880.0, -102776890.0, -102776900.0, -102776980.0, -102777120.0, -102777144.0, -102777160.0, -102777230.0, -102777260.0, -102777336.0, -102777350.0, -102777360.0, -102777650.0, -102777700.0, -102777780.0, -102777790.0, -102777810.0, -102777820.0, -102777830.0, -102777840.0, -102777860.0, -102778024.0, -102778130.0, -102778170.0, -102778184.0, -102778210.0, -102778220.0, -102778240.0, -102778270.0, -102778330.0, -102778350.0, -102778616.0, -102778856.0, -102778950.0, -102778990.0, -102779080.0, -102779100.0, -102779250.0, -102779310.0, -102779340.0, -102779410.0, 162246900.0, 162254960.0, 162255650.0, 162255740.0, 162255970.0, 162256050.0, 162256220.0, 162256340.0, 162256500.0, 162256540.0, 162256590.0, 162256860.0, 162257090.0, 162257140.0, 162257150.0, 162257180.0, 162257340.0, 162257440.0, 162257600.0, 162257620.0, 162257630.0, 162257780.0, 162257820.0, 162257890.0, 162257920.0, 162258140.0, 162258180.0, 162258220.0, 162258260.0, 162258320.0, 162258350.0, 162258370.0, 162258400.0, 162258430.0, 162258500.0, 162258600.0, 162258830.0, 162258860.0, 162258910.0, 162259040.0, 162259090.0, 162259140.0, 162259200.0, 162259340.0, 162259420.0, 162259460.0, 162259490.0, 162259650.0, 162259660.0, 162259710.0, 162259860.0, 162260000.0, 162260160.0, 162260210.0, 162260380.0, 162260640.0, 162260740.0, 162260930.0, 162261250.0, 162267950.0, 162267970.0, 162268020.0, 162268450.0, 162268460.0, 162269260.0, 162269340.0, 162269500.0, 162269600.0, 162269760.0, 162269920.0, 162269940.0, 162270020.0, 162270050.0, 162270260.0, 162270270.0, 162270300.0, 162270340.0, 162270500.0, 162270530.0, 162270540.0, 162270600.0, 162270690.0, 162270800.0, 162270820.0, 162270940.0, 162270960.0, 162271120.0, 162271140.0, 162271150.0, 162271170.0, 162271200.0, 162271410.0, 162271440.0, 162271520.0, 162271550.0, 162271780.0, 162271790.0, 162272000.0, 162272060.0, 162272320.0, 162272420.0, 162272700.0, 162272960.0, 162273090.0, 162273120.0, 162273310.0, 162273390.0, 162273630.0, 162273810.0, 162274640.0, 162282020.0, 162282400.0, 162282480.0, 162283220.0, 162283230.0, 162283580.0, 162283680.0, 162283840.0, 162284320.0, 162284340.0, 162284380.0, 162284400.0, 162284450.0, 162284590.0, 162284600.0, 162284800.0, 162284830.0, 162284880.0, 162284900.0, 162285100.0, 162285150.0, 162285200.0, 162285340.0, 162285360.0, 162285420.0, 162285470.0, 162285540.0, 162285550.0, 162285630.0, 162285710.0, 162285740.0, 162285980.0, 162286100.0, 162286190.0, 162286200.0, 162286750.0, 162287060.0, 162287280.0, 162287330.0, 162287710.0, 162288050.0, 162288290.0, 162295740.0, 162296220.0, 162296270.0, 162296290.0, 162296430.0, 162296900.0, 162297250.0, 162297280.0, 162297300.0, 162297340.0, 162297550.0, 162297630.0, 162297760.0, 162298290.0, 162298500.0, 162298510.0, 162298720.0, 162298740.0, 162298780.0, 162298830.0, 162298940.0, 162298980.0, 162299170.0, 162299260.0, 162299400.0, 162299410.0, 162299440.0, 162299460.0, 162299550.0, 162299680.0, 162300060.0, 162300350.0, 162300500.0, 162301060.0, 162301250.0, 162301260.0, 162301860.0, 162309570.0, 162310030.0, 162310270.0, 162310370.0, 162310430.0, 162311000.0, 162311090.0, 162311140.0, 162311460.0, 162311650.0, 162311710.0, 162311730.0, 162311740.0, 162311790.0, 162311950.0, 162311980.0, 162312080.0, 162312420.0, 162312450.0, 162312480.0, 162312690.0, 162312960.0, 162312980.0, 162313060.0, 162313180.0, 162313250.0, 162313260.0, 162313280.0, 162313390.0, 162313840.0, 162313950.0, 162314340.0, 162314370.0, 162314400.0, 162314460.0, 162314560.0, 162314580.0, 162314620.0, 162314670.0, 162314720.0, 162315040.0, 162315070.0, 162315170.0, 162315260.0, 162315310.0, 162315460.0, 162315490.0, 162315520.0, 162315540.0, 162315600.0, 162315650.0, 162327730.0, 162328050.0, 162328400.0, 162328450.0, 162328780.0, 162328830.0, 162328900.0, 162328960.0, 162328980.0, 162329090.0, 162329360.0, 162329470.0, 162329730.0, 162329810.0, 162329860.0, 162329920.0, 162330020.0, 162330190.0, 162330200.0, 162330320.0, 162330400.0, 162330420.0, 162330610.0, 162330620.0, 162330820.0, 162330880.0, 162331070.0, 162331490.0, 162331550.0, 162331630.0, 162331820.0, 162331840.0, 162331950.0, 162332030.0, 162332110.0, 162332860.0, 162333020.0, 162333680.0, 162333730.0, 162334380.0, 162334560.0, 162334600.0, 162334690.0, 162334720.0, 162334850.0, 162335150.0, 162335170.0, 162348580.0, 162349070.0, 162350180.0, 162350300.0, 162350480.0, 162350540.0, 162350800.0, 162350820.0, 162350880.0, 162350910.0, 162350960.0, 162351120.0, 162351200.0, 162351260.0, 162351420.0, 162351570.0, 162351730.0, 162351940.0, 162352060.0, 162352380.0, 162352450.0, 162352500.0, 162352510.0, 162352600.0, 162352620.0, 162352740.0, 162352750.0, 162352820.0, 162352830.0, 162352880.0, 162352900.0, 162352980.0, 162353020.0, 162353060.0, 162353220.0, 162353460.0, 162353540.0, 162353620.0, 162353630.0, 162353800.0, 162353820.0, 162353860.0, 162353870.0, 162353980.0, 162354060.0, 162354140.0, 162354200.0, 162354420.0, 162354430.0, 162354770.0, 162354820.0, 162354910.0, 162355020.0, 162355100.0, 162355260.0, 162355360.0, 162355540.0, 162355550.0, 162355630.0, 162356000.0, 162356130.0, 162356430.0, 162356600.0, 162369570.0, 162369810.0, 162370370.0, 162370500.0, 162371150.0, 162371200.0, 162371300.0, 162371360.0, 162371400.0, 162371550.0, 162371600.0, 162371780.0, 162371870.0, 162372000.0, 162372290.0, 162372350.0, 162372420.0, 162372560.0, 162372600.0, 162372660.0, 162372750.0, 162372800.0, 162372880.0, 162373000.0, 162373040.0, 162373150.0, 162373180.0, 162373280.0, 162373310.0, 162373330.0, 162373520.0, 162373580.0, 162373600.0, 162373710.0, 162373980.0, 162374110.0, 162374200.0, 162374260.0, 162374340.0, 162374370.0, 162374460.0, 162374560.0, 162374690.0, 162374780.0, 162374830.0, 162374990.0, 162375000.0, 162375380.0, 162375420.0, 162375460.0, 162375550.0, 162375620.0, 162375630.0, 162375890.0, 162375920.0, 162376500.0, 162376540.0, 162376830.0, 162376900.0, 162377100.0, 162377420.0, 162377520.0, 162377620.0, 162377890.0, 162391800.0, 162392080.0, 162392670.0, 162392880.0, 162393360.0, 162393380.0, 162393400.0, 162393500.0, 162393540.0, 162393710.0, 162393730.0, 162393780.0, 162393970.0, 162393980.0, 162394080.0, 162394240.0, 162394340.0, 162394420.0, 162394430.0, 162394450.0, 162394480.0, 162394500.0, 162394830.0, 162395000.0, 162395140.0, 162395200.0, 162395250.0, 162395340.0, 162395360.0, 162395410.0, 162395620.0, 162395630.0, 162395730.0, 162395800.0, 162395840.0, 162395900.0, 162396100.0, 162396210.0, 162396300.0, 162396380.0, 162396420.0, 162396450.0, 162396510.0, 162396600.0, 162396640.0, 162396660.0, 162396740.0, 162396800.0, 162396880.0, 162396900.0, 162397120.0, 162397200.0, 162397230.0, 162397250.0, 162397260.0, 162397500.0, 162397600.0, 162397740.0, 162397840.0, 162397860.0, 162397890.0, 162398020.0, 162398080.0, 162398220.0, 162398270.0, 162398290.0, 162398320.0, 162398340.0, 162398660.0, 162413460.0, 162413570.0, 162413710.0, 162414060.0, 162414220.0, 162414240.0, 162414340.0, 162414350.0, 162414380.0, 162414400.0, 162414420.0, 162414430.0, 162414460.0, 162414560.0, 162414610.0, 162414640.0, 162414690.0, 162414740.0, 162414850.0, 162415020.0, 162415760.0, 162415950.0, 162416060.0, 162416220.0, 162416240.0, 162416260.0, 162416290.0, 162416300.0, 162416450.0, 162416510.0, 162416600.0, 162416640.0, 162416670.0, 162416720.0, 162416850.0, 162416900.0, 162416930.0, 162416960.0, 162417060.0, 162417310.0, 162417400.0, 162417440.0, 162417540.0, 162417660.0, 162417950.0, 162418000.0, 162418020.0, 162418100.0, 162418190.0, 162418320.0, 162418450.0, 162418580.0, 162418700.0, 162418780.0, 162418880.0, 162418910.0, 162419040.0, 162419100.0, 162419260.0, 162419410.0, 162419580.0, 162419700.0, 162419740.0, 162427490.0, 162433550.0, 162434320.0, 162434370.0, 162434480.0, 162434600.0, 162434780.0, 162434830.0, 162434980.0, 162435040.0, 162435200.0, 162435300.0, 162435460.0, 162435520.0, 162435550.0, 162435630.0, 162435700.0, 162435710.0, 162435780.0, 162435790.0, 162435860.0, 162435890.0, 162436030.0, 162436060.0, 162436260.0, 162436290.0, 162436370.0, 162436380.0, 162436450.0, 162436580.0, 162436590.0, 162436690.0, 162436700.0, 162436770.0, 162436780.0, 162436880.0, 162436900.0, 162437000.0, 162437070.0, 162437090.0, 162437100.0, 162437660.0, 162437810.0, 162438110.0, 162438140.0, 162438340.0, 162438370.0, 162438460.0, 162438510.0, 162438530.0, 162438620.0, 162438910.0, 162439150.0, 162439230.0, 162439520.0, 162439580.0, 162439740.0, 162439790.0, 162439820.0, 162440000.0, 162440210.0, 162440590.0, 162440600.0, 162440780.0, 162440830.0, 162454270.0, 162455490.0, 162455630.0, 162455840.0, 162455900.0, 162456130.0, 162457280.0, 162458420.0, 162458580.0, 162459550.0, 162459620.0, 162459650.0, 162459660.0, 162459700.0, 162459740.0, 162459780.0, 162459800.0, 162459870.0, 162459890.0, 162460050.0, 162460060.0, 162460270.0, 162460290.0, 162460300.0, 162460430.0, 162460460.0, 162460820.0, 162460860.0, 162461010.0, 162462020.0, 162462050.0, 162462220.0, 162462340.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7784090909090909\n",
      "Hamming Loss: 0.11931818181818182\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       578\n",
      "           1       0.00      0.00      0.00       126\n",
      "\n",
      "    accuracy                           0.82       704\n",
      "   macro avg       0.41      0.50      0.45       704\n",
      "weighted avg       0.67      0.82      0.74       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 280us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.3065 - binary_accuracy: 0.3967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-111874540.0, -111874610.0, -111874700.0, -111875230.0, -111875300.0, -111875310.0, -111875360.0, -111875816.0, -111875900.0, -111875960.0, -111876030.0, -111876980.0, -111877250.0, -111877260.0, -111877480.0, -111877530.0, -111877690.0, -111878770.0, -111878790.0, -111878980.0, -111879100.0, -111879380.0, -111879680.0, -111882060.0, -111882140.0, -111882160.0, -111882510.0, -111882520.0, -111882560.0, -111882680.0, -111882770.0, -111882856.0, -111883050.0, -111883280.0, -111883950.0, -111883980.0, -111884010.0, -111884130.0, -111884190.0, -111884220.0, -111884240.0, -111884296.0, -111884500.0, -111884620.0, -111884630.0, -111884770.0, -111884990.0, -111885120.0, -111886220.0, -111888780.0, -111889230.0, -111889410.0, -111889500.0, -111889580.0, -111889700.0, -111889790.0, -111889840.0, -111890110.0, -111890180.0, -111890190.0, -111890220.0, -111890260.0, -111890290.0, -111890350.0, -111890376.0, -111890400.0, -111890570.0, -111890580.0, -111890610.0, -111890640.0, -111890696.0, -111890740.0, -111890760.0, -111890790.0, -111890800.0, -111890860.0, -111890870.0, -111890900.0, -111890930.0, -111891010.0, -111891100.0, -111891120.0, -111891200.0, -111891260.0, -111891330.0, -111891440.0, -111891460.0, -111891500.0, -111891550.0, -111891670.0, -111891810.0, -111891976.0, -111891990.0, -111892020.0, -111892050.0, -111892120.0, -111892160.0, -111892290.0, -111892300.0, -111892330.0, -111892370.0, -111892420.0, -111892650.0, -111892780.0, -111892860.0, -111893144.0, -111893220.0, -111893240.0, -111893280.0, -111893440.0, -111893590.0, -111893720.0, -111893730.0, -111893740.0, -111893800.0, -111893860.0, -111893900.0, -111894300.0, -111894460.0, -111894610.0, -111894660.0, -111895320.0, -111897860.0, -111897950.0, -111898216.0, -111898340.0, -111898720.0, -111898850.0, -111898960.0, -111899010.0, -111899550.0, -111899870.0, -111899890.0, -111899920.0, -111899980.0, -111900070.0, -111900300.0, -111900610.0, -111900640.0, -111901090.0, -111901170.0, -111901310.0, -111901510.0, -111901590.0, -111901624.0, -111901630.0, -111901650.0, -111901790.0, -111901840.0, -111901870.0, -111901960.0, -111901980.0, -111902020.0, -111902024.0, -111902050.0, -111902100.0, -111902190.0, -111902240.0, -111902260.0, -111902340.0, -111902400.0, -111902460.0, -111902560.0, -111902584.0, -111902770.0, -111902910.0, -111903180.0, -111903230.0, -111903400.0, -111903650.0, -111903660.0, -111903730.0, -111903870.0, -111904056.0, -111904080.0, -111904140.0, -111904160.0, -111904180.0, -111904610.0, -111904750.0, -111904870.0, -111905090.0, -111905170.0, -111908350.0, -111908430.0, -111908744.0, -111908930.0, -111909210.0, -111909620.0, -111909670.0, -111909736.0, -111909770.0, -111909820.0, -111909950.0, -111910010.0, -111910020.0, -111910024.0, -111910264.0, -111910296.0, -111910430.0, -111911140.0, -111911170.0, -111911384.0, -111911464.0, -111911600.0, -111911820.0, -111911870.0, -111911900.0, -111911960.0, -111911980.0, -111912000.0, -111912020.0, -111912056.0, -111912060.0, -111912140.0, -111912350.0, -111912420.0, -111912430.0, -111912580.0, -111912640.0, -111912770.0, -111912904.0, -111912910.0, -111912930.0, -111912940.0, -111912990.0, -111913020.0, -111913120.0, -111913200.0, -111913280.0, -111913300.0, -111913330.0, -111913440.0, -111913704.0, -111913810.0, -111913920.0, -111918750.0, -111919320.0, -111919630.0, -111919660.0, -111919760.0, -111919816.0, -111919980.0, -111920080.0, -111920200.0, -111920260.0, -111920540.0, -111920730.0, -111920830.0, -111920960.0, -111920970.0, -111920990.0, -111921050.0, -111921170.0, -111921230.0, -111921360.0, -111921500.0, -111921680.0, -111921760.0, -111921870.0, -111922260.0, -111922500.0, -111922580.0, -111922590.0, -111922630.0, -111922730.0, -111922790.0, -111922880.0, -111922960.0, -111922970.0, -111923050.0, -111923100.0, -111923250.0, -111923550.0, -111923704.0, -111923770.0, -111923960.0, -111924110.0, -111924140.0, -111924150.0, -111924200.0, -111924640.0, -111927900.0, -111928080.0, -111928100.0, -111928330.0, -111928670.0, -111929140.0, -111929144.0, -111929310.0, -111929336.0, -111929410.0, -111929570.0, -111929600.0, -111929660.0, -111929750.0, -111929810.0, -111930260.0, -111930296.0, -111930350.0, -111930360.0, -111930536.0, -111930690.0, -111930960.0, -111931000.0, -111931020.0, -111931120.0, -111931140.0, -111931150.0, -111931190.0, -111931230.0, -111931520.0, -111931890.0, -111931940.0, -111932000.0, -111932296.0, -111932350.0, -111932430.0, -111932450.0, -111932456.0, -111933020.0, -111933060.0, -111933380.0, -111933420.0, -111933440.0, -111933550.0, -111933660.0, -111933950.0, -111938620.0, -111938660.0, -111938770.0, -111939340.0, -111940660.0, -111940850.0, -111942010.0, -111942420.0, -111942880.0, -111943420.0, -111943440.0, -111943490.0, -111943520.0, -111943830.0, -111943920.0, -111944050.0, -111944060.0, -111944250.0, -111944260.0, -111944350.0, -111944376.0, -111944400.0, -111944650.0, -111944660.0, -111944790.0, -111944810.0, -111944830.0, -111945070.0, -111945470.0, -111945520.0, -111945864.0, -111946190.0, -111946880.0, -111951700.0, -111952480.0, -111952500.0, -111953380.0, -111954060.0, -111954340.0, -111954350.0, -111954500.0, -111954560.0, -111954630.0, -111955060.0, -111955064.0, -111955300.0, -111955520.0, -111955650.0, -111955970.0, -111956000.0, -111956160.0, -111956220.0, -111956380.0, -111956560.0, -111956720.0, -111956936.0, -111956990.0, -111957016.0, -111957070.0, -111957096.0, -111957150.0, -111957200.0, -111957224.0, -111957230.0, -111957260.0, -111957330.0, -111957340.0, -111957390.0, -111957420.0, -111957610.0, -111957680.0, -111958050.0, -111958056.0, -111958320.0, -111958620.0, -111958830.0, -111959140.0, -111959600.0, -111966240.0, -111966740.0, -111966830.0, -111967120.0, -111967470.0, -111967480.0, -111967710.0, -111967896.0, -111968480.0, -111968590.0, -111968980.0, -111969420.0, -111969440.0, -111969460.0, -111969600.0, -111969680.0, -111969736.0, -111970240.0, -111970440.0, -111971130.0, -111971336.0, -111971470.0, -111971540.0, -111971670.0, -111971680.0, -111971770.0, -111971870.0, -111972000.0, -111972130.0, -111972170.0, -111972190.0, -111972270.0, -111972340.0, -111972440.0, -111972480.0, -111972590.0, -111972610.0, -111972680.0, -111972700.0, -111972780.0, -111973130.0, -111973384.0, -111974120.0, -111974160.0, -111974300.0, -111974760.0, -111974820.0, -111975010.0, -111975100.0, -111975120.0, -111975160.0, -111975220.0, -111975256.0, -111975630.0, -111979440.0, -111979570.0, -111979620.0, -111979730.0, -111980020.0, -111980370.0, -111980530.0, -111980610.0, -111980710.0, -111980850.0, -111980940.0, -111980960.0, -111981020.0, -111981040.0, -111981250.0, -111981480.0, -111981610.0, -111981690.0, -111981710.0, -111982000.0, -111982140.0, -111982270.0, -111982500.0, -111982870.0, -111982980.0, -111983300.0, -111983460.0, -111983704.0, -111983760.0, -111983920.0, -111984110.0, -111984260.0, -111984410.0, -111984550.0, -111984580.0, -111984590.0, -111984770.0, -111984780.0, -111984850.0, -111984870.0, -111984900.0, -111985230.0, -111985520.0, -111985540.0, -111985640.0, -111985700.0, -111985790.0, -111985880.0, -111985896.0, -111985900.0, -111985950.0, -111986024.0, -111986056.0, -111986180.0, -111986220.0, -111986240.0, -111986370.0, -111986400.0, -111986410.0, -111986490.0, -111986560.0, -111986770.0, -111986800.0, -111986820.0, -111986860.0, -111986950.0, -111986980.0, -111987060.0, -111987070.0, -111987230.0, -111987260.0, -111988140.0, -111991120.0, -111991320.0, -111991680.0, -111991700.0, -111991870.0, -111992180.0, -111992480.0, -111992530.0, -111992600.0, -111992670.0, -111992740.0, -111992744.0, -111992750.0, -111992980.0, -111993540.0, -111993550.0, -111993760.0, -111994150.0, -111994660.0, -111994776.0, -111994990.0, -111995060.0, -111995250.0, -111995260.0, -111995480.0, -111995540.0, -111995590.0, -111996296.0, -111996340.0, -111996370.0, -111996400.0, -111996460.0, -111996520.0, -111996620.0, -111996664.0, -111996750.0, -111996980.0, -111997220.0, -111997224.0, -111997280.0, -111997330.0, -111997380.0, -111997390.0, -111997440.0, -111997500.0, -111997520.0, -111997550.0, -111997630.0, -111997640.0, -111997710.0, -111997860.0, -111997950.0, -111998140.0, -111998400.0, -111998580.0, -111998860.0, -111998880.0, -111998890.0, -112002500.0, -112002536.0, -112002580.0, -112002960.0, -112003250.0, -112003530.0, -112003840.0, -112003870.0, -112003920.0, -112004100.0, -112004260.0, -112004370.0, -112004970.0, -112005170.0, -112005350.0, -112005540.0, -112005620.0, -112005860.0, -112006050.0, -112006056.0, -112006270.0, -112006290.0, -112006330.0, -112006450.0, -112006840.0, -112006860.0, -112006990.0, -112007020.0, -112007150.0, -112007256.0, -112007330.0, -112007340.0, -112007360.0, -112007544.0, -112007816.0, -112007850.0, -112007970.0, -112008010.0, -112008150.0, -112008210.0, -112008240.0, -112008260.0, -112008264.0, -112008330.0, -112008340.0, -112008350.0, -112008400.0, -112008420.0, -112008450.0, -112008480.0, -112008580.0, -112008616.0, -112008690.0, -112008696.0, -112008720.0, -112008730.0, -112008800.0, -112008860.0, -112008880.0, -112008936.0, -112009030.0, -112009040.0, -112009120.0, -112009380.0, -112009450.0, -112009600.0, -112009640.0, -112009780.0, -112009810.0, -112009816.0, -112009820.0, -112009860.0, -112009900.0, -112009950.0, -112010160.0, -112010216.0, -112010480.0, -112010536.0, -50571470.0, -50571580.0, -50572732.0, -50572890.0, -50573470.0, -50573548.0, -50573636.0, -50573684.0, -50574276.0, -50574296.0, -50574870.0, -50575630.0, -50576428.0, -50576490.0, -50577140.0, -50577988.0, -50578132.0, -50578664.0, -50578670.0, -50578840.0, -50579350.0, -50579390.0, -50579430.0, -50579990.0, -50580184.0, -50580660.0, -50580764.0, -50580812.0, -50581456.0, -50582250.0, -50582530.0, -50582860.0, -50582924.0, -50582930.0, -50582972.0, -50583190.0, -50583580.0, -50583732.0, -50583790.0, -50583810.0, -50583870.0, -50583890.0, -50583930.0, -50584340.0, -50584388.0, -50584492.0, -50585004.0, -50585036.0, -50585136.0, -50585180.0, -50585196.0, -50585264.0, -50585860.0, -50585870.0, -50585884.0, -50586532.0, -50586560.0, -50586624.0, -50587190.0, -50587416.0, -50587490.0, -50587548.0, -50587590.0, -50588784.0, -50589440.0, -50589492.0, -50589564.0, -50589732.0, -50589760.0, -50590130.0, -50590376.0, -50590420.0, -50590856.0, -50590944.0, -50591570.0, -50591744.0, -50592320.0, -50592336.0, -50592580.0, -50593756.0, -50645304.0, -50645590.0, -50646150.0, -50646264.0, -50646910.0, -50646988.0, -50647524.0, -50647564.0, -50648270.0, -50649616.0, -50649680.0, -50650496.0, -50650530.0, -50651370.0, -50651948.0, -50651970.0, -50652060.0, -50652100.0, -50652890.0, -50653252.0, -50653260.0, -50654156.0, -50654772.0, -50654840.0, -50654868.0, -50654960.0, -50655452.0, -50655588.0, -50655612.0, -50655652.0, -50656260.0, -50656304.0, -50656412.0, -50656440.0, -50656550.0, -50656860.0, -50657572.0, -50657610.0, -50657652.0, -50657716.0, -50657908.0, -50657960.0, -50658064.0, -50658380.0, -50659084.0, -50659260.0, -50659320.0, -50659390.0, -50659890.0, -50659990.0, -50660084.0, -50660110.0, -50660548.0, -50660624.0, -50660640.0, -50660650.0, -50660652.0, -50660836.0, -50661356.0, -50661612.0, -50662696.0, -50664224.0, -50715744.0, -50715980.0, -50716716.0, -50717172.0, -50717230.0, -50717456.0, -50718630.0, -50718940.0, -50720012.0, -50720210.0, -50720852.0, -50720930.0, -50721124.0, -50721628.0, -50721692.0, -50721868.0, -50722292.0, -50722308.0, -50722330.0, -50722396.0, -50722412.0, -50722440.0, -50722584.0, -50723040.0, -50723756.0, -50724400.0, -50725172.0, -50725256.0, -50725264.0, -50725308.0, -50725332.0, -50725456.0, -50725948.0, -50726030.0, -50726736.0, -50726804.0, -50726904.0, -50727410.0, -50727520.0, -50727580.0, -50728056.0, -50728164.0, -50728350.0, -50728756.0, -50728788.0, -50728800.0, -50728880.0, -50729044.0, -50729092.0, -50729544.0, -50729572.0, -50729620.0, -50729650.0, -50729850.0, -50729852.0, -50730360.0, -50730384.0, -50730412.0, -50730436.0, -50730984.0, -50731350.0, -50731664.0, -50732430.0, -50732668.0, -50733064.0, -50733670.0, -50733950.0, -50734620.0, -50735384.0, -50735624.0, -50735724.0, -50736748.0, -50736860.0, -50737520.0, -50737720.0, -50738936.0, -50738948.0, -50786360.0, -50786388.0, -50787116.0, -50787640.0, -50787656.0, -50787720.0, -50787748.0, -50788052.0, -50788416.0, -50788464.0, -50789270.0, -50789500.0, -50789520.0, -50789650.0, -50789816.0, -50790224.0, -50790880.0, -50790904.0, -50791236.0, -50791284.0, -50793164.0, -50793170.0, -50793540.0, -50795628.0, -50796236.0, -50797224.0, -50797244.0, -50797604.0, -50799040.0, -50799060.0, -50799070.0, -50799104.0, -50799110.0, -50799148.0, -50799150.0, -50799490.0, -50799524.0, -50799532.0, -50799650.0, -50799780.0, -50800296.0, -50800412.0, -50800770.0, -50800980.0, -50801016.0, -50802212.0, -50802276.0, -50802292.0, -50802468.0, -50802610.0, -50802696.0, -50802964.0, -50803964.0, -50851490.0, -50852110.0, -50852140.0, -50852744.0, -50852784.0, -50853724.0, -50853770.0, -50855116.0, -50855444.0, -50856290.0, -50856510.0, -50857516.0, -50858730.0, -50858750.0, -50859068.0, -50859250.0, -50859280.0, -50859750.0, -50859972.0, -50859980.0, -50859984.0, -50860564.0, -50861080.0, -50861170.0, -50861196.0, -50861230.0, -50861256.0, -50861308.0, -50861930.0, -50861980.0, -50862320.0, -50862376.0, -50862496.0, -50862572.0, -50863144.0, -50863184.0, -50865550.0, -50865570.0, -50865636.0, -50865684.0, -50865732.0, -50865736.0, -50866480.0, -50866736.0, -50866936.0, -50868260.0, -50911856.0, -50912596.0, -50913240.0, -50913290.0, -50913356.0, -50913828.0, -50913856.0, -50913864.0, -50914336.0, -50914484.0, -50914508.0, -50914516.0, -50915244.0, -50915600.0, -50916416.0, -50916450.0, -50916504.0, -50916810.0, -50917120.0, -50917510.0, -50917580.0, -50917616.0, -50917700.0, -50918236.0, -50919400.0, -50920236.0, -50920984.0, -50922588.0, -50922656.0, -50923190.0, -50925840.0, -50928916.0, -50930390.0, -50930430.0, -50930600.0, -50930852.0, -50931050.0, -50971050.0, -50973880.0, -50973948.0, -50973960.0, -50975050.0, -50975104.0, -50975170.0, -50975200.0, -50975596.0, -50976136.0, -50976744.0, -50977330.0, -50978440.0, -50978660.0, -50978780.0, -50979330.0, -50979770.0, -50979790.0, -50980440.0, -50980504.0, -50980516.0, -50981056.0, -50981156.0, -50981390.0, -50981504.0, -50981510.0, -50981690.0, -50982010.0, -50982110.0, -50982204.0, -50982284.0, -50982332.0, -50982350.0, -50982628.0, -50982644.0, -50982684.0, -50982820.0, -50983224.0, -50983480.0, -50983880.0, -50984396.0, -50984920.0, -50985560.0, -50986960.0, -50987436.0, -50987532.0, -50988224.0, -50989224.0, -50990480.0, -51033068.0, -51035124.0, -51035664.0, -51035748.0, -51036640.0, -51036744.0, -51037148.0, -51037820.0, -51038040.0, -51038640.0, -51038950.0, -51039216.0, -51039830.0, -51039876.0, -51040170.0, -51040670.0, -51040720.0, -51040730.0, -51040744.0, -51040810.0, -51041320.0, -51042148.0, -51042160.0, -51042240.0, -51043184.0, -51043380.0, -51044504.0, -51044788.0, -51044864.0, -51044876.0, -51044880.0, -51045036.0, -51045084.0, -51045104.0, -51045124.0, -51045170.0, -51045344.0, -51045356.0, -51046250.0, -51047156.0, -51048280.0, -51048350.0, -51048370.0, -51049040.0, -51092360.0, -51092390.0, -51093476.0, -51093532.0, -51094040.0, -51094056.0, -51094104.0, -51094110.0, -51094120.0, -51094132.0, -51094696.0, -51095240.0, -51095510.0, -51095916.0, -51096588.0, -51096750.0, -51097000.0, -51097140.0, -51097616.0, -51097780.0, -51097850.0, -51097900.0, -51098204.0, -51098468.0, -51098780.0, -51098980.0, -51099096.0, -51099110.0, -51100244.0, -51100270.0, -51101304.0, -51101468.0, -51102036.0, -51102428.0, -51102564.0, -51102580.0, -51102588.0, -51102604.0, -51102644.0, -51103124.0, -51103204.0, -51103524.0, -51104240.0, -51104270.0, -51104376.0, -51104420.0, -51104890.0, -51105468.0, -51105564.0, -51105864.0, -51105904.0, -51105910.0, -51106412.0, -51106468.0, -51107600.0, -51148350.0, -51148370.0, -51149580.0, -51150444.0, -51150490.0, -51150532.0, -51151004.0, -51151176.0, -51151348.0, -51151636.0, -51151644.0, -51152164.0, -51153080.0, -51153084.0, -51153340.0, -51153376.0, -51153450.0, -51153510.0, -51153604.0, -51153620.0, -51153988.0, -51154250.0, -51154530.0, -51154570.0, -51154630.0, -51154652.0, -51154790.0, -51155064.0, -51155136.0, -51155180.0, -51155400.0, -51155444.0, -51155708.0, -51155732.0, -51156492.0, -51156550.0, -51157064.0, -51158300.0, -51158310.0, -51158404.0, -51158410.0, -51158628.0, -51158676.0, -51159870.0, -51160584.0, -51162244.0, -51162336.0, -51163004.0, -51163080.0, -51163384.0, -51163440.0, -51163904.0, -51164224.0, -51164756.0, -51164820.0, -51165348.0, -51165904.0, -51165960.0, -51166320.0, -51166376.0, -51167052.0, -51167136.0, -51209690.0, -51209972.0, -51209980.0, -51210316.0, -51210320.0, -51210504.0, -51210572.0, -51210584.0, -51210876.0, -51211092.0, -51211144.0, -51211184.0, -51211530.0, -51212120.0, -51212652.0, -51212696.0, -51212948.0, -51213240.0, -51213830.0, -51214044.0, -51214064.0, -51214092.0, -51214100.0, -51214376.0, -51214676.0, -51215228.0, -51215580.0, -51215668.0, -51216150.0, -51216350.0, -51216450.0, -51216810.0, -51216856.0, -51216964.0, -51216970.0, -51217120.0, -51217350.0, -51217396.0, -51217400.0, -51217456.0, -51217620.0, -51217624.0, -51217930.0, -51218044.0, -51218224.0, -51218264.0, -51218468.0, -51218508.0, -51218536.0, -51219080.0, -51219304.0, -51219730.0, -51220210.0, -51220240.0, -51220264.0, -51220304.0, -51220390.0, -51220424.0, -51220452.0, -51220532.0, -51221120.0, -51221216.0, -51221356.0, -51221456.0, -51221468.0, -51221470.0, -51221532.0, -51221544.0, -51221740.0, -51222004.0, -51222040.0, -51222084.0, -51222340.0, -51222556.0, -51222630.0, -51223250.0, -51224084.0, -51224100.0, -51224410.0, -51224980.0, -51224996.0, -51265800.0, -51266772.0, -51267164.0, -51267510.0, -51267860.0, -51267868.0, -51268036.0, -51268050.0, -51268388.0, -51268540.0, -51268652.0, -51269024.0, -51269156.0, -51269228.0, -51269230.0, -51269316.0, -51269510.0, -51269564.0, -51269604.0, -51269704.0, -51269824.0, -51270950.0, -51271060.0, -51271068.0, -51271550.0, -51271604.0, -51271616.0, -51272184.0, -51272196.0, -51272760.0, -51272828.0, -51276252.0, -51276844.0, -51277456.0, -51278052.0, -51278140.0, -51278536.0, -51279800.0, -51280370.0, -51282060.0, -51282130.0, -51282136.0, -51282204.0, -51282776.0, -51284556.0, -51284990.0, -51285016.0, -51285100.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7997159090909091\n",
      "Hamming Loss: 0.10014204545454546\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       563\n",
      "           1       0.20      1.00      0.33       141\n",
      "\n",
      "    accuracy                           0.20       704\n",
      "   macro avg       0.10      0.50      0.17       704\n",
      "weighted avg       0.04      0.20      0.07       704\n",
      "\n",
      "Train on 2816 samples\n",
      "Epoch 1/100\n",
      "2816/2816 [==============================] - 1s 332us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 2/100\n",
      "2816/2816 [==============================] - 0s 85us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 3/100\n",
      "2816/2816 [==============================] - 0s 88us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 4/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 5/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 6/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 7/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 8/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 9/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 10/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 11/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 12/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 13/100\n",
      "2816/2816 [==============================] - 0s 112us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 14/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 15/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 16/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 17/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 18/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 19/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 20/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 21/100\n",
      "2816/2816 [==============================] - 0s 98us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 22/100\n",
      "2816/2816 [==============================] - 0s 97us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 23/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 24/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 25/100\n",
      "2816/2816 [==============================] - 0s 95us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 26/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 27/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 28/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 29/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 30/100\n",
      "2816/2816 [==============================] - 0s 99us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 31/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 32/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 33/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 34/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 35/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 36/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 37/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 38/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 39/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 40/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 41/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 42/100\n",
      "2816/2816 [==============================] - 0s 94us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 43/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 44/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 45/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 46/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 47/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 48/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 49/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 50/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 51/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 52/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 53/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 54/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 55/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 56/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 57/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 58/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 59/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 60/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 61/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 62/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 63/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 64/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 65/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 66/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 67/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 69/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 70/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 71/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 72/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 73/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 74/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 75/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 76/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 77/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 78/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 79/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 80/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 81/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 82/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 83/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 84/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 85/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 86/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 87/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 88/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 89/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 90/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 91/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 92/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 93/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 94/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 95/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 96/100\n",
      "2816/2816 [==============================] - 0s 90us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 97/100\n",
      "2816/2816 [==============================] - 0s 92us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 98/100\n",
      "2816/2816 [==============================] - 0s 93us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 99/100\n",
      "2816/2816 [==============================] - 0s 91us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Epoch 100/100\n",
      "2816/2816 [==============================] - 0s 89us/sample - loss: 9.2489 - binary_accuracy: 0.4004\n",
      "Accuracy: 0.7599431818181818\n",
      "Hamming Loss: 0.125\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       575\n",
      "           1       0.00      0.00      0.00       129\n",
      "\n",
      "    accuracy                           0.82       704\n",
      "   macro avg       0.41      0.50      0.45       704\n",
      "weighted avg       0.67      0.82      0.73       704\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-139960860.0, -139960960.0, -139961680.0, -139961730.0, -139964000.0, -139964030.0, -139964060.0, -139964960.0, -139965060.0, -139965070.0, -139965400.0, -139965500.0, -139965540.0, -139966060.0, -139966240.0, -139966270.0, -139966320.0, -139966850.0, -139967230.0, -139967330.0, -139967490.0, -139967520.0, -139967620.0, -139967650.0, -139967900.0, -139967950.0, -139968080.0, -139968110.0, -139968510.0, -139968530.0, -139968540.0, -139968590.0, -139968640.0, -139968670.0, -139968900.0, -139969360.0, -139969650.0, -139969700.0, -139969920.0, -139970180.0, -139970600.0, -139970740.0, -139970980.0, -139971490.0, -139987800.0, -139988620.0, -139988690.0, -139988770.0, -139988780.0, -139989020.0, -139989060.0, -139989170.0, -139989310.0, -139989330.0, -139989340.0, -139989400.0, -139989540.0, -139989600.0, -139989660.0, -139989870.0, -139989920.0, -139989950.0, -139990080.0, -139990130.0, -139990140.0, -139990180.0, -139990200.0, -139990340.0, -139990380.0, -139990580.0, -139990640.0, -139990660.0, -139990670.0, -139990770.0, -139991180.0, -139991260.0, -139991420.0, -139991650.0, -139991710.0, -139991740.0, -139991950.0, -139991970.0, -139992450.0, -139992480.0, -139992510.0, -139992900.0, -139992930.0, -139993060.0, -139993170.0, -139993220.0, -139993420.0, -139993520.0, -139994000.0, -139994300.0, -139994430.0, -139994500.0, -139994540.0, -139994560.0, -139994600.0, -139994980.0, -139995100.0, -139995400.0, -139995800.0, -139995840.0, -139996290.0, -140012670.0, -140012720.0, -140012960.0, -140013000.0, -140013180.0, -140013300.0, -140013470.0, -140013540.0, -140013810.0, -140013820.0, -140013870.0, -140014020.0, -140014240.0, -140014300.0, -140014750.0, -140014780.0, -140014820.0, -140014940.0, -140014960.0, -140014980.0, -140015000.0, -140015260.0, -140015340.0, -140015600.0, -140016100.0, -140016110.0, -140016160.0, -140016200.0, -140016580.0, -140016600.0, -140016640.0, -140016900.0, -140017020.0, -140017060.0, -140017070.0, -140017180.0, -140017390.0, -140017400.0, -140017520.0, -140017540.0, -140017730.0, -140017820.0, -140017870.0, -140017940.0, -140018080.0, -140018100.0, -140018140.0, -140018260.0, -140018450.0, -140018460.0, -140018600.0, -140018770.0, -140018850.0, -140019180.0, -140020080.0, -140020220.0, -140021940.0, -140038940.0, -140039360.0, -140039660.0, -140039680.0, -140039700.0, -140039840.0, -140039900.0, -140039940.0, -140040160.0, -140040200.0, -140040220.0, -140040460.0, -140040930.0, -140041000.0, -140041180.0, -140041220.0, -140041260.0, -140041380.0, -140041460.0, -140041470.0, -140041540.0, -140041730.0, -140041760.0, -140041970.0, -140042240.0, -140042290.0, -140042580.0, -140042620.0, -140042990.0, -140043100.0, -140043120.0, -140043170.0, -140043310.0, -140043540.0, -140044110.0, -140044580.0, -140044590.0, -140044900.0, -140044910.0, -140045000.0, -140045140.0, -140045500.0, -140045570.0, -140045860.0, -140046050.0, -140046370.0, -140046420.0, -140046560.0, -140046850.0, -140048200.0, -140065150.0, -140065280.0, -140065300.0, -140065310.0, -140065400.0, -140065760.0, -140066240.0, -140066300.0, -140066320.0, -140066450.0, -140066720.0, -140066930.0, -140067680.0, -140067710.0, -140067870.0, -140067890.0, -140067920.0, -140067950.0, -140067970.0, -140068110.0, -140068140.0, -140068340.0, -140068380.0, -140068400.0, -140068460.0, -140068600.0, -140068690.0, -140068830.0, -140069000.0, -140069760.0, -140069920.0, -140070020.0, -140070320.0, -140070690.0, -140070940.0, -140071550.0, -140071840.0, -140071860.0, -140072450.0, -140091040.0, -140091150.0, -140091260.0, -140091360.0, -140091410.0, -140091440.0, -140091500.0, -140091600.0, -140092320.0, -140092510.0, -140093000.0, -140093010.0, -140093020.0, -140093090.0, -140093100.0, -140093230.0, -140093330.0, -140093340.0, -140093360.0, -140093550.0, -140093650.0, -140093660.0, -140093920.0, -140094110.0, -140094190.0, -140094340.0, -140094620.0, -140094640.0, -140094820.0, -140094850.0, -140094980.0, -140095070.0, -140095340.0, -140095360.0, -140095400.0, -140095410.0, -140095470.0, -140095650.0, -140095920.0, -140095940.0, -140096320.0, -140096450.0, -140096480.0, -140096510.0, -140096700.0, -140096720.0, -140096830.0, -140096930.0, -140097000.0, -140097680.0, -140116180.0, -140116400.0, -140116460.0, -140116600.0, -140117140.0, -140117500.0, -140117540.0, -140117620.0, -140117660.0, -140117900.0, -140117940.0, -140118140.0, -140118240.0, -140118270.0, -140118500.0, -140118600.0, -140118720.0, -140118750.0, -140118780.0, -140119000.0, -140119020.0, -140119180.0, -140119230.0, -140119250.0, -140119410.0, -140119740.0, -140119780.0, -140119800.0, -140119840.0, -140120020.0, -140120060.0, -140120080.0, -140120180.0, -140120240.0, -140120300.0, -140120460.0, -140120510.0, -140120540.0, -140120750.0, -140120780.0, -140121060.0, -140121090.0, -140121120.0, -140121550.0, -140121630.0, -140121760.0, -140122080.0, -140122110.0, -140122260.0, -140122880.0, -140123360.0, -140123630.0, -140123660.0, -140124200.0, -140124930.0, -140125000.0, -140125170.0, -140141340.0, -140141520.0, -140141580.0, -140141760.0, -140141970.0, -140142020.0, -140142350.0, -140142510.0, -140142560.0, -140142770.0, -140142880.0, -140143380.0, -140143400.0, -140143580.0, -140143620.0, -140143710.0, -140143760.0, -140144260.0, -140144340.0, -140144350.0, -140144420.0, -140144700.0, -140144860.0, -140144900.0, -140145390.0, -140145520.0, -140145540.0, -140145980.0, -140146340.0, -140146540.0, -140147120.0, -140147400.0, -140147460.0, -140147740.0, -140147950.0, -140148560.0, -140148580.0, -140148600.0, -140148940.0, -140149090.0, -140149230.0, -140150260.0, -140166400.0, -140167140.0, -140167490.0, -140167660.0, -140168000.0, -140168210.0, -140168240.0, -140168420.0, -140168450.0, -140168670.0, -140168720.0, -140168740.0, -140168910.0, -140169060.0, -140169100.0, -140169280.0, -140169440.0, -140169470.0, -140169600.0, -140169780.0, -140169860.0, -140169870.0, -140169900.0, -140169980.0, -140170200.0, -140170300.0, -140170400.0, -140170460.0, -140170540.0, -140170960.0, -140170980.0, -140171250.0, -140171580.0, -140171660.0, -140171740.0, -140171780.0, -140171790.0, -140171860.0, -140171890.0, -140171920.0, -140172000.0, -140172050.0, -140172160.0, -140172220.0, -140172260.0, -140172880.0, -140173360.0, -140173400.0, -140173420.0, -140173580.0, -140173650.0, -140173860.0, -140174110.0, -140174430.0, -140192510.0, -140192540.0, -140193150.0, -140193180.0, -140193220.0, -140193250.0, -140193280.0, -140193360.0, -140193390.0, -140193420.0, -140193540.0, -140193620.0, -140193730.0, -140193760.0, -140193870.0, -140193890.0, -140194160.0, -140194300.0, -140194420.0, -140194640.0, -140194690.0, -140194830.0, -140194960.0, -140195040.0, -140195200.0, -140195220.0, -140195300.0, -140195420.0, -140195820.0, -140195940.0, -140196020.0, -140196060.0, -140196160.0, -140196450.0, -140196500.0, -140196530.0, -140196540.0, -140196580.0, -140196620.0, -140196670.0, -140196700.0, -140196740.0, -140196860.0, -140196880.0, -140197180.0, -140197220.0, -140198030.0, -140198240.0, -140198290.0, -140198380.0, -140198430.0, -140198500.0, -140198670.0, -140198770.0, -140198850.0, -140198930.0, -140199150.0, -140199170.0, -140199650.0, -140199680.0, -140200100.0, -140200500.0, -140200560.0, -140200590.0, -140201920.0, -140202060.0, -140202160.0, -140202240.0, -140202450.0, -140217180.0, -140217440.0, -140217470.0, -140217980.0, -140218220.0, -140219020.0, -140219300.0, -140219340.0, -140219470.0, -140219540.0, -140219550.0, -140219570.0, -140219580.0, -140219650.0, -140219660.0, -140219740.0, -140219840.0, -140219870.0, -140220160.0, -140220210.0, -140220220.0, -140220240.0, -140220260.0, -140220290.0, -140220320.0, -140220340.0, -140220350.0, -140220580.0, -140220830.0, -140221000.0, -140221100.0, -140221150.0, -140221260.0, -140221300.0, -140221310.0, -140221330.0, -140221620.0, -140221730.0, -140221800.0, -140221820.0, -140221890.0, -140221900.0, -140222140.0, -140222190.0, -140222200.0, -140222320.0, -140222400.0, -140222690.0, -140222860.0, -140222900.0, -140222940.0, -140223140.0, -140223490.0, -140223580.0, -140223650.0, -140223730.0, -140223820.0, -140223920.0, -140223940.0, -140224200.0, -140224940.0, -140225020.0, -140225070.0, -140225180.0, -140225220.0, -140225280.0, -140243020.0, -140244140.0, -140244180.0, -140244210.0, -140244240.0, -140244340.0, -140244480.0, -140244740.0, -140244750.0, -140244900.0, -140245260.0, -140245280.0, -140245400.0, -140245440.0, -140245500.0, -140245650.0, -140245890.0, -140245920.0, -140246060.0, -140246180.0, -140246200.0, -140246420.0, -140246600.0, -140246800.0, -140246820.0, -140246940.0, -140246960.0, -140246980.0, -140247020.0, -140247120.0, -140247230.0, -140247280.0, -140247420.0, -140247490.0, -140248030.0, -140248050.0, -140248110.0, -140248510.0, -140248860.0, -140248910.0, -140249520.0, -140249680.0, -140249700.0, -140250690.0, -140251090.0, -140251490.0, -140251620.0, -2580040.0, -2580560.0, -2580696.0, -2580712.0, -2580728.0, -2580736.0, -2580752.0, -2580800.0, -2580832.0, -2580848.0, -2580864.0, -2580968.0, -2581024.0, -2581096.0, -2581104.0, -2581280.0, -2581344.0, -2581360.0, -2581456.0, -2581512.0, -2581544.0, -2581552.0, -2581560.0, -2581568.0, -2581632.0, -2581696.0, -2581808.0, -2581872.0, -2581888.0, -2581904.0, -2581952.0, -2581968.0, -2581984.0, -2582000.0, -2582008.0, -2582016.0, -2582032.0, -2582072.0, -2582272.0, -2582352.0, -2582400.0, -2582424.0, -2582608.0, -2582784.0, -2582816.0, -2582832.0, -2583744.0, -2583776.0, -2584048.0, -2584176.0, -2584264.0, -2587104.0, -2587376.0, -2587456.0, -2587536.0, -2587720.0, -2587808.0, -2587856.0, -2587872.0, -2588016.0, -2588080.0, -2588096.0, -2588112.0, -2588136.0, -2588144.0, -2588160.0, -2588176.0, -2588184.0, -2588224.0, -2588240.0, -2588248.0, -2588320.0, -2588336.0, -2588344.0, -2588368.0, -2588392.0, -2588464.0, -2588488.0, -2588496.0, -2588608.0, -2588672.0, -2588688.0, -2588704.0, -2588720.0, -2588792.0, -2588800.0, -2588840.0, -2588848.0, -2588864.0, -2588904.0, -2588960.0, -2589008.0, -2589032.0, -2589040.0, -2589280.0, -2589344.0, -2589536.0, -2589688.0, -2589728.0, -2589768.0, -2590376.0, -2590464.0, -2590480.0, -2590496.0, -2590512.0, -2590672.0, -2590704.0, -2590776.0, -2590976.0, -2591448.0, -2591464.0, -2591680.0, -2594000.0, -2594096.0, -2594160.0, -2594208.0, -2594224.0, -2594272.0, -2594512.0, -2594648.0, -2594688.0, -2594728.0, -2594768.0, -2594856.0, -2594872.0, -2594912.0, -2594960.0, -2595000.0, -2595008.0, -2595032.0, -2595040.0, -2595200.0, -2595216.0, -2595256.0, -2595272.0, -2595384.0, -2595408.0, -2595440.0, -2595544.0, -2595576.0, -2595592.0, -2595608.0, -2595640.0, -2595816.0, -2595872.0, -2595936.0, -2595968.0, -2596032.0, -2596056.0, -2596080.0, -2596088.0, -2596096.0, -2596120.0, -2596128.0, -2596136.0, -2596144.0, -2596176.0, -2596192.0, -2596200.0, -2596256.0, -2596344.0, -2596392.0, -2596456.0, -2596480.0, -2596496.0, -2596512.0, -2596560.0, -2596568.0, -2596608.0, -2596616.0, -2596800.0, -2596848.0, -2596928.0, -2596936.0, -2597040.0, -2597064.0, -2597088.0, -2597216.0, -2597472.0, -2597648.0, -2598432.0, -2598544.0, -2601480.0, -2601552.0, -2601600.0, -2601672.0, -2601696.0, -2601824.0, -2602016.0, -2602032.0, -2602144.0, -2602192.0, -2602216.0, -2602256.0, -2602272.0, -2602328.0, -2602408.0, -2602432.0, -2602464.0, -2602536.0, -2602544.0, -2602560.0, -2602624.0, -2602648.0, -2602768.0, -2602800.0, -2602848.0, -2602896.0, -2602904.0, -2602936.0, -2602944.0, -2602992.0, -2603128.0, -2603144.0, -2603160.0, -2603216.0, -2603280.0, -2603288.0, -2603448.0, -2603544.0, -2603680.0, -2603696.0, -2603736.0, -2603776.0, -2603824.0, -2603888.0, -2603976.0, -2604016.0, -2604144.0, -2604896.0, -2604976.0, -2605888.0, -2608784.0, -2608896.0, -2609080.0, -2609216.0, -2609232.0, -2609272.0, -2609280.0, -2609296.0, -2609424.0, -2609520.0, -2609528.0, -2609568.0, -2609576.0, -2609584.0, -2609744.0, -2609792.0, -2609848.0, -2610000.0, -2610008.0, -2610056.0, -2610064.0, -2610112.0, -2610256.0, -2610312.0, -2610328.0, -2610336.0, -2610360.0, -2610456.0, -2610528.0, -2610576.0, -2610608.0, -2610624.0, -2610656.0, -2610880.0, -2611072.0, -2611088.0, -2611136.0, -2611296.0, -2611312.0, -2611376.0, -2611664.0, -2611688.0, -2612992.0, -2615992.0, -2616480.0, -2616576.0, -2616624.0, -2616704.0, -2616720.0, -2616736.0, -2616800.0, -2616808.0, -2616880.0, -2616968.0, -2616992.0, -2617008.0, -2617040.0, -2617048.0, -2617136.0, -2617160.0, -2617168.0, -2617184.0, -2617200.0, -2617248.0, -2617336.0, -2617376.0, -2617424.0, -2617488.0, -2617520.0, -2617528.0, -2617544.0, -2617744.0, -2617760.0, -2617800.0, -2617808.0, -2617912.0, -2617920.0, -2617968.0, -2618040.0, -2618048.0, -2618112.0, -2618176.0, -2618200.0, -2618216.0, -2618280.0, -2618336.0, -2618384.0, -2618448.0, -2618480.0, -2618528.0, -2618552.0, -2618856.0, -2618864.0, -2619136.0, -2619152.0, -2619192.0, -2619200.0, -2619232.0, -2619296.0, -2619328.0, -2619376.0, -2619408.0, -2619504.0, -2619768.0, -2623296.0, -2623472.0, -2623568.0, -2623704.0, -2623856.0, -2623936.0, -2623960.0, -2623984.0, -2624016.0, -2624032.0, -2624040.0, -2624048.0, -2624080.0, -2624120.0, -2624184.0, -2624320.0, -2624336.0, -2624512.0, -2624536.0, -2624560.0, -2624584.0, -2624592.0, -2624736.0, -2624744.0, -2624816.0, -2624880.0, -2624904.0, -2624960.0, -2625176.0, -2625368.0, -2625424.0, -2625440.0, -2625456.0, -2625520.0, -2625576.0, -2625840.0, -2625856.0, -2625888.0, -2625920.0, -2625928.0, -2626224.0, -2626256.0, -2626272.0, -2626384.0, -2626512.0, -2626576.0, -2626600.0, -2626704.0, -2626800.0, -2626944.0, -2626976.0, -2627008.0, -2629440.0, -2630656.0, -2630880.0, -2630888.0, -2631120.0, -2631152.0, -2631176.0, -2631184.0, -2631200.0, -2631296.0, -2631304.0, -2631408.0, -2631424.0, -2631504.0, -2631552.0, -2631728.0, -2631744.0, -2631760.0, -2631776.0, -2631792.0, -2631824.0, -2631872.0, -2631888.0, -2631968.0, -2631984.0, -2632320.0, -2632416.0, -2632456.0, -2632480.0, -2632624.0, -2632632.0, -2632784.0, -2632840.0, -2633120.0, -2633376.0, -2633616.0, -2633648.0, -2633824.0, -2633904.0, -2637936.0, -2638040.0, -2638136.0, -2638144.0, -2638256.0, -2638304.0, -2638416.0, -2638448.0, -2638464.0, -2638544.0, -2638576.0, -2638624.0, -2638672.0, -2638704.0, -2638776.0, -2638784.0, -2638800.0, -2638872.0, -2638880.0, -2638888.0, -2638896.0, -2638912.0, -2638920.0, -2638944.0, -2638960.0, -2638968.0, -2638992.0, -2639008.0, -2639072.0, -2639104.0, -2639152.0, -2639200.0, -2639216.0, -2639264.0, -2639280.0, -2639320.0, -2639328.0, -2639376.0, -2639400.0, -2639416.0, -2639600.0, -2639648.0, -2639832.0, -2639936.0, -2639968.0, -2640848.0, -2640976.0, -2640984.0, -2641136.0, -2645184.0, -2645576.0, -2645592.0, -2645624.0, -2645664.0, -2645744.0, -2645784.0, -2645800.0, -2645808.0, -2645856.0, -2645864.0, -2645904.0, -2645920.0, -2645936.0, -2645944.0, -2646040.0, -2646048.0, -2646056.0, -2646064.0, -2646096.0, -2646112.0, -2646192.0, -2646232.0, -2646240.0, -2646320.0, -2646368.0, -2646392.0, -2646400.0, -2646416.0, -2646424.0, -2646432.0, -2646448.0, -2646480.0, -2646488.0, -2646496.0, -2646560.0, -2646624.0, -2646656.0, -2646688.0, -2646752.0, -2646864.0, -2647072.0, -2647104.0, -2647120.0, -2647184.0, -2647648.0, -2647752.0, -2647856.0, -2647904.0, -2647912.0, -2648080.0, -2648096.0, -2648112.0, -2648176.0, -2648200.0, -2648256.0, -2648288.0, -2648592.0, -2648616.0, -2651872.0, -2651960.0, -2651984.0, -2652304.0, -2652328.0, -2652336.0, -2652392.0, -2652424.0, -2652432.0, -2652496.0, -2652616.0, -2652672.0, -2652680.0, -2652720.0, -2652736.0, -2652752.0, -2652776.0, -2652880.0, -2652928.0, -2652936.0, -2652976.0, -2653008.0, -2653016.0, -2653040.0, -2653128.0, -2653168.0, -2653176.0, -2653440.0, -2653456.0, -2653560.0, -2653576.0, -2653744.0, -2653768.0, -2653824.0, -2653936.0, -2653944.0, -2653952.0, -2653984.0, -2654096.0, -2654128.0, -2654144.0, -2654152.0, -2654176.0, -2654224.0, -2654296.0, -2654400.0, -2654432.0, -2654440.0, -2654448.0, -2654528.0, -2654560.0, -2654704.0, -2654776.0, -2654872.0, -2654920.0, -2655040.0, -2655072.0, -2655600.0, -2655624.0, -2655632.0, -2659376.0, -2659416.0, -2659488.0, -2659528.0, -2659568.0, -2659584.0, -2659592.0, -2659600.0, -2659624.0, -2659664.0, -2659696.0, -2659728.0, -2659744.0, -2659808.0, -2659824.0, -2659872.0, -2659912.0, -2659944.0, -2659968.0, -2659984.0, -2660032.0, -2660072.0, -2660080.0, -2660088.0, -2660144.0, -2660184.0, -2660232.0, -2660240.0, -2660304.0, -2660336.0, -2660400.0, -2660416.0, -2660440.0, -2660448.0, -2660472.0, -2660624.0, -2660696.0, -2660768.0, -2660824.0, -2660880.0, -2660896.0, -2661056.0, -2661152.0, -2661344.0, -2661568.0, -2661648.0, -2661696.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2)\n",
    "    ]) \n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "\n",
    "#cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "# create model\n",
    "#model = LogisticRegression()\n",
    "# evaluate model\n",
    "#scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "#print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22053, 33)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_KGFS25.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17642 samples\n",
      "Epoch 1/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 2/100\n",
      "17642/17642 [==============================] - 2s 90us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 3/100\n",
      "17642/17642 [==============================] - 2s 87us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 4/100\n",
      "17642/17642 [==============================] - 2s 87us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 5/100\n",
      "17642/17642 [==============================] - 2s 103us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 6/100\n",
      "17642/17642 [==============================] - 2s 92us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 7/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 8/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 9/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 10/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 11/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 12/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 13/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 14/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 15/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 16/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 17/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 18/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 19/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 20/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 21/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 22/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 23/100\n",
      "17642/17642 [==============================] - 2s 118us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 24/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 25/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 26/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 27/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 28/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 29/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 30/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 31/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 32/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 33/100\n",
      "17642/17642 [==============================] - 2s 118us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 34/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 35/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 36/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 37/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 38/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 39/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 40/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 41/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 42/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 43/100\n",
      "17642/17642 [==============================] - 2s 127us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 44/100\n",
      "17642/17642 [==============================] - 3s 161us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 45/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 46/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 47/100\n",
      "17642/17642 [==============================] - 2s 129us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 48/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 49/100\n",
      "17642/17642 [==============================] - 2s 128us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 50/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 51/100\n",
      "17642/17642 [==============================] - 2s 139us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 52/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 53/100\n",
      "17642/17642 [==============================] - 3s 143us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 54/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 55/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 56/100\n",
      "17642/17642 [==============================] - 2s 109us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 57/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 58/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 59/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 60/100\n",
      "17642/17642 [==============================] - 2s 103us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 61/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 62/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 63/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 64/100\n",
      "17642/17642 [==============================] - 2s 141us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 65/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 66/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 67/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 68/100\n",
      "17642/17642 [==============================] - 2s 134us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 69/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 70/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 72/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 73/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 74/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 75/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 76/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 77/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 78/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 79/100\n",
      "17642/17642 [==============================] - 2s 138us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 80/100\n",
      "17642/17642 [==============================] - 3s 189us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 81/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 82/100\n",
      "17642/17642 [==============================] - 4s 243us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 83/100\n",
      "17642/17642 [==============================] - 2s 136us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 84/100\n",
      "17642/17642 [==============================] - 3s 148us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 85/100\n",
      "17642/17642 [==============================] - 2s 132us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 86/100\n",
      "17642/17642 [==============================] - 2s 132us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 87/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 88/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 89/100\n",
      "17642/17642 [==============================] - 3s 149us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 90/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 91/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 92/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 93/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 94/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 95/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 96/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 97/100\n",
      "17642/17642 [==============================] - 2s 118us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 98/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 99/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n",
      "Epoch 100/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 9.2836 - binary_accuracy: 0.3981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-332316830.0, -332316860.0, -332316900.0, -332316930.0, -332316960.0, -332317000.0, -332317020.0, -332317060.0, -332317100.0, -332317120.0, -332317150.0, -332317200.0, -332317220.0, -332317250.0, -332317280.0, -332317300.0, -332317340.0, -332317380.0, -332317400.0, -332317440.0, -332317470.0, -332317500.0, -332317540.0, -332317570.0, -332317600.0, -332317630.0, -332317660.0, -332317700.0, -332317730.0, -332317760.0, -332317860.0, -332317920.0, -332317950.0, -332317980.0, -332318000.0, -332318050.0, -332318080.0, -332318100.0, -332318140.0, -332318180.0, -332318200.0, -332318240.0, -332318270.0, -332318300.0, -332318340.0, -332318370.0, -332318400.0, -332318430.0, -332318460.0, -332318500.0, -332318530.0, -332318560.0, -332318600.0, -332318620.0, -332318660.0, -332318700.0, -332318720.0, -332318750.0, -332318800.0, -332318820.0, -332318850.0, -332318880.0, -332318900.0, -332318940.0, -332318980.0, -332319000.0, -332319040.0, -332319070.0, -332319100.0, -332319140.0, -332319170.0, -332319200.0, -332319230.0, -332319260.0, -332319300.0, -332319330.0, -332319360.0, -332319400.0, -332319420.0, -332319460.0, -332319500.0, -332319520.0, -332319550.0, -332319580.0, -332319600.0, -332319650.0, -332319680.0, -332319700.0, -332319740.0, -332319780.0, -332320000.0, -332320030.0, -332320060.0, -332320100.0, -332320130.0, -332320160.0, -332320200.0, -332320220.0, -332320260.0, -332320300.0, -332320320.0, -332320350.0, -332320400.0, -332320420.0, -43370920.0, -43370936.0, -43370944.0, -43370950.0, -43370960.0, -43370970.0, -43370976.0, -43370984.0, -43370990.0, -43371010.0, -43371016.0, -43371024.0, -43371030.0, -43371040.0, -43371050.0, -43371056.0, -43371064.0, -43371070.0, -43371080.0, -43371090.0, -43371096.0, -43371104.0, -43371110.0, -43371120.0, -43371130.0, -43371136.0, -43371144.0, -43371150.0, -43371160.0, -43371170.0, -43371176.0, -43371184.0, -43371190.0, -43371200.0, -43371210.0, -43371224.0, -43372570.0, -43372576.0, -43372584.0, -43372610.0, -43372616.0, -43372624.0, -43372630.0, -43372640.0, -43372650.0, -43372656.0, -43372664.0, -43372670.0, -43372680.0, -43372690.0, -43372696.0, -43372704.0, -43372710.0, -43372720.0, -43372730.0, -43372736.0, -43372744.0, -43372750.0, -43372760.0, -43372770.0, -43372776.0, -43372784.0, -43372790.0, -43372800.0, -43372810.0, -43372816.0, -43372824.0, -43372830.0, -43372840.0, -43372850.0, -43372856.0, -43372864.0, -43372870.0, -43372880.0, -43372890.0, -43372896.0, -43372904.0, -43372910.0, -43372920.0, -43372930.0, -43372936.0, -43372944.0, -43372950.0, -43372960.0, -43372970.0, -43372976.0, -43372984.0, -43372990.0, -43373000.0, -43373010.0, -43373016.0, -43373024.0, -43373030.0, -43373040.0, -43373050.0, -43373056.0, -43373064.0, -43373070.0, -43373080.0, -43373090.0, -43373096.0, -43373104.0, -43373110.0, -43373120.0, -43373130.0, -43373136.0, -43373144.0, -43373150.0, -43373160.0, -43373170.0, -43373176.0, -43373184.0, -43373190.0, -43373200.0, -43373210.0, -43373216.0, -43373224.0, -43373230.0, -43373240.0, -43373250.0, -43373256.0, -43373270.0, -43374056.0, -43374064.0, -43374070.0, -43374080.0, -43374090.0, -43374096.0, -43374110.0, -43374130.0, -43374136.0, -43374144.0, -43374150.0, -43374160.0, -43374170.0, -43374176.0, -43374184.0, -43374190.0, -43374200.0, -43374210.0, -43374216.0, -43374224.0, -43374230.0, -43374240.0, -43374250.0, -43374256.0, -43374264.0, -43374270.0, -43374280.0, -43374290.0, -43374296.0, -43374304.0, -43374310.0, -43374320.0, -43374330.0, -43374336.0, -43374344.0, -43374350.0, -43374360.0, -43374370.0, -43374376.0, -43374384.0, -43374390.0, -43374400.0, -43374410.0, -43374416.0, -43374424.0, -43374430.0, -43374440.0, -43374450.0, -43374456.0, -43374464.0, -43374470.0, -43374480.0, -43374490.0, -43374496.0, -43374504.0, -43374510.0, -43374520.0, -43374530.0, -43374536.0, -43374544.0, -43374550.0, -43374560.0, -43374570.0, -43374576.0, -43374584.0, -43374590.0, -43374600.0, -43374610.0, -43374616.0, -43374624.0, -43374630.0, -43374640.0, -43374650.0, -43374656.0, -43374664.0, -43374670.0, -43374680.0, -43374690.0, -43374696.0, -43374704.0, -43374710.0, -43374720.0, -43374730.0, -43374736.0, -43374744.0, -43374750.0, -43374760.0, -43374770.0, -43374776.0, -43374784.0, -43374790.0, -43374800.0, -43374810.0, -43374840.0, -43374856.0, -43374870.0, -43374896.0, -43374920.0, -43375810.0, -43375830.0, -43375840.0, -43375850.0, -43375856.0, -43375864.0, -43375870.0, -43375880.0, -43375890.0, -43375896.0, -43375904.0, -43375910.0, -43375920.0, -43375930.0, -43375936.0, -43375944.0, -43375950.0, -43375960.0, -43375970.0, -43375976.0, -43375984.0, -43375990.0, -43376000.0, -43376010.0, -43376016.0, -43376024.0, -43376030.0, -43376040.0, -43376050.0, -43376056.0, -43376064.0, -43376070.0, -43376080.0, -43376090.0, -43376096.0, -43376104.0, -43376110.0, -43376120.0, -43376130.0, -43376136.0, -43376144.0, -43376150.0, -43376160.0, -43376170.0, -43376176.0, -43376184.0, -43376190.0, -43376200.0, -43376210.0, -43376216.0, -43376224.0, -43376230.0, -43376240.0, -43376250.0, -43376256.0, -43376264.0, -43376270.0, -43376280.0, -43376290.0, -43376296.0, -43376304.0, -43376310.0, -43376320.0, -43376330.0, -43376336.0, -43376344.0, -43376350.0, -43376360.0, -43376370.0, -43376376.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8195420539560191\n",
      "Hamming Loss: 0.09215597370210836\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3679\n",
      "           1       0.17      1.00      0.28       732\n",
      "\n",
      "    accuracy                           0.17      4411\n",
      "   macro avg       0.08      0.50      0.14      4411\n",
      "weighted avg       0.03      0.17      0.05      4411\n",
      "\n",
      "Train on 17642 samples\n",
      "Epoch 1/100\n",
      "17642/17642 [==============================] - 3s 154us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 2/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 3/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 4/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 5/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 6/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 7/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 8/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 9/100\n",
      "17642/17642 [==============================] - 2s 139us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 10/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 11/100\n",
      "17642/17642 [==============================] - 3s 149us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 12/100\n",
      "17642/17642 [==============================] - 3s 154us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 13/100\n",
      "17642/17642 [==============================] - 2s 134us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 14/100\n",
      "17642/17642 [==============================] - 2s 134us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 15/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 16/100\n",
      "17642/17642 [==============================] - ETA: 0s - loss: 6.2502 - binary_accuracy: 0.592 - 2s 117us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 17/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 18/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 6.2499 - binary_accuracy: 0.5924s - loss: 6.2274 - binary_accurac\n",
      "Epoch 19/100\n",
      "17642/17642 [==============================] - 2s 138us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 20/100\n",
      "17642/17642 [==============================] - 2s 136us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 21/100\n",
      "17642/17642 [==============================] - 2s 129us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 22/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 23/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 24/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 25/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 26/100\n",
      "17642/17642 [==============================] - 3s 143us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 27/100\n",
      "17642/17642 [==============================] - 2s 137us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 28/100\n",
      "17642/17642 [==============================] - 2s 138us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 29/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 30/100\n",
      "17642/17642 [==============================] - 2s 123us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 31/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 32/100\n",
      "17642/17642 [==============================] - 2s 127us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 33/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 34/100\n",
      "17642/17642 [==============================] - 2s 105us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 35/100\n",
      "17642/17642 [==============================] - 2s 134us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 36/100\n",
      "17642/17642 [==============================] - 4s 199us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 37/100\n",
      "17642/17642 [==============================] - 3s 161us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 38/100\n",
      "17642/17642 [==============================] - 2s 129us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 39/100\n",
      "17642/17642 [==============================] - 2s 123us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 40/100\n",
      "17642/17642 [==============================] - 2s 126us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 41/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 42/100\n",
      "17642/17642 [==============================] - 2s 122us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 43/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 44/100\n",
      "17642/17642 [==============================] - 2s 130us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 45/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 46/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 47/100\n",
      "17642/17642 [==============================] - 2s 126us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 48/100\n",
      "17642/17642 [==============================] - 2s 126us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 49/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 50/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 51/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 52/100\n",
      "17642/17642 [==============================] - 2s 123us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 53/100\n",
      "17642/17642 [==============================] - 2s 123us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 54/100\n",
      "17642/17642 [==============================] - 2s 126us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 55/100\n",
      "17642/17642 [==============================] - 3s 151us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 56/100\n",
      "17642/17642 [==============================] - 2s 131us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 57/100\n",
      "17642/17642 [==============================] - 2s 129us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 58/100\n",
      "17642/17642 [==============================] - 2s 138us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 59/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 60/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 61/100\n",
      "17642/17642 [==============================] - 2s 107us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 62/100\n",
      "17642/17642 [==============================] - 2s 128us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 63/100\n",
      "17642/17642 [==============================] - 3s 151us/sample - loss: 6.2499 - binary_accuracy: 0.5924s - loss: 6.2260\n",
      "Epoch 64/100\n",
      "17642/17642 [==============================] - 3s 172us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 65/100\n",
      "17642/17642 [==============================] - 3s 173us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17642/17642 [==============================] - 3s 148us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 67/100\n",
      "17642/17642 [==============================] - 4s 201us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 68/100\n",
      "17642/17642 [==============================] - 3s 156us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 69/100\n",
      "17642/17642 [==============================] - 3s 156us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 70/100\n",
      "17642/17642 [==============================] - 3s 142us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 71/100\n",
      "17642/17642 [==============================] - 2s 127us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 72/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 73/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 74/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 75/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 76/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 77/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 78/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 79/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 80/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 81/100\n",
      "17642/17642 [==============================] - 2s 139us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 82/100\n",
      "17642/17642 [==============================] - 2s 126us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 83/100\n",
      "17642/17642 [==============================] - 2s 133us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 84/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 85/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 86/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 87/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 88/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 89/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 90/100\n",
      "17642/17642 [==============================] - 3s 166us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 91/100\n",
      "17642/17642 [==============================] - 3s 172us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 92/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 93/100\n",
      "17642/17642 [==============================] - 2s 127us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 94/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 95/100\n",
      "17642/17642 [==============================] - 2s 126us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 96/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 97/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 98/100\n",
      "17642/17642 [==============================] - 2s 123us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 99/100\n",
      "17642/17642 [==============================] - 2s 128us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n",
      "Epoch 100/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 6.2499 - binary_accuracy: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [217129150.0, 217129170.0, 217129180.0, 217129200.0, 217129220.0, 217129230.0, 217129340.0, 217129360.0, 217129380.0, 217129390.0, 217129400.0, 217129420.0, 217129440.0, 217129460.0, 217129470.0, 217129490.0, 217129500.0, 217129520.0, 217129540.0, 217129550.0, 217129570.0, 217129580.0, 217129600.0, 217129620.0, 217129630.0, 217129650.0, 217129660.0, 217129680.0, 217129700.0, 217129730.0, 217129800.0, 217129940.0, 217129950.0, 217129970.0, 217129980.0, 217130000.0, 217130020.0, 217130030.0, 217130050.0, 217130100.0, 217130130.0, 217130140.0, 217130180.0, 217130190.0, 217130200.0, 217130220.0, 217130240.0, 217130260.0, 217130270.0, 217130290.0, 217130300.0, 217130320.0, 217130340.0, 217130350.0, 217130370.0, 217130380.0, 217130400.0, 217130420.0, 217130430.0, 217130450.0, 217130460.0, 217130480.0, 217130500.0, 217130510.0, 217130530.0, 217130540.0, 217130560.0, 217130580.0, 217130600.0, 217130610.0, 217130620.0, 217130640.0, 217130660.0, 217130670.0, 217130690.0, 217130700.0, 217130720.0, 217130740.0, 217130750.0, 217130770.0, 217130780.0, 217130800.0, 217130820.0, 217130830.0, 217130850.0, 217130860.0, 217130880.0, 217130940.0, 217130960.0, 217130980.0, 217130990.0, 217131000.0, 217131020.0, 217131040.0, 217131060.0, 217131090.0, 217131120.0, 217131140.0, 217131150.0, 217131180.0, 217131900.0, 217131940.0, 217131950.0, 217131970.0, 217131980.0, 217132000.0, 217132020.0, 217132030.0, 217132050.0, 217132060.0, 217132080.0, 217132100.0, 217132110.0, 217132130.0, 217132140.0, 217132160.0, 217132180.0, 217132200.0, 217132210.0, 217132220.0, 217132240.0, 217132260.0, 217132270.0, 217132290.0, 217132370.0, 236876530.0, 236876540.0, 236876560.0, 236876580.0, 236876590.0, 236876600.0, 236876620.0, 236876640.0, 236876660.0, 236876670.0, 236876690.0, 236876700.0, 236876720.0, 236876740.0, 236876750.0, 236876770.0, 236876780.0, 236876800.0, 236876820.0, 236876830.0, 236876850.0, 236876860.0, 236876880.0, 236876900.0, 236876910.0, 236876930.0, 236876940.0, 236876960.0, 236876980.0, 236877000.0, 236877010.0, 236877060.0, 236877070.0, 236884590.0, 236884600.0, 236884620.0, 236884640.0, 236884660.0, 236884670.0, 236884690.0, 236884700.0, 236884720.0, 236884770.0, 236884780.0, 236884930.0, 236884940.0, 236884960.0, 236884980.0, 236885000.0, 236885010.0, 236885020.0, 236885040.0, 236885060.0, 236885070.0, 236885090.0, 236885100.0, 236885120.0, 236885140.0, 236885150.0, 236885170.0, 236885180.0, 236885200.0, 236885220.0, 236885230.0, 236885250.0, 236885260.0, 236885280.0, 236885300.0, 236885310.0, 236885330.0, 236885340.0, 236885360.0, 236885380.0, 236885390.0, 236885400.0, 236885420.0, 236885440.0, 236885460.0, 236885470.0, 236885490.0, 236885500.0, 236885520.0, 236885540.0, 236885550.0, 236885570.0, 236885580.0, 236885600.0, 236885620.0, 236885700.0, 236885800.0, 236885820.0, 236885840.0, 236885860.0, 236885870.0, 236885890.0, 236885970.0, 236886080.0, 236886110.0, 236886140.0, 236886160.0, 236886180.0, 236886190.0, 236886200.0, 236886220.0, 236886240.0, 236886260.0, 236886270.0, 236886290.0, 236886300.0, 236886320.0, 236886340.0, 236886350.0, 236886370.0, 236886380.0, 236886400.0, 236886420.0, 236886430.0, 236886450.0, 236886460.0, 236886480.0, 236886500.0, 236886510.0, 236886530.0, 236886540.0, 236886560.0, 236886580.0, 236886600.0, 236886610.0, 236886620.0, 236886640.0, 236886660.0, 236886670.0, 236886690.0, 236886700.0, 236886720.0, 236886740.0, 236886750.0, 236886770.0, 236886780.0, 236886800.0, 236886820.0, 236886830.0, 236886850.0, 236886860.0, 236886880.0, 236886900.0, 236886910.0, 236886930.0, 236886940.0, 236886960.0, 236886980.0, 236886990.0, 236887000.0, 236887020.0, 236887040.0, 236887060.0, 236887070.0, 236887090.0, 236887100.0, 236887120.0, 236887140.0, 236887150.0, 236887170.0, 236887180.0, 236887200.0, 236887220.0, 236887230.0, 236887250.0, 236887260.0, 236887280.0, 236887300.0, 236887310.0, 236887330.0, 236887340.0, 236887360.0, 236887380.0, 236887400.0, 236888420.0, 236888430.0, 236888450.0, 236888460.0, 236888480.0, 236888500.0, 236888510.0, 236888590.0, 236888600.0, 236888620.0, 236888640.0, 236888660.0, 236888670.0, 236888690.0, 236888700.0, 236888720.0, 236888740.0, 236888750.0, 236888770.0, 236888780.0, 236888800.0, 236888820.0, 236888830.0, 236888850.0, 236888860.0, 236888880.0, 236888900.0, 236888910.0, 236888930.0, 236888940.0, 236888960.0, 236888980.0, 236889000.0, 236889010.0, 236889020.0, 236889040.0, 236889060.0, 236889070.0, 236889090.0, 236889100.0, 236889120.0, 236889140.0, 236889150.0, 236889170.0, 236889180.0, 236889200.0, 236889220.0, 236889230.0, 236889250.0, 236889300.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7250056676490592\n",
      "Hamming Loss: 0.14883246429381092\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3717\n",
      "           1       0.16      1.00      0.27       694\n",
      "\n",
      "    accuracy                           0.16      4411\n",
      "   macro avg       0.08      0.50      0.14      4411\n",
      "weighted avg       0.02      0.16      0.04      4411\n",
      "\n",
      "Train on 17642 samples\n",
      "Epoch 1/100\n",
      "17642/17642 [==============================] - 3s 193us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 2/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 3/100\n",
      "17642/17642 [==============================] - 2s 107us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 4/100\n",
      "17642/17642 [==============================] - 2s 109us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 5/100\n",
      "17642/17642 [==============================] - 2s 118us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 6/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 7/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 8/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 9/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 10/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 11/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 12/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 13/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 14/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 15/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 16/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 17/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 18/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 19/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 20/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 21/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 22/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 23/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 24/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 25/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 26/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 27/100\n",
      "17642/17642 [==============================] - 2s 114us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 28/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 29/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 30/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 31/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 32/100\n",
      "17642/17642 [==============================] - ETA: 0s - loss: 13.7126 - binary_accuracy: 0.10 - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 33/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 34/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 35/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 36/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 37/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 38/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 39/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 40/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 41/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 42/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 43/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 44/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 45/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 46/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 47/100\n",
      "17642/17642 [==============================] - 3s 144us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 48/100\n",
      "17642/17642 [==============================] - 3s 162us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 49/100\n",
      "17642/17642 [==============================] - 2s 136us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 50/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 51/100\n",
      "17642/17642 [==============================] - 2s 129us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 52/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 53/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 54/100\n",
      "17642/17642 [==============================] - 3s 155us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 55/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 56/100\n",
      "17642/17642 [==============================] - 2s 124us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 57/100\n",
      "17642/17642 [==============================] - 2s 108us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 58/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 59/100\n",
      "17642/17642 [==============================] - 2s 128us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 60/100\n",
      "17642/17642 [==============================] - 2s 116us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 61/100\n",
      "17642/17642 [==============================] - 2s 119us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 62/100\n",
      "17642/17642 [==============================] - 2s 111us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 63/100\n",
      "17642/17642 [==============================] - 3s 155us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 64/100\n",
      "17642/17642 [==============================] - 2s 137us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 65/100\n",
      "17642/17642 [==============================] - 3s 164us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17642/17642 [==============================] - 2s 141us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 67/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 68/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 69/100\n",
      "17642/17642 [==============================] - 3s 151us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 70/100\n",
      "17642/17642 [==============================] - 2s 135us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 71/100\n",
      "17642/17642 [==============================] - 2s 103us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 72/100\n",
      "17642/17642 [==============================] - 2s 101us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 73/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 74/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 75/100\n",
      "17642/17642 [==============================] - 2s 109us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 76/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 77/100\n",
      "17642/17642 [==============================] - 2s 125us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 78/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 79/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 80/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 81/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 82/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 83/100\n",
      "17642/17642 [==============================] - 2s 110us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 84/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 85/100\n",
      "17642/17642 [==============================] - 2s 104us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 86/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 87/100\n",
      "17642/17642 [==============================] - 2s 115us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 88/100\n",
      "17642/17642 [==============================] - 2s 112us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 89/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 90/100\n",
      "17642/17642 [==============================] - 2s 96us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 91/100\n",
      "17642/17642 [==============================] - 2s 113us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 92/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 93/100\n",
      "17642/17642 [==============================] - 2s 100us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 94/100\n",
      "17642/17642 [==============================] - 2s 117us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 95/100\n",
      "17642/17642 [==============================] - 2s 120us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 96/100\n",
      "17642/17642 [==============================] - 2s 121us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 97/100\n",
      "17642/17642 [==============================] - 2s 99us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 98/100\n",
      "17642/17642 [==============================] - 2s 95us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 99/100\n",
      "17642/17642 [==============================] - 2s 98us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n",
      "Epoch 100/100\n",
      "17642/17642 [==============================] - 2s 106us/sample - loss: 13.7134 - binary_accuracy: 0.1086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-12361992.0, -12361996.0, -12362000.0, -12362004.0, -12362008.0, -12362012.0, -12362016.0, -12362020.0, -12362024.0, -12362028.0, -12362032.0, -12362036.0, -12362040.0, -12362044.0, -12362048.0, -12362052.0, -12362056.0, -12362060.0, -12362064.0, -12362068.0, -12362072.0, -12362076.0, -12362080.0, -12362084.0, -12362088.0, -12362092.0, -12362096.0, -12362100.0, -12362104.0, -12362108.0, -12362112.0, -12362116.0, -12362120.0, -12362124.0, -12362128.0, -12362132.0, -12362136.0, -12362140.0, -12362144.0, -12362148.0, -12362152.0, -12362156.0, -12362160.0, -12362164.0, -12362168.0, -12362172.0, -12362176.0, -12362180.0, -12362184.0, -12362188.0, -12362192.0, -12362196.0, -12362200.0, -12362204.0, -12362208.0, -12362212.0, -12362216.0, -12362220.0, -12362224.0, -12362228.0, -12362232.0, -12362236.0, -12362240.0, -12362244.0, -12362248.0, -12362252.0, -12362256.0, -12362260.0, -12362264.0, -12362268.0, -12362272.0, -12362276.0, -12362280.0, -12362284.0, -12362288.0, -12362292.0, -12362296.0, -12362300.0, -12362304.0, -12362308.0, -12362312.0, -12362316.0, -12362320.0, -12362324.0, -12362328.0, -12362332.0, -12362336.0, -12362340.0, -12362344.0, -12362348.0, -12362352.0, -12362356.0, -12362360.0, -12362364.0, -12362368.0, -12362372.0, -12362376.0, -12362380.0, -12362384.0, -12362388.0, -12362392.0, -12362396.0, -12362400.0, -12362404.0, -12362408.0, -12362412.0, -12362416.0, -12362420.0, -12362424.0, -12362428.0, -12362432.0, -12362436.0, -12362440.0, -12362444.0, -12362448.0, -12362452.0, -12362456.0, -12362460.0, -12362464.0, -12362468.0, -12362472.0, -12362476.0, -12362480.0, -12362484.0, -12362488.0, -12362492.0, -12362496.0, -12362500.0, -12362504.0, -12362508.0, -12362512.0, -12362516.0, -12362520.0, -12362524.0, -12362528.0, -12362532.0, -12362536.0, -12362540.0, -12362544.0, -12362548.0, -12362552.0, -12362556.0, -12362560.0, -12362564.0, -12362568.0, -12362572.0, -12362576.0, -12362580.0, -12362584.0, -12362588.0, -12362592.0, -12362596.0, -12362600.0, -12362604.0, -12362608.0, -12362612.0, -12362616.0, -12362620.0, -12362624.0, -12362628.0, -12362632.0, -12362636.0, -12362640.0, -12362644.0, -12362648.0, -12362652.0, -12362656.0, -12362660.0, -12362664.0, -12362668.0, -12362672.0, -12362676.0, -12362680.0, -12362684.0, -12362688.0, -12362692.0, -12362696.0, -12362700.0, -12362704.0, -12362708.0, -12362712.0, -12362716.0, -12362720.0, -12362724.0, -12362728.0, -12362732.0, -12362736.0, -12362740.0, -12362744.0, -12362748.0, -12362752.0, -12362756.0, -12362760.0, -12362764.0, -12362768.0, -12362772.0, -12362776.0, -12362784.0, -12362788.0, -12362796.0, -12362804.0, -12362820.0, -12362828.0, -12362832.0, -12362840.0, -12362852.0, -12362856.0, -12362860.0, -12362864.0, -12362868.0, -12362872.0, -12362876.0, -12362880.0, -12362884.0, -12362888.0, -12362892.0, -12362896.0, -12362900.0, -12362904.0, -12362912.0, -12362916.0, -12362920.0, -12362924.0, -12362932.0, -12362936.0, -12362940.0, -12362944.0, -12362948.0, -12362952.0, -12362956.0, -12362960.0, -12362964.0, -12362968.0, -12362972.0, -12362976.0, -12362980.0, -12362984.0, -12362988.0, -12362992.0, -12362996.0, -12363000.0, -12363004.0, -12363008.0, -12363012.0, -12363016.0, -12363020.0, -12363024.0, -12363028.0, -12363032.0, -12363036.0, -12363040.0, -12363044.0, -12363048.0, -12363052.0, -12363056.0, -12363060.0, -12363064.0, -12363068.0, -12363072.0, -12363076.0, -12363080.0, -12363084.0, -12363088.0, -12363092.0, -12363096.0, -12363100.0, -12363104.0, -12363108.0, -12363112.0, -12363116.0, -12363120.0, -12363124.0, -12363128.0, -12363132.0, -12363136.0, -12363140.0, -12363144.0, -12363152.0, -12363156.0, -12363160.0, -12363164.0, -12363168.0, -12363172.0, -12363176.0, -12363184.0, -12363192.0, -12363204.0, -12363220.0, -12363468.0, -12363476.0, -12363480.0, -12363484.0, -12363488.0, -12363492.0, -12363496.0, -12363500.0, -12363504.0, -12363508.0, -12363512.0, -12363516.0, -12363520.0, -12363524.0, -12363528.0, -12363532.0, -12363536.0, -12363540.0, -12363544.0, -12363548.0, -12363552.0, -12363556.0, -12363560.0, -12363564.0, -12363568.0, -12363572.0, -12363576.0, -12363580.0, -12363584.0, -12363588.0, -12363592.0, -12363600.0, -12363604.0, -12363624.0, -12363628.0, -12363648.0, 162849890.0, 162849900.0, 162849920.0, 162849940.0, 162849950.0, 162849980.0, 162850020.0, 162850030.0, 162850050.0, 162850060.0, 162850080.0, 162850100.0, 162850110.0, 162850130.0, 162850140.0, 162850160.0, 162850180.0, 162850190.0, 162850200.0, 162850220.0, 162850240.0, 162850260.0, 162850270.0, 162850290.0, 162850300.0, 162850320.0, 162850340.0, 162850350.0, 162850370.0, 162850380.0, 162850400.0, 162850420.0, 162850430.0, 162850460.0, 162850480.0, 162850500.0, 162850510.0, 162850530.0, 162850540.0, 162850560.0, 162850580.0, 162850620.0, 162850640.0, 162850660.0, 162850670.0, 162850690.0, 162850700.0, 162850720.0, 162850740.0, 162850750.0, 162850770.0, 162850780.0, 162850800.0, 162850820.0, 162850830.0, 162850850.0, 162850860.0, 162850880.0, 162850900.0, 162850910.0, 162850930.0, 162850940.0, 162850960.0, 162850980.0, 162850990.0, 162851000.0, 162851020.0, 162851040.0, 162851060.0, 162851070.0, 162851090.0, 162851100.0, 162851120.0, 162851140.0, 162851150.0, 162851170.0, 162851180.0, 162851200.0, 162851220.0, 162851230.0, 162851250.0, 162851260.0, 162851280.0, 162851300.0, 162851310.0, 162851330.0, 162851490.0, 162851520.0, 162851540.0, 162851550.0, 162851660.0, 162851700.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7975515756064384\n",
      "Hamming Loss: 0.1062117433688506\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3740\n",
      "           1       0.15      1.00      0.26       671\n",
      "\n",
      "    accuracy                           0.15      4411\n",
      "   macro avg       0.08      0.50      0.13      4411\n",
      "weighted avg       0.02      0.15      0.04      4411\n",
      "\n",
      "Train on 17643 samples\n",
      "Epoch 1/100\n",
      "17643/17643 [==============================] - 3s 184us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 2/100\n",
      "17643/17643 [==============================] - 2s 105us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 3/100\n",
      "17643/17643 [==============================] - 2s 119us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 4/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 5/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 6/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 7/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 8/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 9/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 10/100\n",
      "17643/17643 [==============================] - 2s 110us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 11/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 12/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 13/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 14/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 15/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 13.6569 - binary_accuracy: 0.1123 - loss: 13.66\n",
      "Epoch 16/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 17/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 18/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 19/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 20/100\n",
      "17643/17643 [==============================] - 2s 95us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 21/100\n",
      "17643/17643 [==============================] - 2s 95us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 22/100\n",
      "17643/17643 [==============================] - 2s 94us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 23/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 24/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 25/100\n",
      "17643/17643 [==============================] - 2s 124us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 26/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 27/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 28/100\n",
      "17643/17643 [==============================] - 2s 122us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 29/100\n",
      "17643/17643 [==============================] - 2s 122us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 30/100\n",
      "17643/17643 [==============================] - 2s 124us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 31/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 32/100\n",
      "17643/17643 [==============================] - 2s 131us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 33/100\n",
      "17643/17643 [==============================] - 2s 120us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 34/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 35/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 36/100\n",
      "17643/17643 [==============================] - 2s 110us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 37/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 38/100\n",
      "17643/17643 [==============================] - 2s 110us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 39/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 40/100\n",
      "17643/17643 [==============================] - 2s 116us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 41/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 42/100\n",
      "17643/17643 [==============================] - 2s 132us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 43/100\n",
      "17643/17643 [==============================] - 2s 123us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 44/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 45/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 46/100\n",
      "17643/17643 [==============================] - 2s 100us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 47/100\n",
      "17643/17643 [==============================] - 2s 94us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 48/100\n",
      "17643/17643 [==============================] - 2s 100us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 49/100\n",
      "17643/17643 [==============================] - 2s 94us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 50/100\n",
      "17643/17643 [==============================] - 2s 95us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 51/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 52/100\n",
      "17643/17643 [==============================] - 2s 94us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 53/100\n",
      "17643/17643 [==============================] - 2s 94us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 54/100\n",
      "17643/17643 [==============================] - 2s 95us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 55/100\n",
      "17643/17643 [==============================] - 2s 95us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 56/100\n",
      "17643/17643 [==============================] - 2s 95us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 57/100\n",
      "17643/17643 [==============================] - 2s 94us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 58/100\n",
      "17643/17643 [==============================] - 2s 95us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 59/100\n",
      "17643/17643 [==============================] - 2s 120us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 60/100\n",
      "17643/17643 [==============================] - 2s 119us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 61/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 62/100\n",
      "17643/17643 [==============================] - 2s 98us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 63/100\n",
      "17643/17643 [==============================] - 2s 103us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 64/100\n",
      "17643/17643 [==============================] - 2s 105us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 65/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17643/17643 [==============================] - 2s 130us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 67/100\n",
      "17643/17643 [==============================] - 2s 105us/sample - loss: 13.6569 - binary_accuracy: 0.1123 - loss: 13.6486 - binary_accu\n",
      "Epoch 68/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 69/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 70/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 71/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 72/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 73/100\n",
      "17643/17643 [==============================] - 2s 97us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 74/100\n",
      "17643/17643 [==============================] - 2s 102us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 75/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 76/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 77/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 78/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 79/100\n",
      "17643/17643 [==============================] - 3s 144us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 80/100\n",
      "17643/17643 [==============================] - 3s 182us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 81/100\n",
      "17643/17643 [==============================] - 3s 177us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 82/100\n",
      "17643/17643 [==============================] - 3s 164us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 83/100\n",
      "17643/17643 [==============================] - 3s 161us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 84/100\n",
      "17643/17643 [==============================] - 3s 180us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 85/100\n",
      "17643/17643 [==============================] - 2s 115us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 86/100\n",
      "17643/17643 [==============================] - 3s 143us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 87/100\n",
      "17643/17643 [==============================] - 2s 133us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 88/100\n",
      "17643/17643 [==============================] - 2s 105us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 89/100\n",
      "17643/17643 [==============================] - 2s 118us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 90/100\n",
      "17643/17643 [==============================] - 2s 133us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 91/100\n",
      "17643/17643 [==============================] - 2s 119us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 92/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 93/100\n",
      "17643/17643 [==============================] - 3s 143us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 94/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 95/100\n",
      "17643/17643 [==============================] - 2s 135us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 96/100\n",
      "17643/17643 [==============================] - 4s 201us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 97/100\n",
      "17643/17643 [==============================] - 3s 184us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 98/100\n",
      "17643/17643 [==============================] - 2s 130us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 99/100\n",
      "17643/17643 [==============================] - 2s 128us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n",
      "Epoch 100/100\n",
      "17643/17643 [==============================] - 2s 118us/sample - loss: 13.6569 - binary_accuracy: 0.1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-217234850.0, -217234860.0, -217234880.0, -217234900.0, -217235040.0, -217235060.0, -217235070.0, -217235090.0, -217235100.0, -217235120.0, -217235140.0, -217235150.0, -217235170.0, -217235180.0, -217235200.0, -217235220.0, -217235230.0, -217235250.0, -217235260.0, -217239620.0, -217239630.0, -217239650.0, -217239660.0, -217239680.0, -217239700.0, -217239710.0, -217239740.0, -217240320.0, -217240340.0, -217240350.0, -217240370.0, -217240380.0, -217240400.0, -217240420.0, -217240430.0, -217240450.0, -217240460.0, -217240480.0, -217240500.0, -217240510.0, -217240530.0, -217240540.0, -217240560.0, -217240580.0, -217240590.0, -217240600.0, -217240620.0, -217240640.0, -217240660.0, -217240670.0, -217240690.0, -217240700.0, -217240720.0, -217240740.0, -217240750.0, -217240770.0, -217240780.0, -217240800.0, -217240980.0, -217241000.0, -217241010.0, -217241020.0, -217241040.0, -217241060.0, -217241070.0, -217241740.0, -217241760.0, -217241800.0, -217241810.0, -217241820.0, -217241840.0, -217241860.0, -217241870.0, -217241890.0, -217241900.0, -217241920.0, -217241940.0, -217241950.0, -217241970.0, -217241980.0, -217242000.0, -217242020.0, -217242030.0, -217242050.0, -217242060.0, -217242080.0, -217242100.0, -217242110.0, -217242130.0, -217242140.0, -217242160.0, -217242180.0, -217242190.0, -217242200.0, -217245280.0, -217245300.0, -217245310.0, -217245330.0, -217245340.0, -217245360.0, -217245380.0, -217245390.0, -217245400.0, -217245420.0, -217245440.0, -217245460.0, -217245470.0, -217245490.0, -217245500.0, -217245520.0, -217245540.0, -217245550.0, -217245570.0, -217245580.0, -217245600.0, -217245620.0, 22459264.0, 22459272.0, 22459280.0, 22459288.0, 22459296.0, 22459304.0, 22459312.0, 22459320.0, 22459328.0, 22459336.0, 22459344.0, 22459352.0, 22459360.0, 22459368.0, 22459376.0, 22459384.0, 22459392.0, 22459400.0, 22459408.0, 22459416.0, 22459424.0, 22459432.0, 22459440.0, 22459448.0, 22459456.0, 22459464.0, 22459472.0, 22459480.0, 22459488.0, 22459496.0, 22459504.0, 22459512.0, 22459520.0, 22459528.0, 22459536.0, 22459544.0, 22459552.0, 22459560.0, 22459568.0, 22459576.0, 22459584.0, 22459592.0, 22459600.0, 22459608.0, 22459616.0, 22459624.0, 22459632.0, 22459640.0, 22459648.0, 22459656.0, 22459664.0, 22459672.0, 22459680.0, 22459688.0, 22459696.0, 22459704.0, 22459712.0, 22459720.0, 22459728.0, 22459736.0, 22459744.0, 22459752.0, 22459760.0, 22460392.0, 22460400.0, 22460408.0, 22460416.0, 22460424.0, 22460432.0, 22460440.0, 22460448.0, 22460456.0, 22460464.0, 22460472.0, 22460480.0, 22460488.0, 22460496.0, 22460504.0, 22460512.0, 22460520.0, 22460528.0, 22460536.0, 22460544.0, 22460552.0, 22460560.0, 22460568.0, 22460576.0, 22460584.0, 22460592.0, 22460600.0, 22460608.0, 22460616.0, 22460624.0, 22460632.0, 22460640.0, 22460648.0, 22460656.0, 22460664.0, 22460672.0, 22460680.0, 22460688.0, 22460696.0, 22460704.0, 22460712.0, 22460720.0, 22460728.0, 22460736.0, 22460744.0, 22460752.0, 22461192.0, 22461240.0, 22461256.0, 22461264.0, 22461272.0, 22461280.0, 22461288.0, 22461296.0, 22461312.0, 22461320.0, 22461328.0, 22461336.0, 22461344.0, 22461352.0, 22461360.0, 22461368.0, 22461376.0, 22461384.0, 22461392.0, 22461400.0, 22461408.0, 22461416.0, 22461424.0, 22461432.0, 22461440.0, 22461448.0, 22461456.0, 22461464.0, 22461472.0, 22461480.0, 22461488.0, 22461496.0, 22461504.0, 22461512.0, 22461520.0, 22461528.0, 22461536.0, 22461544.0, 22461552.0, 22461560.0, 22461568.0, 22461576.0, 22461584.0, 22461592.0, 22461600.0, 22461608.0, 22461616.0, 22461624.0, 22461632.0, 22461640.0, 22461648.0, 22461656.0, 22461664.0, 22461672.0, 22461680.0, 22461688.0, 22461696.0, 22461704.0, 22461712.0, 22461720.0, 22461728.0, 22461736.0, 22461744.0, 22461752.0, 22461760.0, 22461768.0, 22461776.0, 22461784.0, 22461792.0, 22461800.0, 22461808.0, 22461816.0, 22461824.0, 22461832.0, 22461840.0, 22461848.0, 22461856.0, 22461864.0, 22461872.0, 22461880.0, 22461888.0, 22461896.0, 22461904.0, 22461912.0, 22461920.0, 22461928.0, 22461936.0, 22461944.0, 22461952.0, 22461960.0, 22461968.0, 22461976.0, 22461984.0, 22461992.0, 22462000.0, 22462008.0, 22462016.0, 22462024.0, 22462032.0, 22462040.0, 22462048.0, 22462056.0, 22462064.0, 22462072.0, 22462080.0, 22462088.0, 22462096.0, 22462104.0, 22462112.0, 22462120.0, 22462128.0, 22462136.0, 22462144.0, 22462152.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8170068027210884\n",
      "Hamming Loss: 0.09149659863945578\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3603\n",
      "           1       0.18      1.00      0.31       807\n",
      "\n",
      "    accuracy                           0.18      4410\n",
      "   macro avg       0.09      0.50      0.15      4410\n",
      "weighted avg       0.03      0.18      0.06      4410\n",
      "\n",
      "Train on 17643 samples\n",
      "Epoch 1/100\n",
      "17643/17643 [==============================] - 3s 186us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 2/100\n",
      "17643/17643 [==============================] - 2s 120us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 3/100\n",
      "17643/17643 [==============================] - 2s 123us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 4/100\n",
      "17643/17643 [==============================] - 2s 138us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 5/100\n",
      "17643/17643 [==============================] - 2s 121us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 6/100\n",
      "17643/17643 [==============================] - 2s 127us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 7/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 8/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 9/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 10/100\n",
      "17643/17643 [==============================] - 2s 103us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 11/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 12/100\n",
      "17643/17643 [==============================] - 2s 125us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 13/100\n",
      "17643/17643 [==============================] - 2s 126us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 14/100\n",
      "17643/17643 [==============================] - 2s 116us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 15/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 16/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 17/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 18/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 19/100\n",
      "17643/17643 [==============================] - 2s 115us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 20/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 21/100\n",
      "17643/17643 [==============================] - 2s 116us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 22/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 23/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 24/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 25/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 26/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 27/100\n",
      "17643/17643 [==============================] - 2s 118us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 28/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 29/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 30/100\n",
      "17643/17643 [==============================] - 2s 110us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 31/100\n",
      "17643/17643 [==============================] - 2s 112us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 32/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 33/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 34/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 35/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 36/100\n",
      "17643/17643 [==============================] - 2s 115us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 37/100\n",
      "17643/17643 [==============================] - 3s 143us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 38/100\n",
      "17643/17643 [==============================] - 2s 136us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 39/100\n",
      "17643/17643 [==============================] - 2s 135us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 40/100\n",
      "17643/17643 [==============================] - 2s 128us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 41/100\n",
      "17643/17643 [==============================] - 2s 118us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 42/100\n",
      "17643/17643 [==============================] - 2s 110us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 43/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 44/100\n",
      "17643/17643 [==============================] - 2s 121us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 45/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 46/100\n",
      "17643/17643 [==============================] - 2s 121us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 47/100\n",
      "17643/17643 [==============================] - 3s 185us/sample - loss: 1.6913 - binary_accuracy: 0.8903s - loss: 1.6795 - binary_\n",
      "Epoch 48/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 49/100\n",
      "17643/17643 [==============================] - 2s 101us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 50/100\n",
      "17643/17643 [==============================] - 2s 105us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 51/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 52/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 53/100\n",
      "17643/17643 [==============================] - 2s 110us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 54/100\n",
      "17643/17643 [==============================] - 2s 134us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 55/100\n",
      "17643/17643 [==============================] - 2s 126us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 56/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 57/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 58/100\n",
      "17643/17643 [==============================] - 3s 180us/sample - loss: 1.6913 - binary_accuracy: 0.8903s - loss: 1.699\n",
      "Epoch 59/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 60/100\n",
      "17643/17643 [==============================] - 2s 114us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 61/100\n",
      "17643/17643 [==============================] - 2s 136us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 62/100\n",
      "17643/17643 [==============================] - 2s 108us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 63/100\n",
      "17643/17643 [==============================] - 2s 100us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 64/100\n",
      "17643/17643 [==============================] - 2s 99us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 65/100\n",
      "17643/17643 [==============================] - 2s 107us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17643/17643 [==============================] - 2s 101us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 67/100\n",
      "17643/17643 [==============================] - 2s 102us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 68/100\n",
      "17643/17643 [==============================] - 2s 100us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 69/100\n",
      "17643/17643 [==============================] - 2s 105us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 70/100\n",
      "17643/17643 [==============================] - 2s 111us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 71/100\n",
      "17643/17643 [==============================] - 2s 113us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 72/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 73/100\n",
      "17643/17643 [==============================] - 2s 104us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 74/100\n",
      "17643/17643 [==============================] - 2s 109us/sample - loss: 1.6913 - binary_accuracy: 0.8903s - lo\n",
      "Epoch 75/100\n",
      "17643/17643 [==============================] - 2s 110us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 76/100\n",
      "17643/17643 [==============================] - 3s 163us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 77/100\n",
      "17643/17643 [==============================] - 3s 160us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 78/100\n",
      "17643/17643 [==============================] - 2s 141us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 79/100\n",
      "17643/17643 [==============================] - 2s 133us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 80/100\n",
      "17643/17643 [==============================] - 3s 148us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 81/100\n",
      "17643/17643 [==============================] - 2s 136us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 82/100\n",
      "17643/17643 [==============================] - 2s 106us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 83/100\n",
      "17643/17643 [==============================] - 2s 117us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 84/100\n",
      "17643/17643 [==============================] - 2s 118us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n",
      "Epoch 85/100\n",
      "17643/17643 [==============================] - 2s 119us/sample - loss: 1.6913 - binary_accuracy: 0.8903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-29278012.0, -29278020.0, -29278022.0, -29278024.0, -29278026.0, -29278030.0, -29278032.0, -29278034.0, -29278036.0, -29278038.0, -29278040.0, -29278042.0, -29278044.0, -29278046.0, -29278048.0, -29278050.0, -29278052.0, -29278054.0, -29278056.0, -29278058.0, -29278060.0, -29278062.0, -29278064.0, -29278066.0, -29278068.0, -29278070.0, -29278072.0, -29278074.0, -29278076.0, -29278078.0, -29278080.0, -29278082.0, -29278084.0, -29278086.0, -29278088.0, -29278092.0, -29278096.0, -29278098.0, -29278100.0, -29278106.0, -29278108.0, -29278120.0, -29278408.0, -29278414.0, -29278440.0, -29278442.0, -29278444.0, -29278452.0, -29278454.0, -29278460.0, -29278464.0, -29278466.0, -29278468.0, -29278470.0, -29278474.0, -29278476.0, -29278482.0, -29278486.0, -29278746.0, -29278754.0, -29278756.0, -29278758.0, -29278760.0, -29278762.0, -29278764.0, -29278766.0, -29278768.0, -29278770.0, -29278772.0, -29278774.0, -29278776.0, -29278778.0, -29278780.0, -29278782.0, -29278784.0, -29278786.0, -29278788.0, -29278792.0, -29278794.0, -29278796.0, -29278798.0, -29278800.0, -29278802.0, -29278804.0, -29278806.0, -29278808.0, -29278810.0, -29278812.0, -29278814.0, -29278816.0, -29278818.0, -29278820.0, -29278822.0, -29278824.0, -29278826.0, -29278828.0, -29278830.0, -29278832.0, -29278834.0, -29278836.0, -29278838.0, -29278840.0, -29278842.0, -29278844.0, -29278846.0, -29278848.0, -29278850.0, -29278852.0, -29278854.0, -29278856.0, -29278858.0, -29278860.0, -29278862.0, -29278864.0, -29278868.0, -29278870.0, -29278872.0, -29278874.0, -29278876.0, -29278878.0, -29278880.0, -29278882.0, -29278884.0, -29278886.0, -29278888.0, -29278890.0, -29278892.0, -29278894.0, -29278896.0, -29278898.0, -29278900.0, -29278902.0, -29278904.0, -29278906.0, -29278908.0, -29278912.0, -29278914.0, -29278916.0, -29278918.0, -29278920.0, -29278922.0, -29278924.0, -29278926.0, -29278928.0, -29278930.0, -29278932.0, -29278934.0, -29278936.0, -29278938.0, -29278940.0, -29278942.0, -29278944.0, -29278946.0, -29278948.0, -29278952.0, -29278954.0, -29278956.0, -29278958.0, -29278960.0, -29278962.0, -29278964.0, -29278966.0, -29278968.0, -29278970.0, -29278972.0, -29278974.0, -29278976.0, -29278978.0, -29278980.0, -29278982.0, -29278984.0, -29278986.0, -29278988.0, -29278990.0, -29278992.0, -29278994.0, -29278996.0, -29278998.0, -29279000.0, -29279002.0, -29279004.0, -29279006.0, -29279008.0, -29279010.0, -29279012.0, -29279014.0, -29279016.0, -29279018.0, -29279020.0, -29279022.0, -29279024.0, -29279026.0, -29279028.0, -29279030.0, -29279032.0, -29279034.0, -29279036.0, -29279038.0, -29279040.0, -29279042.0, -29279044.0, -29279046.0, -29279048.0, -29279050.0, -29279052.0, -29279054.0, -29279056.0, -29279058.0, -29279060.0, -29279062.0, -29279064.0, -29279066.0, -29279068.0, -29279070.0, -29279072.0, -29279074.0, -29279076.0, -29279078.0, -29279080.0, -29279082.0, -29279084.0, -29279086.0, -29279088.0, -29279092.0, -29279094.0, -29279098.0, -29279102.0, -29279116.0, -29279120.0, -29279876.0, -29279882.0, -29279884.0, -29279888.0, -29279890.0, -29279892.0, -29279898.0, -29279900.0, -29279902.0, -29279904.0, -29279908.0, -29279912.0, -29279914.0, -29279920.0, -29279926.0, -29279928.0, -29279934.0, -29279936.0, -29279940.0, -29279944.0, -29279946.0, -29279948.0, -29279950.0, -29279952.0, -29279954.0, -29279956.0, -29279958.0, -29279960.0, -29279962.0, -29279964.0, -29279966.0, -29279968.0, -29279970.0, -29279972.0, -29279974.0, -29279976.0, -29279978.0, -29279980.0, -29279982.0, -29279984.0, -29279986.0, -29279988.0, -29279992.0, -29279994.0, -29279996.0, -29279998.0, -29280000.0, -29280002.0, -29280004.0, -29280006.0, -29280008.0, -29280010.0, -29280012.0, -29280014.0, -29280016.0, -29280018.0, -29280020.0, -29280022.0, -29280024.0, -29280026.0, -29280028.0, -29280030.0, -29280032.0, -29280034.0, -29280036.0, -29280038.0, -29280040.0, -29280042.0, -29280044.0, -29280046.0, -29280048.0, -29280050.0, -29280052.0, -29280056.0, -29280058.0, -29280060.0, -29280062.0, -29280064.0, -29280068.0, -29280070.0, -29280072.0, -29280074.0, -29280076.0, -29280078.0, -29280080.0, -29280082.0, -29280084.0, -29280086.0, -29280088.0, -29280090.0, -29280092.0, -29280094.0, -29280096.0, -29280098.0, -29280100.0, -29280102.0, -29280104.0, -29280106.0, -29280108.0, -29280110.0, -29280112.0, -29280114.0, -29280116.0, -29280118.0, -29280120.0, -29280122.0, -29280124.0, -29280126.0, -29280128.0, -29280130.0, -29280132.0, -29280134.0, -29280136.0, -29280138.0, -29280140.0, -29280142.0, -29280144.0, -29280146.0, -29280148.0, -29280150.0, -29280152.0, -29280154.0, -29280156.0, -29280158.0, -29280160.0, -29280162.0, -29280164.0, -29280166.0, -29280168.0, -29280170.0, -29280172.0, -29280174.0, -29280176.0, -29280178.0, -29280180.0, -29280182.0, -29280184.0, -29280186.0, -29280188.0, -29280190.0, -29280192.0, -29280194.0, -29280196.0, -29280198.0, -29280200.0, -29280204.0, -29280206.0, -29280208.0, -29280210.0, -29280212.0, -29280214.0, -29280216.0, -29280218.0, -29280220.0, -29280222.0, -29280224.0, -29280226.0, -29280228.0, -29280230.0, -29280232.0, -29280234.0, -29280236.0, -29280238.0, -29280240.0, -29280242.0, -29280244.0, -29280246.0, -29280248.0, -29280250.0, -29280252.0, -29280256.0, -29280258.0, -29280260.0, -29280262.0, -29280264.0, -29280266.0, -29280268.0, -29280270.0, -29280272.0, -29280274.0, -29280276.0, -29280278.0, -29280280.0, -29280282.0, -29280284.0, -29280286.0, -29280288.0, -29280290.0, -29280292.0, -29280294.0, -29280296.0, -29280298.0, -29280300.0, -29280304.0, -29280306.0, -29280308.0, -29280312.0, -29280316.0, -29280318.0, -29280320.0, -29280324.0, -29280326.0, -29280328.0, -29280332.0, -29280334.0, -29280336.0, -29280338.0, -29280340.0, -29280344.0, -29280346.0, -29280348.0, -29280352.0, -29280354.0, -29280356.0, -29280360.0, -29280362.0, -29280364.0, -29280366.0, -29280368.0, -29280370.0, -29280372.0, -29280374.0, -29280376.0, -29280378.0, -29280380.0, -29280382.0, -29280384.0, -29280386.0, -29280388.0, -29280390.0, -29280392.0, -29280394.0, -29280396.0, -29280398.0, -29280400.0, -29280402.0, -29280404.0, -29280406.0, -29280408.0, -29280410.0, -29280412.0, -29280414.0, -29280416.0, -29280418.0, -29280420.0, -29280422.0, -29280424.0, -29280426.0, -29280428.0, -29280430.0, -29280432.0, -29280434.0, -29280436.0, -29280438.0, -29280440.0, -29280442.0, -29280444.0, -29280446.0, -29280448.0, -29280450.0, -29280452.0, -29280454.0, -29280456.0, -29280458.0, -29280460.0, -29280462.0, -29280464.0, -29280466.0, -29280468.0, -29280470.0, -29280472.0, -29280474.0, -29280476.0, -29280478.0, -29280480.0, -29280484.0, -29280486.0, -29280488.0, -29280490.0, -29280492.0, -29280496.0, -29280498.0, -29280500.0, -29280502.0, -29280504.0, -29280506.0, -29280508.0, -29280512.0, -29280514.0, -29280516.0, -29280518.0, -29280520.0, -29280522.0, -29280524.0, -29280526.0, -29280528.0, -29280530.0, -29280532.0, -29280534.0, -29280536.0, -29280538.0, -29280540.0, -29280542.0, -29280544.0, -29280546.0, -29280548.0, -29280550.0, -29280552.0, -29280554.0, -29280556.0, -29280558.0, -29280560.0, -29280562.0, -29280564.0, -29280566.0, -29280568.0, -29280570.0, -29280572.0, -29280574.0, -29280576.0, -29280578.0, -29280580.0, -29280582.0, -29280584.0, -29280586.0, -29280588.0, -29280590.0, -29280592.0, -29280594.0, -29280596.0, -29280598.0, -29280600.0, -29280602.0, -29280604.0, -29280606.0, -29280608.0, -29280610.0, -29280612.0, -29280614.0, -29280616.0, -29280618.0, -29280620.0, -29280622.0, -29280624.0, -29280626.0, -29280628.0, -29280630.0, -29280632.0, -29280634.0, -29280636.0, -29280638.0, -29280640.0, -29280642.0, -29280644.0, -29280646.0, -29280648.0, -29280650.0, -29280652.0, -29280654.0, -29280656.0, -29280658.0, -29280660.0, -29280662.0, -29280664.0, -29280666.0, -29280668.0, -29280670.0, -29280672.0, -29280674.0, -29280676.0, -29280678.0, -29280680.0, -29280682.0, -29280684.0, -29280686.0, -29280688.0, -29280690.0, -29280692.0, -29280694.0, -29280696.0, -29280698.0, -29280700.0, -29280702.0, -29280704.0, -29280706.0, -29280708.0, -29280710.0, -29280712.0, -29280714.0, -29280716.0, -29280720.0, -29280722.0, -29280724.0, -29280726.0, -29280728.0, -29280730.0, -29280732.0, -29280736.0, -29280738.0, -29280740.0, -29280742.0, -29280744.0, -29280746.0, -29280748.0, -29280750.0, -29280752.0, -29280756.0, -29280758.0, -29280760.0, -29280762.0, -29280764.0, -29280766.0, -29280768.0, -29280770.0, -29280772.0, -29280774.0, -29280776.0, -29280780.0, -29280782.0, -29280784.0, -29280786.0, -29280788.0, -29280790.0, -29280792.0, -29280794.0, -29280796.0, -29280798.0, -29280800.0, -29280802.0, -29280804.0, -29280806.0, -29280808.0, -29280812.0, -29280814.0, -29280816.0, -29280820.0, -29280822.0, -29280824.0, -29280826.0, -29280828.0, -29280830.0, -29280832.0, -29280834.0, -29280836.0, -29280838.0, -29280840.0, -29280842.0, -29280844.0, -29280846.0, -29280848.0, -29280850.0, -29280852.0, -29280854.0, -29280856.0, -29280858.0, -29280860.0, -29280862.0, -29280864.0, -29280866.0, -29280868.0, -29280870.0, -29280872.0, -29280874.0, -29280876.0, -29280878.0, -29280880.0, -29280882.0, -29280884.0, -29280886.0, -29280888.0, -29280890.0, -29280892.0, -29280894.0, -29280896.0, -29280898.0, -29280900.0, -29280902.0, -29280904.0, -29280906.0, -29280908.0, -29280910.0, -29280912.0, -29280914.0, -29280916.0, -29280918.0, -29280920.0, -29280922.0, -29280924.0, -29280926.0, -29280928.0, -29280930.0, -29280932.0, -29280934.0, -29280936.0, -29280938.0, -29280940.0, -29280942.0, -29280944.0, -29280946.0, -29280948.0, -29280950.0, -29280952.0, -29280954.0, -29280956.0, -29280958.0, -29280960.0, -29280962.0, -29280964.0, -29280966.0, -29280968.0, -29280970.0, -29280972.0, -29280974.0, -29280976.0, -29280978.0, -29280980.0, -29280982.0, -29280984.0, -29280986.0, -29280988.0, -29280990.0, -29280992.0, -29280994.0, -29281000.0, -29281008.0, -29282514.0, -29282524.0, -29282536.0, -29282546.0, -29282548.0, -29282550.0, -29282552.0, -29282554.0, -29282560.0, -29282562.0, -29282564.0, -29282566.0, -29282568.0, -29282570.0, -29282572.0, -29282576.0, -29282578.0, -29282580.0, -29282582.0, -29282584.0, -29282586.0, -29282588.0, -29282590.0, -29282592.0, -29282594.0, -29282596.0, -29282598.0, -29282600.0, -29282602.0, -29282604.0, -29282606.0, -29282608.0, -29282610.0, -29282612.0, -29282614.0, -29282616.0, -29282618.0, -29282620.0, -29282622.0, -29282624.0, -29282626.0, -29282628.0, -29282630.0, -29282632.0, -29282634.0, -29282636.0, -29282638.0, -29282640.0, -29282642.0, -29282644.0, -29282646.0, -29282648.0, -29282650.0, -29282652.0, -29282654.0, -29282656.0, -29282658.0, -29282660.0, -29282662.0, -29282664.0, -29282666.0, -29282668.0, -29282670.0, -29282672.0, -29282674.0, -29282676.0, -29282678.0, -29282680.0, -29282682.0, -29282684.0, -29282686.0, -29282688.0, -29282690.0, -29282692.0, -29282694.0, -29282696.0, -29282698.0, -29282700.0, -29282702.0, -29282704.0, -29282706.0, -29282708.0, -29282710.0, -29282712.0, -29282714.0, -29282716.0, -29282718.0, -29282720.0, -29282722.0, -29282724.0, -29282728.0, -29282730.0, -29282732.0, -29282734.0, -29282736.0, -29282738.0, -29282740.0, -29282742.0, -29282744.0, -29282746.0, -29282748.0, -29282750.0, -29282752.0, -29282754.0, -29282756.0, -29282758.0, -29282760.0, -29282762.0, -29282764.0, -29282766.0, -29282768.0, -29282770.0, -29282772.0, -29282774.0, -29282776.0, -29282778.0, -29282780.0, -29282782.0, -29282784.0, -29282786.0, -29282788.0, -29282790.0, -29282792.0, -29282794.0, -29282796.0, -29282798.0, -29282800.0, -29282802.0, -29282804.0, -29282806.0, -29282808.0, -29282810.0, -29282812.0, -29282814.0, -29282816.0, -29282818.0, -29282820.0, -29282822.0, -29282824.0, -29282826.0, -29282828.0, -29282830.0, -29282832.0, -29282834.0, -29282836.0, -29282838.0, -29282840.0, -29282842.0, -29282844.0, -29282846.0, -29282848.0, -29282850.0, -29282852.0, -29282854.0, -29282856.0, -29282858.0, -29282860.0, -29282862.0, -29282864.0, -29282868.0, -29282872.0, -29282876.0, -29282880.0, -29282882.0, -29282884.0, -29282886.0, -29282888.0, -29282892.0, -29282896.0, -29282900.0, -29282904.0, -29282908.0, 40966596.0, 40966612.0, 40966624.0, 40966628.0, 40966630.0, 40966636.0, 40966640.0, 40966644.0, 40966650.0, 40966652.0, 40966656.0, 40966660.0, 40966664.0, 40966668.0, 40966670.0, 40966676.0, 40966680.0, 40966684.0, 40966690.0, 40966692.0, 40966696.0, 40966700.0, 40966704.0, 40966708.0, 40966710.0, 40966716.0, 40966720.0, 40966724.0, 40966730.0, 40966732.0, 40966736.0, 40966740.0, 40966744.0, 40966748.0, 40966750.0, 40966756.0, 40966760.0, 40966764.0, 40966770.0, 40966772.0, 40966776.0, 40966780.0, 40966784.0, 40966790.0, 40968230.0, 40968250.0, 40968252.0, 40968260.0, 40968264.0, 40968270.0, 40968276.0, 40968280.0, 40968284.0, 40968290.0, 40968292.0, 40968296.0, 40968300.0, 40968304.0, 40968308.0, 40968310.0, 40968316.0, 40968320.0, 40968324.0, 40968330.0, 40968332.0, 40968336.0, 40968340.0, 40968344.0, 40968348.0, 40968350.0, 40968356.0, 40968360.0, 40968364.0, 40968370.0, 40968372.0, 40968376.0, 40968380.0, 40968390.0, 40968644.0, 40968652.0, 40968656.0, 40968660.0, 40968664.0, 40968668.0, 40968670.0, 40968676.0, 40968680.0, 40968684.0, 40968690.0, 40968692.0, 40968696.0, 40968700.0, 40968704.0, 40968708.0, 40968710.0, 40968716.0, 40968720.0, 40968724.0, 40968730.0, 40968732.0, 40968736.0, 40968740.0, 40968744.0, 40968748.0, 40968750.0, 40968756.0, 40968760.0, 40968764.0, 40968770.0, 40968772.0, 40968776.0, 40968784.0, 40968788.0, 40968892.0, 40968904.0, 40968910.0, 40968932.0, 40969784.0, 40969790.0, 40969800.0, 40969804.0, 40969810.0, 40969812.0, 40969816.0, 40969820.0, 40969824.0, 40969828.0, 40969830.0, 40969836.0, 40969840.0, 40969844.0, 40969850.0, 40969852.0, 40969856.0, 40969860.0, 40969864.0, 40969868.0, 40969870.0, 40969876.0, 40969880.0, 40969884.0, 40969890.0, 40969892.0, 40969896.0, 40969900.0, 40969910.0, 40969916.0, 40969920.0, 40970616.0, 40970620.0, 40970624.0, 40970628.0, 40970630.0, 40970636.0, 40970640.0, 40970644.0, 40970650.0, 40970652.0, 40970656.0, 40970660.0, 40970664.0, 40970668.0, 40970670.0, 40970676.0, 40970680.0, 40970684.0, 40970690.0, 40970692.0, 40970696.0, 40970704.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8045351473922903\n",
      "Hamming Loss: 0.10192743764172335\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.92      3723\n",
      "           1       0.00      0.00      0.00       687\n",
      "\n",
      "    accuracy                           0.84      4410\n",
      "   macro avg       0.42      0.50      0.46      4410\n",
      "weighted avg       0.71      0.84      0.77      4410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(1)\n",
    "  #]) \n",
    "    \n",
    "  #inputs = tf.keras.layers.Input(shape=(X_train.shape))\n",
    "  #outputs = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "  #model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "\n",
    "  #outputs = keras.layers.Dense(tf.keras.layers.Dense(number_of_classes)) #(x) \n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(2)\n",
    "  #])   \n",
    "  \n",
    "  #model = tf.contrib.learn.DNNClassifier(hidden_units=[5,10,5],\n",
    "  #                                          n_classes=4)    \n",
    "    \n",
    "\n",
    "  inputs = tf.keras.Input(shape=(X_train.shape))\n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "  outputs = tf.keras.layers.Dense(number_of_classes)  #, activation=activation)\n",
    "                             #kernel_initializer=initializer,\n",
    "                             #activation=activation) #(x) \n",
    "  #model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ]) \n",
    "    \n",
    "  #model.add(layers.Activation(activations.relu))\n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.001))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  #print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "    \n",
    "  #print(multilabel_confusion_matrix(y_test, y_pred2))\n",
    "  #cm = multilabel_confusion_matrix(y_test, y_pred2)\n",
    "  #print(cm)\n",
    "  #print(classification_report(y_test,y_pred2))\n",
    "\n",
    "df1 = pd.DataFrame(y_test)     \n",
    "filepath1 = 'output_y_test.csv'     \n",
    "df1.to_csv(filepath1)\n",
    "\n",
    "df2 = pd.DataFrame(y_pred2)\n",
    "filepath2 = 'output_y_pred.csv'\n",
    "df2.to_csv(filepath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107235, 15)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "folder = \"C:/Users/mdjaw/OneDrive/Documents/Markian Training/Research_USQ/USCenterDiseaseControlPrevention/LLCP2018XPT\" \n",
    "os.chdir(folder)\n",
    "\n",
    "BRFSS_Grounded = pd.read_csv(\"LLCP2018XPT_Grounded_KGFS10.csv\")\n",
    "\n",
    "BRFSS_Grounded = BRFSS_Grounded.dropna() \n",
    "\n",
    "#print(BRFSS_Grounded.shape)\n",
    "\n",
    "BRFSS_Grounded_X = BRFSS_Grounded.drop(['CNCRTYP1', 'DIABETE3'], axis = 1)\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "print(BRFSS_Grounded_X.shape)\n",
    "\n",
    "#X = BRFSS_Grounded_X\n",
    "X = np.array(BRFSS_Grounded_X)\n",
    "y = BRFSS_Grounded[['CNCRTYP1', 'DIABETE3']].to_numpy()\n",
    "\n",
    "#y = BRFSS_Grounded[['DIABETE4']].to_numpy()\n",
    "\n",
    "#X_train, Y_train = X[:175000], y[:175000]\n",
    "#X_test = X[175000:]\n",
    "#y_test = y[175000:]\n",
    "\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 8s 96us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 10s 119us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 9s 108us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 9s 103us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 10s 119us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 10s 120us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 12s 142us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 12s 134us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 12s 142us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 12s 142us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 11s 129us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 12s 144us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 14s 163us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 14s 158us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 12s 142us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 1.4249 - binary_accuracy: 0.9076 - loss: 1.4297 - bina\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 14s 161us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 55/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 56/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 57/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 58/100\n",
      "85788/85788 [==============================] - 9s 109us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 9s 109us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 9s 107us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 10s 119us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 14s 164us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 65/100\n",
      "85788/85788 [==============================] - 14s 166us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 11s 130us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 11s 128us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 15s 177us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 75/100\n",
      "85788/85788 [==============================] - 14s 165us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 14s 161us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 14s 163us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 14s 165us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 14s 163us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 14s 162us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 14s 162us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 14s 163us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 15s 171us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 14s 165us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 14s 159us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 14s 160us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 14s 160us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 14s 167us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 15s 172us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 14s 159us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 13s 148us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 11s 132us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 12s 143us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 11s 124us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 11s 133us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 14s 169us/sample - loss: 1.4249 - binary_accuracy: 0.9076\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 14s 166us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 12s 135us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - ETA: 0s - loss: 1.3944 - binary_accuracy: 0.909 - 12s 138us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 9s 104us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 9s 102us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 9s 104us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 14s 168us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 14s 159us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 14s 161us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 13s 151us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 13s 149us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 11s 124us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 11s 131us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 11s 133us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 15s 176us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 15s 178us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 10s 119us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 9s 109us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 10s 120us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 10s 120us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 14s 160us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 13s 155us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 11s 127us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 11s 125us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 11s 126us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 11s 125us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 1.3946 - binary_accuracy: 0.9096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-38891040.0, -38891050.0, -38891052.0, -38891056.0, -38891060.0, -38891064.0, -38891068.0, -38891070.0, -38891076.0, -38891080.0, -38891084.0, -38891090.0, -38891092.0, -38891096.0, -38891100.0, -38891104.0, -38891108.0, -38891110.0, -38891116.0, -38891120.0, -38891124.0, -38891130.0, -38891132.0, -38891136.0, -38891140.0, -38891144.0, -38891148.0, -38891150.0, -38891156.0, -38891160.0, -38891164.0, -38891170.0, -38891172.0, -38891176.0, -38891180.0, -38891184.0, -38891188.0, -38891190.0, -38891196.0, -38891200.0, -38891204.0, -38891210.0, -38891212.0, -38891216.0, -38891220.0, -38891224.0, -38891228.0, -38891230.0, -38891236.0, -38891240.0, -38891244.0, -38891250.0, -38891252.0, -38891256.0, -38891260.0, -38891264.0, -38891268.0, -38891270.0, -38891276.0, -38891280.0, -38891284.0, -38891290.0, -38891292.0, -38891296.0, -38891300.0, -38891304.0, -38891308.0, -38891310.0, -38891316.0, -38891320.0, -38891324.0, -38891330.0, -38891332.0, -38891336.0, -38891340.0, -38891344.0, -38891348.0, -38891350.0, -38891356.0, -38891360.0, -38891364.0, -38891370.0, -38891372.0, -38891376.0, -38891380.0, -38891384.0, -38891388.0, -38891390.0, -38891396.0, -38891400.0, -38891404.0, -38891410.0, -38891412.0, -38891416.0, -38891420.0, -38891424.0, -38891428.0, -38891430.0, -38891436.0, -38891440.0, -38891444.0, -38891450.0, -38891452.0, -38891456.0, -38891460.0, -38891464.0, -38891468.0, -38891470.0, -38891476.0, -38891480.0, -38891484.0, -38891490.0, -38891492.0, -38891496.0, -38891500.0, -38891504.0, -38891508.0, -38891510.0, -38891516.0, -38891520.0, -38891524.0, -38891530.0, -38891532.0, -38891536.0, -38891540.0, -38891544.0, -38891548.0, -38891550.0, -38891556.0, -38891560.0, -38891564.0, -38891570.0, -38891572.0, -38891576.0, -38891580.0, -38891584.0, -38891588.0, -38891590.0, -38891596.0, -38891600.0, -38891604.0, -38891610.0, -38891612.0, -38891616.0, -38891620.0, -38891624.0, -38891628.0, -38891630.0, -38891636.0, -38891640.0, -38891644.0, -38891650.0, -38891652.0, -38891656.0, -38891660.0, -38891664.0, -38891668.0, -38891670.0, -38891676.0, -38891680.0, -38891684.0, -38891690.0, -38891692.0, -38891696.0, -38891700.0, -38891704.0, -38891708.0, -38891710.0, -38891716.0, -38891720.0, -38891724.0, -38891730.0, -38891732.0, -38891736.0, -38891740.0, -38891744.0, -38891748.0, -38891750.0, -38891756.0, -38891760.0, -38891764.0, -38891770.0, -38891772.0, -38891776.0, -38891780.0, -38891784.0, -38891788.0, -38891790.0, -38891796.0, -38891800.0, -38891804.0, -38891810.0, -38891812.0, -38891816.0, -38891820.0, -38891824.0, -38891828.0, -38891830.0, -38891836.0, -38891840.0, -38891844.0, -38891850.0, -38891852.0, -38891856.0, -38891860.0, -38891864.0, -38891868.0, -38891870.0, -38891876.0, -38891880.0, -38891884.0, -38891890.0, -38891892.0, -38891896.0, -38891900.0, -38891904.0, -38891908.0, -38891910.0, -38891916.0, -38891920.0, -38891924.0, -38891930.0, -38891932.0, -38891936.0, -38891940.0, -38891944.0, -38891948.0, -38891950.0, -38891956.0, -38891960.0, -38891964.0, -38891970.0, -38891972.0, -38891976.0, -38891980.0, -38891984.0, -38891988.0, -38891990.0, -38891996.0, -38892000.0, -38892004.0, -38892010.0, -38892012.0, -38892016.0, -38892020.0, -38892024.0, -38892028.0, -38892030.0, -38892036.0, -38892040.0, -38892044.0, -38892050.0, -38892052.0, -38892056.0, -38892060.0, -38892064.0, -38892068.0, -38892070.0, -38892076.0, -38892084.0, -38892108.0, -38892116.0, -38892120.0, -38892450.0, -38892452.0, -38892456.0, -38892460.0, -38892464.0, -38892468.0, -38892470.0, -38892476.0, -38892480.0, -38892484.0, -38892490.0, -38892492.0, -38892496.0, -38892500.0, -38892504.0, -38892508.0, -38892510.0, -38892516.0, -38892520.0, -38892524.0, -38892530.0, -38892532.0, -38892536.0, -38892540.0, -38892544.0, -38892548.0, -38892550.0, -38892556.0, -38892560.0, -38892564.0, -38892570.0, -38892572.0, -38892576.0, -38892580.0, -38892584.0, -38892588.0, -38892590.0, -38892596.0, -38892600.0, -38892604.0, -38892610.0, -38892612.0, -38892616.0, -38892620.0, -38892624.0, -38892628.0, -38892630.0, -38892636.0, -38892640.0, -38892644.0, -38892650.0, -38892652.0, -38892656.0, -38892660.0, -38892664.0, -38892668.0, -38892670.0, -38892676.0, -38892680.0, -38892684.0, -38892690.0, -38892692.0, -38892696.0, -38892700.0, -38892704.0, -38892708.0, -38892710.0, -38892716.0, -38892720.0, -38892724.0, -38892730.0, -38892732.0, -38892736.0, -38892740.0, -38892744.0, -38892748.0, -38892750.0, -38892756.0, -38892760.0, -38892764.0, -38892770.0, -38892772.0, -38892776.0, -38892780.0, -38892784.0, -38892788.0, -38892790.0, -38892796.0, -38892800.0, -38892804.0, -38892810.0, -38892812.0, -38892816.0, -38892820.0, -38892824.0, -38892828.0, -38892830.0, -38892836.0, -38892840.0, -38892850.0, -38892852.0, -38892856.0, -38892860.0, -38892864.0, -38892868.0, -38892870.0, -38892876.0, -38892880.0, -38892884.0, -38892890.0, -38892892.0, -38892896.0, -38892900.0, -38892904.0, -38892908.0, -38892910.0, -38892916.0, -38892920.0, -38892924.0, -38892930.0, -38892932.0, -38892936.0, -38892940.0, -38892944.0, -38892948.0, -38892950.0, -38892956.0, -38892960.0, -38892964.0, -38892970.0, -38892972.0, -38892976.0, -38892980.0, -38892984.0, -38892988.0, -38892990.0, -38892996.0, -38893000.0, -38893004.0, -38893010.0, -38893012.0, -38893016.0, -38893020.0, -38893024.0, -38893028.0, -38893030.0, -38893036.0, -38893040.0, -38893044.0, -38893050.0, -38893052.0, -38893056.0, -38893060.0, -38893064.0, -38893068.0, -38893070.0, -38893076.0, -38893080.0, -38893084.0, -38893090.0, -38893092.0, -38893096.0, -38893100.0, -38893104.0, -38893108.0, -38893110.0, -38893116.0, -38893120.0, -38893124.0, -38893130.0, -38893132.0, -38893136.0, -38893140.0, -38893144.0, -38893148.0, -38893150.0, -38893156.0, -38893160.0, -38893164.0, -38893170.0, -38893172.0, -38893176.0, -38893180.0, -38893184.0, -38893188.0, -38893190.0, -38893196.0, -38893200.0, -38893204.0, -38893210.0, -38893212.0, -38893216.0, -38893220.0, -38893224.0, -38893228.0, -38893230.0, -38893236.0, -38893240.0, -38893244.0, -38893250.0, -38893252.0, -38893256.0, -38893260.0, -38893264.0, -38893268.0, -38893270.0, -38893276.0, -38893280.0, -38893284.0, -38893290.0, -38893292.0, -38893296.0, -38893300.0, -38893304.0, -38893308.0, -38893310.0, -38893316.0, -38893320.0, -38893324.0, -38893330.0, -38893332.0, -38893336.0, -38893340.0, -38893344.0, -38893348.0, -38893350.0, -38893356.0, -38893360.0, -38893364.0, -38893370.0, -38893372.0, -38893376.0, -38893380.0, -38893384.0, -38893388.0, -38893390.0, -38893396.0, -38893400.0, -38893404.0, -38893410.0, -38893412.0, -38893416.0, -38893420.0, -38893424.0, -38893428.0, -38893430.0, -38893436.0, -38893440.0, -38893444.0, -38893450.0, -38893452.0, -38893456.0, -38893460.0, -38893464.0, -38893468.0, -38893470.0, -38893476.0, -38893480.0, -38893484.0, -38893490.0, -38893492.0, -38893496.0, -38893500.0, -38893504.0, -38893508.0, -38893510.0, -38893516.0, -38893520.0, -38893524.0, -38893530.0, -38893532.0, -38893536.0, -38893540.0, -38893544.0, -38893548.0, -38893550.0, -38893556.0, -38893560.0, -38893564.0, -38893570.0, -38893572.0, -38893576.0, -38893580.0, -38893584.0, -38893588.0, -38893590.0, -38893596.0, -38893600.0, -38893604.0, -38893610.0, -38893612.0, -38893616.0, -38893620.0, -38893624.0, -38893628.0, -38893630.0, -38893636.0, -38893640.0, -38893644.0, -38893650.0, -38893652.0, -38893656.0, -38893660.0, -38893664.0, -38893668.0, -38893670.0, -38893676.0, -38893680.0, -38893684.0, -38893690.0, -38893692.0, -38893696.0, -38893700.0, -38893704.0, -38893708.0, -38893710.0, -38893716.0, -38893720.0, -38893724.0, -38893730.0, -38893732.0, -38893736.0, -38893740.0, -38893744.0, -38893748.0, -38893750.0, -38893756.0, -38893760.0, -38893764.0, -38893770.0, -38893772.0, -38893776.0, -38893780.0, -38893784.0, -38893788.0, -38893790.0, -38893796.0, -38893800.0, -38893804.0, -38893810.0, -38893812.0, -38893816.0, -38893820.0, -38893824.0, -38893828.0, -38893830.0, -38893836.0, -38893840.0, -38893844.0, -38893850.0, -38893852.0, -38893856.0, -38893860.0, -38893864.0, -38893868.0, -38893870.0, -38893876.0, -38893880.0, -38893884.0, -38893890.0, -38893892.0, -38893896.0, -38893900.0, -38893904.0, -38893908.0, -38893910.0, -38893916.0, -38893920.0, -38893924.0, -38893930.0, -38893932.0, -38893936.0, -38893940.0, -38893944.0, -38893948.0, -38893950.0, -38893956.0, -38893960.0, -38893964.0, -38893970.0, -38893972.0, -38893976.0, -38893984.0, -38893990.0, -38893996.0, -38894004.0, -38894010.0, -38894016.0, -38895336.0, -38895344.0, -38895348.0, -38895350.0, -38895356.0, -38895360.0, -38895364.0, -38895370.0, -38895372.0, -38895376.0, -38895380.0, -38895384.0, -38895388.0, -38895390.0, -38895396.0, -38895400.0, -38895404.0, -38895410.0, -38895412.0, -38895416.0, -38895420.0, -38895424.0, -38895428.0, -38895430.0, -38895436.0, -38895440.0, -38895444.0, -38895450.0, -38895452.0, -38895456.0, -38895460.0, -38895464.0, -38895468.0, -38895470.0, -38895476.0, -38895480.0, -38895484.0, -38895490.0, -38895492.0, -38895496.0, -38895500.0, -38895504.0, -38895508.0, -38895510.0, -38895516.0, -38895520.0, -38895524.0, -38895530.0, -38895532.0, -38895536.0, -38895540.0, -38895544.0, -38895548.0, -38895550.0, -38895556.0, -38895560.0, -38895564.0, -38895570.0, -38895572.0, -38895576.0, -38895580.0, -38895584.0, -38895588.0, -38895590.0, -38895596.0, -38895600.0, -38895604.0, -38895610.0, -38895612.0, -38895616.0, -38895620.0, -38895624.0, -38895628.0, -38895630.0, -38895636.0, -38895640.0, -38895644.0, -38895650.0, -38895652.0, -38895656.0, -38895660.0, -38895664.0, -38895668.0, -38895670.0, -38895676.0, -38895680.0, -38895684.0, -38895690.0, -38895692.0, -38895696.0, -38895700.0, -38895704.0, -38895708.0, -38895710.0, -38895716.0, -38895720.0, -38895724.0, -38895730.0, -38895732.0, -38895736.0, -38895740.0, -38895744.0, -38895748.0, -38895750.0, -38895756.0, -38895760.0, -38895764.0, -38895770.0, -38895772.0, -38895776.0, -38895780.0, -38895784.0, -38895788.0, -38895790.0, -38895796.0, -38895800.0, -38895804.0, -38895810.0, -38895812.0, -38895816.0, -38895820.0, -38895824.0, -38895828.0, -38895830.0, -38895836.0, -38895840.0, -38895844.0, -38895850.0, -38895852.0, -38895856.0, -38895860.0, -38895864.0, -38895868.0, -38895870.0, -38895876.0, -38895880.0, -38895884.0, -38895890.0, -38895892.0, -38895896.0, -38895900.0, -38895904.0, -38895908.0, -38895910.0, -38895916.0, -38895924.0, -38895932.0, -38898104.0, -38898108.0, -38898110.0, -38898116.0, -38898120.0, -38898124.0, -38898130.0, -38898132.0, -38898136.0, -38898140.0, -38898144.0, -38898148.0, -38898150.0, -38898156.0, -38898160.0, -38898164.0, -38898170.0, -38898172.0, -38898176.0, -38898180.0, -38898184.0, -38898188.0, -38898190.0, -38898196.0, -38898200.0, -38898204.0, -38898210.0, -38898212.0, -38898216.0, -38898220.0, -38898224.0, -38898228.0, -38898230.0, -38898236.0, -38898240.0, -38898244.0, -38898250.0, -38898252.0, -38898256.0, -38898260.0, -38898264.0, -38898268.0, -38898270.0, -38898276.0, -38898280.0, -38898284.0, -38898290.0, -38898292.0, -38898296.0, -38898300.0, -38898304.0, -38898308.0, -38898310.0, -38898316.0, -38898320.0, -38898324.0, -38898330.0, -38898332.0, -38898336.0, -38898340.0, -38898348.0, -38898350.0, -38898356.0, -38898360.0, -38898364.0, -38898370.0, -38898372.0, -38898376.0, -38898380.0, -38898384.0, -38898388.0, -38898390.0, -38898396.0, -38898400.0, -38898404.0, -38898410.0, -38898412.0, -38898416.0, -38898420.0, -38898424.0, -38898428.0, -38898430.0, -38898436.0, -38898440.0, -38898444.0, -38898450.0, -38898452.0, -38898456.0, -38898460.0, -38898464.0, -38898468.0, -38898470.0, -38898476.0, -38898480.0, -38898484.0, -38898490.0, -38898492.0, -38898496.0, -38898500.0, -38898504.0, -38898508.0, -38898510.0, -38898516.0, -38898520.0, -38898524.0, -38898530.0, -38898532.0, -38898536.0, -38898540.0, -38898544.0, -38898548.0, -38898550.0, -38898556.0, -38898560.0, -38898564.0, -38898570.0, -38898572.0, -38898576.0, -38898580.0, -38898584.0, -38898588.0, -38898590.0, -38898596.0, -38898600.0, -38898604.0, -38898610.0, -38898612.0, -38898616.0, -38898620.0, -38898624.0, -38898628.0, -38898630.0, -38898636.0, -38898640.0, -38898644.0, -38898650.0, -38898652.0, -38898656.0, -38898660.0, -38898664.0, -38898668.0, -38898670.0, -38898676.0, -38898680.0, -38898684.0, -38898690.0, -38898692.0, -38898696.0, -38898700.0, -38898704.0, -38898708.0, -38898710.0, -38898716.0, -38898720.0, -38898724.0, -38898730.0, -38898732.0, -38898736.0, -38898740.0, -38898744.0, -38898748.0, -38898750.0, -38898756.0, -38898760.0, -38898764.0, -38898770.0, -38898772.0, -38898776.0, -38898780.0, -38898784.0, -38898788.0, -38898790.0, -38898796.0, -38898800.0, -38898804.0, -38898810.0, -38898816.0, 91910210.0, 91910216.0, 91910220.0, 91910230.0, 91910240.0, 91910250.0, 91910260.0, 91910264.0, 91910270.0, 91910280.0, 91910290.0, 91910296.0, 91910300.0, 91910310.0, 91910320.0, 91910330.0, 91910340.0, 91910344.0, 91910350.0, 91910360.0, 91910370.0, 91910376.0, 91910380.0, 91910390.0, 91910400.0, 91910410.0, 91910420.0, 91910424.0, 91910430.0, 91910440.0, 91910450.0, 91910456.0, 91910460.0, 91910470.0, 91910480.0, 91910490.0, 91910500.0, 91910504.0, 91910510.0, 91910520.0, 91910530.0, 91910536.0, 91910540.0, 91910550.0, 91910560.0, 91910570.0, 91910580.0, 91910584.0, 91910590.0, 91910600.0, 91910610.0, 91910616.0, 91910620.0, 91910630.0, 91910640.0, 91910650.0, 91910660.0, 91910664.0, 91910670.0, 91910680.0, 91910690.0, 91910696.0, 91910700.0, 91910710.0, 91910720.0, 91910730.0, 91910740.0, 91910744.0, 91910750.0, 91910760.0, 91910770.0, 91910776.0, 91910780.0, 91910790.0, 91910800.0, 91910810.0, 91910820.0, 91910824.0, 91910830.0, 91910840.0, 91910850.0, 91910856.0, 91910860.0, 91910870.0, 91910880.0, 91910890.0, 91910900.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8170373478808225\n",
      "Hamming Loss: 0.09635380239660558\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93     18537\n",
      "           1       0.00      0.00      0.00      2910\n",
      "\n",
      "    accuracy                           0.86     21447\n",
      "   macro avg       0.43      0.50      0.46     21447\n",
      "weighted avg       0.75      0.86      0.80     21447\n",
      "\n",
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 11s 127us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - ETA: 0s - loss: 14.0273 - binary_accuracy: 0.08 - 12s 136us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 11s 125us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 11s 132us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 16s 189us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 17/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 18/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 19/100\n",
      "85788/85788 [==============================] - 10s 121us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 20/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 21/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 11s 128us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 13s 150us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 11s 126us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 9s 109us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 10s 120us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 12s 135us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 10s 121us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 12s 141us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 11s 125us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 11s 124us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 12s 135us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 9s 107us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 9s 108us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 9s 107us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 65/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 11s 124us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 10s 119us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 14s 162us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 13s 156us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 12s 145us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 14s 160us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 12s 140us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 10s 116us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 12s 140us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 11s 124us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 10s 121us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 15s 176us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 11s 130us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 11s 130us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 11s 131us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 14.0277 - binary_accuracy: 0.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-19460870.0, -19460880.0, -19460882.0, -19460884.0, -19460886.0, -19460888.0, -19460892.0, -19460894.0, -19460896.0, -19460898.0, -19460900.0, -19460902.0, -19460904.0, -19460906.0, -19460908.0, -19460910.0, -19460912.0, -19460914.0, -19460916.0, -19460920.0, -19460922.0, -19460924.0, -19460926.0, -19460928.0, -19460930.0, -19460932.0, -19460934.0, -19460936.0, -19460938.0, -19460940.0, -19460942.0, -19460944.0, -19460946.0, -19460948.0, -19460952.0, -19460954.0, -19460956.0, -19460958.0, -19460960.0, -19461378.0, -19461380.0, -19461384.0, -19461388.0, -19461390.0, -19461392.0, -19461394.0, -19461396.0, -19461398.0, -19461400.0, -19461402.0, -19461404.0, -19461406.0, -19461408.0, -19461410.0, -19461412.0, -19461414.0, -19461416.0, -19461418.0, -19461420.0, -19461422.0, -19461424.0, -19461426.0, -19461428.0, -19461430.0, -19461432.0, -19461434.0, -19461436.0, -19461438.0, -19461440.0, -19461442.0, -19461444.0, -19461446.0, -19461448.0, -19461450.0, -19461452.0, -19461454.0, -19461456.0, -19461458.0, -19461460.0, -19461462.0, -19461464.0, -19461466.0, -19461468.0, -19461470.0, -19461472.0, -19461474.0, -19461476.0, -19461478.0, -19461480.0, -19461482.0, -19461484.0, -19461486.0, -19461488.0, -19461490.0, -19461492.0, -19461494.0, -19461496.0, -19461498.0, -19461500.0, -19461502.0, -19461504.0, -19461506.0, -19461508.0, -19461510.0, -19461512.0, -19461514.0, -19461516.0, -19461518.0, -19461520.0, -19461522.0, -19461524.0, -19461526.0, -19461528.0, -19461530.0, -19461532.0, -19461534.0, -19461536.0, -19461538.0, -19461540.0, -19461542.0, -19461544.0, -19461546.0, -19461548.0, -19461550.0, -19461552.0, -19461554.0, -19461556.0, -19461558.0, -19461560.0, -19461562.0, -19461564.0, -19461566.0, -19461568.0, -19461570.0, -19461572.0, -19461574.0, -19461576.0, -19461578.0, -19461580.0, -19461582.0, -19461584.0, -19461586.0, -19461588.0, -19461590.0, -19461592.0, -19461594.0, -19461596.0, -19461598.0, -19461600.0, -19461602.0, -19461604.0, -19461606.0, -19461608.0, -19461610.0, -19461612.0, -19461614.0, -19461616.0, -19461618.0, -19461620.0, -19461622.0, -19461624.0, -19461626.0, -19461628.0, -19461630.0, -19461632.0, -19461634.0, -19461636.0, -19461638.0, -19461640.0, -19461642.0, -19461644.0, -19461646.0, -19461648.0, -19461650.0, -19461652.0, -19461654.0, -19461656.0, -19461658.0, -19461660.0, -19461662.0, -19461664.0, -19461666.0, -19461668.0, -19461670.0, -19461672.0, -19461674.0, -19461676.0, -19461678.0, -19461680.0, -19461682.0, -19461684.0, -19461686.0, -19461688.0, -19461690.0, -19461692.0, -19461694.0, -19461696.0, -19461698.0, -19461700.0, -19461702.0, -19461704.0, -19461706.0, -19461708.0, -19461710.0, -19461712.0, -19461714.0, -19461716.0, -19461718.0, -19461720.0, -19461722.0, -19461724.0, -19461726.0, -19461728.0, -19461730.0, -19461732.0, -19461734.0, -19461736.0, -19461738.0, -19461740.0, -19461742.0, -19461744.0, -19461746.0, -19461748.0, -19461750.0, -19461752.0, -19461754.0, -19461756.0, -19461758.0, -19461760.0, -19461762.0, -19461764.0, -19461766.0, -19461768.0, -19461770.0, -19461772.0, -19461774.0, -19461776.0, -19461778.0, -19461780.0, -19461782.0, -19461784.0, -19461786.0, -19461788.0, -19461790.0, -19461792.0, -19461794.0, -19461796.0, -19461798.0, -19461800.0, -19461802.0, -19461804.0, -19461806.0, -19461808.0, -19461810.0, -19461812.0, -19461814.0, -19461816.0, -19461818.0, -19461820.0, -19461822.0, -19461824.0, -19461826.0, -19461828.0, -19461830.0, -19461832.0, -19461834.0, -19461836.0, -19461838.0, -19461840.0, -19461842.0, -19461844.0, -19461846.0, -19461848.0, -19461850.0, -19461852.0, -19461854.0, -19461856.0, -19461858.0, -19461860.0, -19461862.0, -19461864.0, -19461866.0, -19461868.0, -19461870.0, -19461872.0, -19461874.0, -19461876.0, -19461878.0, -19461880.0, -19461882.0, -19461884.0, -19461886.0, -19461888.0, -19461890.0, -19461892.0, -19461894.0, -19461896.0, -19461898.0, -19461900.0, -19461902.0, -19461904.0, -19461906.0, -19461908.0, -19461910.0, -19461912.0, -19461914.0, -19461916.0, -19461918.0, -19461920.0, -19461922.0, -19461924.0, -19461926.0, -19461928.0, -19461930.0, -19461932.0, -19461934.0, -19461936.0, -19461938.0, -19461940.0, -19461942.0, -19461944.0, -19461946.0, -19461948.0, -19461950.0, -19461952.0, -19461954.0, -19461956.0, -19461958.0, -19461960.0, -19461962.0, -19461964.0, -19461966.0, -19461968.0, -19461970.0, -19461972.0, -19461974.0, -19461976.0, -19461978.0, -19461980.0, -19461982.0, -19461984.0, -19461986.0, -19461988.0, -19461990.0, -19461992.0, -19461994.0, -19461996.0, -19461998.0, -19462000.0, -19462002.0, -19462004.0, -19462006.0, -19462008.0, -19462010.0, -19462012.0, -19462014.0, -19462016.0, -19462018.0, -19462020.0, -19462022.0, -19462024.0, -19462026.0, -19462028.0, -19462032.0, -19462040.0, -19462042.0, -19462044.0, -19462048.0, -19462052.0, -19462054.0, -19462056.0, -19462058.0, -19462060.0, -19462064.0, -19462066.0, -19462068.0, -19462070.0, -19462072.0, -19462074.0, -19462076.0, -19462078.0, -19462080.0, -19462082.0, -19462084.0, -19462086.0, -19462088.0, -19462090.0, -19462092.0, -19462094.0, -19462096.0, -19462098.0, -19462100.0, -19462102.0, -19462104.0, -19462106.0, -19462108.0, -19462110.0, -19462112.0, -19462114.0, -19462116.0, -19462118.0, -19462120.0, -19462122.0, -19462124.0, -19462126.0, -19462128.0, -19462130.0, -19462132.0, -19462134.0, -19462136.0, -19462138.0, -19462140.0, -19462142.0, -19462144.0, -19462146.0, -19462148.0, -19462150.0, -19462152.0, -19462154.0, -19462156.0, -19462158.0, -19462160.0, -19462162.0, -19462164.0, -19462166.0, -19462168.0, -19462170.0, -19462172.0, -19462174.0, -19462176.0, -19462178.0, -19462180.0, -19462182.0, -19462184.0, -19462186.0, -19462188.0, -19462190.0, -19462192.0, -19462194.0, -19462196.0, -19462198.0, -19462200.0, -19462202.0, -19462204.0, -19462206.0, -19462208.0, -19462210.0, -19462212.0, -19462214.0, -19462216.0, -19462218.0, -19462220.0, -19462222.0, -19462224.0, -19462226.0, -19462228.0, -19462230.0, -19462232.0, -19462234.0, -19462236.0, -19462238.0, -19462240.0, -19462242.0, -19462244.0, -19462246.0, -19462248.0, -19462250.0, -19462252.0, -19462254.0, -19462256.0, -19462258.0, -19462260.0, -19462262.0, -19462264.0, -19462266.0, -19462268.0, -19462272.0, -19462274.0, -19462276.0, -19462278.0, -19462280.0, -19462282.0, -19462284.0, -19462288.0, -19462290.0, -19462292.0, -19462294.0, -19462296.0, -19462300.0, -19462302.0, -19462304.0, -19462308.0, -19462310.0, -19462312.0, -19462314.0, -19462316.0, -19462320.0, -19462322.0, -19462324.0, -19462326.0, -19462328.0, -19462330.0, -19462332.0, -19462334.0, -19462336.0, -19462338.0, -19462340.0, -19462342.0, -19462344.0, -19462346.0, -19462348.0, -19462350.0, -19462352.0, -19462354.0, -19462356.0, -19462358.0, -19462360.0, -19462362.0, -19462364.0, -19462366.0, -19462368.0, -19462370.0, -19462372.0, -19462374.0, -19462376.0, -19462378.0, -19462380.0, -19462382.0, -19462384.0, -19462386.0, -19462388.0, -19462390.0, -19462392.0, -19462394.0, -19462396.0, -19462398.0, -19462400.0, -19462402.0, -19462404.0, -19462406.0, -19462408.0, -19462410.0, -19462412.0, -19462414.0, -19462416.0, -19462418.0, -19462420.0, -19462422.0, -19462424.0, -19462426.0, -19462428.0, -19462430.0, -19462432.0, -19462434.0, -19462436.0, -19462438.0, -19462440.0, -19462442.0, -19462444.0, -19462446.0, -19462448.0, -19462450.0, -19462452.0, -19462454.0, -19462456.0, -19462458.0, -19462460.0, -19462462.0, -19462464.0, -19462466.0, -19462468.0, -19462470.0, -19462472.0, -19462474.0, -19462476.0, -19462478.0, -19462480.0, -19462482.0, -19462484.0, -19462486.0, -19462488.0, -19462490.0, -19462492.0, -19462494.0, -19462496.0, -19462498.0, -19462500.0, -19462502.0, -19462504.0, -19462506.0, -19462508.0, -19462526.0, -19463164.0, -19463168.0, -19463180.0, -19463182.0, -19463184.0, -19463186.0, -19463188.0, -19463190.0, -19463192.0, -19463194.0, -19463196.0, -19463198.0, -19463200.0, -19463202.0, -19463204.0, -19463206.0, -19463208.0, -19463210.0, -19463212.0, -19463214.0, -19463216.0, -19463218.0, -19463220.0, -19463222.0, -19463224.0, -19463226.0, -19463228.0, -19463230.0, -19463232.0, -19463234.0, -19463236.0, -19463238.0, -19463240.0, -19463242.0, -19463244.0, -19463246.0, -19463248.0, -19463250.0, -19463252.0, -19463254.0, -19463256.0, -19463258.0, -19463260.0, -19463262.0, -19463264.0, -19463266.0, -19463268.0, -19463270.0, -19463272.0, -19463274.0, -19463276.0, -19463278.0, -19463280.0, -19463282.0, -19463284.0, -19463286.0, -19463288.0, -19463290.0, -19463292.0, -19463294.0, -19463296.0, -19463298.0, -19463300.0, -19463302.0, -19463304.0, -19463306.0, -19463308.0, -19463310.0, -19463312.0, -19463314.0, -19463316.0, -19463318.0, -19463320.0, -19463322.0, -19463324.0, -19463326.0, -19463328.0, -19463330.0, -19463332.0, -19463334.0, -19463336.0, -19463338.0, -19463340.0, -19463342.0, -19463344.0, -19463346.0, -19463348.0, -19463350.0, -19463352.0, -19463354.0, -19463356.0, -19463358.0, -19463360.0, -19463362.0, -19463364.0, -19463366.0, -19463368.0, -19463370.0, -19463372.0, -19463374.0, -19463376.0, -19463378.0, -19463380.0, -19463382.0, -19463384.0, -19463386.0, -19463388.0, -19463390.0, -19463392.0, -19463394.0, -19463396.0, -19463398.0, -19463400.0, -19463402.0, -19463404.0, -19463406.0, -19463408.0, -19463410.0, -19463412.0, -19463414.0, -19463416.0, -19463418.0, -19463420.0, -19463422.0, -19463424.0, -19463426.0, -19463428.0, -19463430.0, -19463432.0, -19463434.0, -19463436.0, -19463438.0, -19463440.0, -19463442.0, -19463444.0, -19463446.0, -19463448.0, -19463450.0, -19463452.0, -19463454.0, -19463456.0, -19463458.0, -19463460.0, -19463462.0, -19463464.0, -19463466.0, -19463468.0, -19463470.0, -19463472.0, -19463474.0, -19463476.0, -19463478.0, -19463480.0, -19463482.0, -19463484.0, -19463486.0, -19463488.0, -19463490.0, -19463492.0, -19463494.0, -19463496.0, -19463498.0, -19463500.0, -19463504.0, -19463506.0, -19463508.0, -19463510.0, -19463512.0, -19463514.0, -19463516.0, -19463520.0, 17134528.0, 17134532.0, 17134536.0, 17134540.0, 17134544.0, 17134548.0, 17134552.0, 17134556.0, 17134560.0, 17134564.0, 17134568.0, 17134572.0, 17134576.0, 17134580.0, 17134584.0, 17134588.0, 17134592.0, 17134600.0, 17134920.0, 17134944.0, 17134952.0, 17134956.0, 17134960.0, 17134964.0, 17134968.0, 17134972.0, 17134976.0, 17134980.0, 17134984.0, 17134988.0, 17134992.0, 17134996.0, 17135000.0, 17135004.0, 17135008.0, 17135012.0, 17135016.0, 17135020.0, 17135024.0, 17135028.0, 17135032.0, 17135036.0, 17135040.0, 17135044.0, 17135048.0, 17135052.0, 17135056.0, 17135060.0, 17135064.0, 17135068.0, 17135072.0, 17135076.0, 17135080.0, 17135084.0, 17135088.0, 17135092.0, 17135096.0, 17135100.0, 17135104.0, 17135108.0, 17135112.0, 17135116.0, 17135120.0, 17135124.0, 17135128.0, 17135132.0, 17135136.0, 17135140.0, 17135144.0, 17135148.0, 17135152.0, 17135156.0, 17135160.0, 17135164.0, 17135168.0, 17135172.0, 17135176.0, 17135180.0, 17135184.0, 17135188.0, 17135192.0, 17135196.0, 17135200.0, 17135204.0, 17135208.0, 17135212.0, 17135216.0, 17135220.0, 17135224.0, 17135228.0, 17135232.0, 17135236.0, 17135240.0, 17135244.0, 17135248.0, 17135252.0, 17135256.0, 17135260.0, 17135264.0, 17135268.0, 17135272.0, 17135276.0, 17135280.0, 17135284.0, 17135288.0, 17135292.0, 17135296.0, 17135300.0, 17135304.0, 17135308.0, 17135312.0, 17135316.0, 17135320.0, 17135324.0, 17135328.0, 17135332.0, 17135336.0, 17135340.0, 17135344.0, 17135348.0, 17135352.0, 17135356.0, 17135360.0, 17135364.0, 17135368.0, 17135372.0, 17135376.0, 17135380.0, 17135384.0, 17135388.0, 17135392.0, 17135396.0, 17135400.0, 17135404.0, 17135408.0, 17135412.0, 17135416.0, 17135420.0, 17135424.0, 17135428.0, 17135432.0, 17135436.0, 17135440.0, 17135444.0, 17135448.0, 17135452.0, 17135456.0, 17135460.0, 17135464.0, 17135468.0, 17135472.0, 17135476.0, 17135480.0, 17135484.0, 17135488.0, 17135492.0, 17135496.0, 17135500.0, 17135504.0, 17135508.0, 17135512.0, 17135516.0, 17135520.0, 17135524.0, 17135528.0, 17135532.0, 17135536.0, 17135540.0, 17135544.0, 17135548.0, 17135552.0, 17135556.0, 17135560.0, 17135564.0, 17135568.0, 17135572.0, 17135576.0, 17135580.0, 17135584.0, 17135588.0, 17135592.0, 17135596.0, 17135600.0, 17135604.0, 17135608.0, 17135612.0, 17135616.0, 17135620.0, 17135624.0, 17135628.0, 17135632.0, 17135636.0, 17135640.0, 17135644.0, 17135648.0, 17135652.0, 17135656.0, 17135660.0, 17135664.0, 17135668.0, 17135672.0, 17135676.0, 17135680.0, 17135684.0, 17135688.0, 17135692.0, 17135696.0, 17135700.0, 17135704.0, 17135708.0, 17135712.0, 17135716.0, 17135720.0, 17135724.0, 17135728.0, 17135732.0, 17135736.0, 17135740.0, 17135744.0, 17135748.0, 17135752.0, 17135756.0, 17135760.0, 17135764.0, 17136220.0, 17136224.0, 17136228.0, 17136232.0, 17136236.0, 17136240.0, 17136244.0, 17136248.0, 17136252.0, 17136256.0, 17136260.0, 17136264.0, 17136268.0, 17136272.0, 17136276.0, 17136280.0, 17136284.0, 17136288.0, 17136292.0, 17136296.0, 17136300.0, 17136304.0, 17136308.0, 17136312.0, 17136316.0, 17136320.0, 17136324.0, 17136328.0, 17136332.0, 17136336.0, 17136340.0, 17136344.0, 17136348.0, 17136352.0, 17136356.0, 17136360.0, 17136364.0, 17136368.0, 17136372.0, 17136376.0, 17136380.0, 17136384.0, 17136388.0, 17136392.0, 17136396.0, 17136400.0, 17136404.0, 17136408.0, 17136412.0, 17136416.0, 17136420.0, 17136424.0, 17136428.0, 17136432.0, 17136436.0, 17136440.0, 17136444.0, 17136448.0, 17136452.0, 17136456.0, 17136460.0, 17136464.0, 17136468.0, 17136472.0, 17136476.0, 17136480.0, 17136484.0, 17136488.0, 17136492.0, 17136496.0, 17136500.0, 17136504.0, 17136508.0, 17136512.0, 17136520.0, 17136524.0, 17136528.0, 17136532.0, 17136536.0, 17136556.0, 17136572.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8002517834662191\n",
      "Hamming Loss: 0.10560917610854664\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     18474\n",
      "           1       0.14      1.00      0.24      2973\n",
      "\n",
      "    accuracy                           0.14     21447\n",
      "   macro avg       0.07      0.50      0.12     21447\n",
      "weighted avg       0.02      0.14      0.03     21447\n",
      "\n",
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 10s 121us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 11s 127us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 11s 130us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 11s 131us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 11s 133us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 12s 141us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 12s 139us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 13s 152us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 11s 133us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 9s 108us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 16/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 17/100\n",
      "85788/85788 [==============================] - 12s 135us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 18/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 19/100\n",
      "85788/85788 [==============================] - 10s 120us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 20/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 21/100\n",
      "85788/85788 [==============================] - 11s 122us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 11s 127us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 11s 129us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 11s 124us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 11s 128us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 9s 109us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 11s 133us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 9s 101us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 9s 106us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 9s 108us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 10s 122us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 11s 125us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 13s 153us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 11s 132us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 12s 141us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 12s 135us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 54/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 55/100\n",
      "85788/85788 [==============================] - 12s 137us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 56/100\n",
      "85788/85788 [==============================] - 11s 132us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 57/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 58/100\n",
      "85788/85788 [==============================] - 10s 121us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 59/100\n",
      "85788/85788 [==============================] - 10s 119us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 60/100\n",
      "85788/85788 [==============================] - 11s 129us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 61/100\n",
      "85788/85788 [==============================] - 11s 126us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 62/100\n",
      "85788/85788 [==============================] - 12s 141us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 63/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 64/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 65/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 66/100\n",
      "85788/85788 [==============================] - 11s 128us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 11s 129us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 11s 127us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 10s 122us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 72/100\n",
      "85788/85788 [==============================] - 10s 120us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 9s 102us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 9s 101us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 10s 112us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 10s 120us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 11s 123us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 12s 135us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 13s 157us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 12s 142us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 10s 122us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 84/100\n",
      "85788/85788 [==============================] - 13s 146us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 9s 109us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 10s 117us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 13s 147us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 11s 126us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 11s 124us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 12s 134us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 11s 128us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 1.4016 - binary_accuracy: 0.9091\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 26s 304us/sample - loss: 1.4016 - binary_accuracy: 0.9091 - loss: 1.4036 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-202277090.0, -202277100.0, -202277120.0, -202277140.0, -202277150.0, -202277170.0, -202277180.0, -202277200.0, -202277220.0, -202277230.0, -202277250.0, -202277260.0, -202277280.0, -202277300.0, -202277310.0, -202277330.0, -202277340.0, -202277360.0, -202277380.0, -202277390.0, -202277400.0, -202277420.0, -202277440.0, -202277460.0, -202277470.0, -202277490.0, -202277500.0, -202277520.0, -202277540.0, -202277550.0, -202277570.0, -202277580.0, -202277600.0, -202277620.0, -202277630.0, -202277650.0, -202277660.0, -202277680.0, -202277700.0, -202277710.0, -202277730.0, -202277740.0, -202277760.0, -202277780.0, -202277800.0, -202277810.0, -202277820.0, -202277840.0, -202277860.0, -202277870.0, -202277890.0, -202277900.0, -202277920.0, -202277940.0, -202277950.0, -202277970.0, -202277980.0, -202278000.0, -202278020.0, -202278030.0, -202278050.0, -202278060.0, -202278080.0, -202278100.0, -202278110.0, -202278130.0, -202278140.0, -202278160.0, -202278180.0, -202278190.0, -202278200.0, -202278220.0, -202278240.0, -202278260.0, -202278270.0, -202279700.0, -202279710.0, -202279730.0, -202279740.0, -202279760.0, -202279780.0, -202279790.0, -202279800.0, -202279820.0, -202279840.0, -202279860.0, -202279870.0, -202279890.0, -202279900.0, -202279920.0, -202279940.0, -202279950.0, -202279970.0, -202279980.0, -202280000.0, -202280020.0, -202280030.0, -202280050.0, -202280060.0, -202280080.0, -202280100.0, -202280110.0, -202280130.0, -202280140.0, -202280160.0, -202280180.0, -202280200.0, -202280210.0, -202280220.0, -202280240.0, -202280260.0, -202280270.0, -202280290.0, -202280300.0, -202280320.0, -202280340.0, -202280350.0, -202280370.0, -202280380.0, -202280400.0, -202280420.0, -202280430.0, -202280450.0, -202280460.0, -202280480.0, -202280500.0, -202280510.0, -202280530.0, -202280540.0, -202280560.0, -202280580.0, -202280590.0, -202280600.0, -202280620.0, -202280640.0, -202280660.0, -202280670.0, -202280690.0, -202280700.0, -202280720.0, -202280740.0, -202280750.0, -202280770.0, -202280780.0, -202280800.0, -202280820.0, -202280830.0, -202280850.0, -202280860.0, -202280880.0, -202280900.0, -202280910.0, -202280930.0, -202280940.0, -202280960.0, -202280980.0, -202281000.0, -202281010.0, -202281020.0, -202281040.0, -202281060.0, -202281070.0, -202281090.0, -202281100.0, -202281120.0, -202281140.0, -202281150.0, -202281170.0, -202281180.0, -202281200.0, -202281220.0, -202281230.0, -202281250.0, -202281260.0, -202281280.0, -202281300.0, -202281310.0, -202281330.0, -202281340.0, -202281360.0, -202281380.0, -202281390.0, -202281400.0, -202281420.0, -202281440.0, -202281460.0, -202281470.0, -202281490.0, -202281500.0, -202281520.0, -202281540.0, -202281550.0, -202281570.0, -202281580.0, -202281600.0, -202281620.0, -202281630.0, -202281650.0, -202281660.0, -202281680.0, -202281700.0, -202281710.0, -202281730.0, -202281740.0, -202281760.0, -202281780.0, -202281800.0, -202281810.0, -202281820.0, -202281840.0, -202281860.0, -202281870.0, -202281890.0, -202281900.0, -202281920.0, -202281940.0, -202281950.0, -202281970.0, -202281980.0, -202282000.0, -202282020.0, -202282030.0, -202282050.0, -202282060.0, -202282080.0, -202282100.0, -202282110.0, -202282130.0, -202282140.0, -202282160.0, -202282180.0, -202282190.0, -202282200.0, -202282220.0, -202282240.0, -202282260.0, -202282270.0, -202282290.0, -202282300.0, -202282320.0, -202282340.0, -202282350.0, -202282370.0, -202282380.0, -202282400.0, -202282420.0, -202282430.0, -202282450.0, -202282460.0, -202282480.0, -202282500.0, -202282510.0, -202282530.0, -202282540.0, -202282560.0, -202282580.0, -202282600.0, -202282610.0, -202282620.0, -202282640.0, -202282660.0, -202282670.0, -202282690.0, -202282700.0, -202282720.0, -202282740.0, -202282750.0, -202282770.0, -202282780.0, -202282800.0, -202282820.0, -202282830.0, -202282850.0, -202282860.0, -202282880.0, -202282900.0, -202282910.0, -202282930.0, -202282940.0, -202282960.0, -202282980.0, -202282990.0, -202283000.0, -202283020.0, -202283040.0, -202283060.0, -202283070.0, -202283090.0, -202283100.0, -202283120.0, -202283140.0, -202283150.0, -202283170.0, -202283180.0, -202283200.0, -202283220.0, -202283230.0, -202283250.0, -202283260.0, -202283280.0, -202283300.0, -202283310.0, -202283330.0, -202283340.0, -202283360.0, -202283380.0, -202283400.0, -202283410.0, -202283420.0, 72742376.0, 72742380.0, 72742390.0, 72742400.0, 72742410.0, 72742420.0, 72742424.0, 72742430.0, 72742440.0, 72742450.0, 72742456.0, 72742460.0, 72742470.0, 72742480.0, 72742490.0, 72742500.0, 72742504.0, 72742510.0, 72742520.0, 72742530.0, 72742536.0, 72742540.0, 72742550.0, 72742560.0, 72742570.0, 72742580.0, 72742584.0, 72742590.0, 72742600.0, 72742610.0, 72742616.0, 72742620.0, 72742630.0, 72742640.0, 72742650.0, 72742660.0, 72742664.0, 72742670.0, 72742680.0, 72742690.0, 72742696.0, 72742700.0, 72742710.0, 72742720.0, 72742730.0, 72742740.0, 72742744.0, 72742750.0, 72742760.0, 72742770.0, 72742776.0, 72742780.0, 72742790.0, 72742800.0, 72742810.0, 72742820.0, 72742824.0, 72742830.0, 72742840.0, 72742850.0, 72742856.0, 72742860.0, 72742870.0, 72742880.0, 72742890.0, 72742900.0, 72742904.0, 72742910.0, 72742920.0, 72742930.0, 72742936.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8155452977106356\n",
      "Hamming Loss: 0.09453536625169022\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91     18036\n",
      "           1       0.00      0.00      0.00      3411\n",
      "\n",
      "    accuracy                           0.84     21447\n",
      "   macro avg       0.42      0.50      0.46     21447\n",
      "weighted avg       0.71      0.84      0.77     21447\n",
      "\n",
      "Train on 85788 samples\n",
      "Epoch 1/100\n",
      "85788/85788 [==============================] - 12s 140us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 2/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 3/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 4/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 5/100\n",
      "85788/85788 [==============================] - 11s 127us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 6/100\n",
      "85788/85788 [==============================] - 11s 124us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 7/100\n",
      "85788/85788 [==============================] - 9s 108us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 8/100\n",
      "85788/85788 [==============================] - 11s 125us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 9/100\n",
      "85788/85788 [==============================] - 12s 143us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 10/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 11/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 12/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 13/100\n",
      "85788/85788 [==============================] - 11s 125us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 14/100\n",
      "85788/85788 [==============================] - 9s 105us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 15/100\n",
      "85788/85788 [==============================] - 9s 103us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 16/100\n",
      "85788/85788 [==============================] - 12s 143us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 22/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 23/100\n",
      "85788/85788 [==============================] - 9s 104us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 24/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 25/100\n",
      "85788/85788 [==============================] - 12s 136us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 26/100\n",
      "85788/85788 [==============================] - 10s 121us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 27/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 28/100\n",
      "85788/85788 [==============================] - 10s 116us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 29/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 30/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 31/100\n",
      "85788/85788 [==============================] - 9s 109us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 32/100\n",
      "85788/85788 [==============================] - 12s 138us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 33/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 34/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 35/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 36/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 37/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 38/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 39/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 40/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 41/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 42/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 43/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 44/100\n",
      "85788/85788 [==============================] - 9s 110us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 45/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 46/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 47/100\n",
      "85788/85788 [==============================] - 10s 118us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 48/100\n",
      "85788/85788 [==============================] - 11s 130us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 49/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 50/100\n",
      "85788/85788 [==============================] - 10s 114us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 51/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 52/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 53/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 67/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 68/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 69/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 70/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 71/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 72/100\n",
      "85788/85788 [==============================] - 10s 115us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 73/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 74/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 75/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 76/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 77/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 78/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 79/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 80/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 81/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 82/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 83/100\n",
      "85788/85788 [==============================] - 11s 128us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 85/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 86/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 87/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 88/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 89/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 90/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 91/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 92/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 93/100\n",
      "85788/85788 [==============================] - 9s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 94/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 95/100\n",
      "85788/85788 [==============================] - 10s 113us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 96/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 97/100\n",
      "85788/85788 [==============================] - 10s 112us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 98/100\n",
      "85788/85788 [==============================] - 10s 111us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 99/100\n",
      "85788/85788 [==============================] - 12s 135us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n",
      "Epoch 100/100\n",
      "85788/85788 [==============================] - 12s 142us/sample - loss: 9.0822 - binary_accuracy: 0.4112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdjaw\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) [-111398810.0, -111398820.0, -111398824.0, -111398830.0, -111398840.0, -111398850.0, -111398856.0, -111398860.0, -111398870.0, -111398880.0, -111398890.0, -111398900.0, -111398904.0, -111398910.0, -111398920.0, -111398930.0, -111398936.0, -111398940.0, -111398950.0, -111398960.0, -111398980.0, -111399020.0, -111399030.0, -111399040.0, -111399050.0, -111399060.0, -111399064.0, -111399070.0, -111399080.0, -111399090.0, -111399096.0, -111399100.0, -111399110.0, -111399120.0, -111399130.0, -111399140.0, -111399144.0, -111399150.0, -111399160.0, -111399170.0, -111399176.0, -111399180.0, -111399190.0, -111399200.0, -111399220.0, -111399224.0, -111399230.0, -111399240.0, -111399250.0, -111399256.0, -111399260.0, -111399270.0, -111399280.0, -111399290.0, -111399300.0, -111399304.0, -111399310.0, -111399320.0, -111399330.0, -111399336.0, -111399340.0, -111399350.0, -111399360.0, -111400296.0, -111400300.0, -111400310.0, -111400320.0, -111400330.0, -111400340.0, -111400344.0, -111400350.0, -111400360.0, -111400370.0, -111400376.0, -111400380.0, -111400510.0, -111400520.0, -111400530.0, -111400536.0, -111400540.0, -111400550.0, -111400560.0, -111400570.0, -111400580.0, -111400584.0, -111400590.0, -111400600.0, -111400610.0, -111400616.0, -111400620.0, -111400630.0, -111400640.0, -111400650.0, -111400660.0, -111400664.0, -111400670.0, -111400680.0, -111400690.0, -111400696.0, -111400700.0, -111400780.0, -111400790.0, -111400800.0, -111400810.0, -111400820.0, -111400824.0, -111400830.0, -111400840.0, -111400850.0, -111400856.0, -111400860.0, -111400870.0, -111400880.0, -111400890.0, -111400900.0, -111400904.0, -111400910.0, -111400920.0, -111400930.0, -111400936.0, -111400940.0, -111400950.0, -111400960.0, -111400970.0, -111401180.0, -111401190.0, -111401200.0, -111401210.0, -111401220.0, -111401224.0, -111401230.0, -111401240.0, -111401250.0, -111401256.0, -111401260.0, -111401270.0, -111401280.0, -111401290.0, -111401300.0, -111401304.0, -111401310.0, -111401320.0, -111401330.0, -111401336.0, -111401340.0, -111401350.0, -111401360.0, -111401370.0, -111401380.0, -111401384.0, -111401390.0, -111401400.0, -111401410.0, -111401416.0, -111401420.0, -111401430.0, -111401440.0, -111401450.0, -111401460.0, -111401464.0, -111401470.0, -111401480.0, -111401740.0, -111401760.0, -111401770.0, -111401784.0, -111401790.0, -111401800.0, -111402270.0, -111402280.0, -111402290.0, -111402296.0, -111402300.0, -111402310.0, -111402320.0, -111402330.0, -111402340.0, -111402344.0, -111402350.0, -111402360.0, -111402370.0, -111402376.0, -111402380.0, -111402390.0, -111402400.0, -111402410.0, -111402420.0, -111402424.0, -111402430.0, -111402440.0, -111402450.0, -235831680.0, -235831700.0, -235831710.0, -235831730.0, -235831740.0, -235831760.0, -235831780.0, -235831790.0, -235831800.0, -235831820.0, -235831840.0, -235831860.0, -235831870.0, -235831890.0, -235831900.0, -235831920.0, -235831940.0, -235831950.0, -235831970.0, -235831980.0, -235832000.0, -235832020.0, -235832030.0, -235832050.0, -235832060.0, -235832080.0, -235832100.0, -235832110.0, -235832130.0, -235832140.0, -235832160.0, -235832180.0, -235832200.0, -235832210.0, -235832220.0, -235832240.0, -235832260.0, -235832270.0, -235832290.0, -235832300.0, -235832320.0, -235832340.0, -235832350.0, -235832370.0, -235832380.0, -235832400.0, -235832420.0, -235832430.0, -235832450.0, -235832460.0, -235832480.0, -235832500.0, -235832510.0, -235832530.0, -235832540.0, -235832560.0, -235832580.0, -235832590.0, -235832600.0, -235832620.0, -235832640.0, -235832660.0, -235832670.0, -235832690.0, -235832700.0, -235832720.0, -235832740.0, -235832750.0, -235832770.0, -235832780.0, -235832800.0, -235832820.0, -235832830.0, -235832850.0, -235832860.0, -235832880.0, -235832900.0, -235832910.0, -235832930.0, -235832940.0, -235832960.0, -235832980.0, -235833000.0, -235833010.0, -235833020.0, -235833040.0, -235833060.0, -235833070.0, -235833090.0, -235833100.0, -235833120.0, -235833140.0, -235833150.0, -235833170.0, -235833180.0, -235833200.0, -235833220.0, -235833230.0, -235833250.0, -235833260.0, -235833280.0, -235833300.0, -235833310.0, -235833330.0, -235833340.0, -235833360.0, -235833380.0, -235833390.0, -235833400.0, -235833420.0, -235833440.0, -235833470.0] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8567631836620506\n",
      "Hamming Loss: 0.07308714505525249\n",
      "BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     18689\n",
      "           1       0.00      0.00      0.00      2758\n",
      "\n",
      "    accuracy                           0.87     21447\n",
      "   macro avg       0.44      0.50      0.47     21447\n",
      "weighted avg       0.76      0.87      0.81     21447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import keras\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "number_of_classes = 4\n",
    "n_split=5\n",
    "\n",
    "for train_index, test_index in KFold(n_split).split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "  #print(y_train.shape)\n",
    "\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  labels = mlb.fit_transform(y_train)\n",
    "\n",
    "  #print(labels.shape)\n",
    "    \n",
    "  mlb = MultiLabelBinarizer()\n",
    "  mlb.fit(labels)\n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(1)\n",
    "  #]) \n",
    "    \n",
    "  #inputs = tf.keras.layers.Input(shape=(X_train.shape))\n",
    "  #outputs = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "  #model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "\n",
    "  #outputs = keras.layers.Dense(tf.keras.layers.Dense(number_of_classes)) #(x) \n",
    "    \n",
    "  #model = tf.keras.Sequential([\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(10),\n",
    "  #tf.keras.layers.Dense(2)\n",
    "  #])   \n",
    "  \n",
    "  #model = tf.contrib.learn.DNNClassifier(hidden_units=[5,10,5],\n",
    "  #                                          n_classes=4)    \n",
    "    \n",
    "\n",
    "  inputs = tf.keras.Input(shape=(X_train.shape))\n",
    "  #x = base_model(inputs, training=False)\n",
    "  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "  #activation =  tf.keras.activations.sigmoid #None  # tf.keras.activations.sigmoid or softmax\n",
    "  #initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    \n",
    "  outputs = tf.keras.layers.Dense(number_of_classes)  #, activation=activation)\n",
    "                             #kernel_initializer=initializer,\n",
    "                             #activation=activation) #(x) \n",
    "  #model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(138, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(276, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(2)\n",
    "  ]) \n",
    "    \n",
    "  #model.add(layers.Activation(activations.relu))\n",
    "    \n",
    "  model.compile(optimizer= tf.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), # default from_logits=False\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "  #model.compile(loss = tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(learning_rate=0.001))\n",
    "  #model.fit(X_train, labels, epochs=20)\n",
    "  model.fit(X_train, mlb.transform(y_train), epochs=100)\n",
    "\n",
    "  y_pred = model.predict(X_test)  \n",
    "  #print(y_pred)\n",
    "  #print(y_pred.shape)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(mlb.transform(y_pred))\n",
    "  y_pred2 = mlb.transform(y_pred)\n",
    "   \n",
    "  #rounded_labels=np.argmax(y_pred, axis=1)\n",
    "  #print(rounded_labels[1])  \n",
    "  #print(y_pred2.shape)\n",
    "  #print(y_test.shape)\n",
    "  #ynew = mlb.fit_transform(y_pred)\n",
    "  #ynew = mlb.fit(y_pred)\n",
    "  #f1_score(y_test, ynew, average='weighted')\n",
    "  #print(f1_score(y_test, y_pred2, average='weighted'))\n",
    "\n",
    "  #print(y_test.shape)\n",
    "  #print(y_pred.shape)\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(ynew)\n",
    "\n",
    "  #y_test = np.argmax(y_test)\n",
    "  #print(y_test.shape)\n",
    "\n",
    "  #ynew = keras.utils.to_categorical(y_pred)\n",
    "  #print(mlb.fit(y_pred))\n",
    "  #print(ynew.shape)\n",
    "\n",
    "  #rec=recall_score(y_test,mlb.fit(y_pred), average='macro')  \n",
    "  \n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  #print('Model evaluation ',model.evaluate(y_test, y_pred2))\n",
    "  #model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "  #print(y_test)\n",
    "  #print(y_pred2)\n",
    "\n",
    "  acc=accuracy_score(y_test,y_pred2)\n",
    "  print(\"Accuracy: \" + str(acc))  \n",
    "\n",
    "  #rec=recall_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Recall: \" + str(rec))\n",
    "\n",
    "  #pre=precision_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"Precision: \" + str(pre))\n",
    "\n",
    "  #f1=f1_score(y_test,y_pred2, average='macro')\n",
    "  #print(\"F1 Score: \" + str(f1))\n",
    "    \n",
    "  hl=hamming_loss(y_test,y_pred2)\n",
    "  print(\"Hamming Loss: \" + str(hl))\n",
    "    \n",
    "  y_pred2 = model.predict(X_test)  \n",
    "  pred2 = np.argmax(y_pred2, axis=1)\n",
    "  y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "  print(\"BRFSS MultiLabel Cancer + Diabetes - Grounded - TensorFlow CNN\")\n",
    "  #print(classification_report(y_test, pred, labels=[1, 2, 3]))\n",
    "  print(classification_report(y_test2, pred2))    \n",
    "    \n",
    "  #print(multilabel_confusion_matrix(y_test, y_pred2))\n",
    "  #cm = multilabel_confusion_matrix(y_test, y_pred2)\n",
    "  #print(cm)\n",
    "  #print(classification_report(y_test,y_pred2))\n",
    "\n",
    "df1 = pd.DataFrame(y_test)     \n",
    "filepath1 = 'output_y_test.csv'     \n",
    "df1.to_csv(filepath1)\n",
    "\n",
    "df2 = pd.DataFrame(y_pred2)\n",
    "filepath2 = 'output_y_pred.csv'\n",
    "df2.to_csv(filepath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
